{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb_EJtcXY3Rh"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ruliana/pytorch-katas/blob/main/dan_1/kata_01_temple_cat_feeding_predictor_unrevised.ipynb)\n",
        "\n",
        "## üèÆ The Ancient Scroll Unfurls üèÆ\n",
        "\n",
        "**THE MYSTERIES OF SUKI'S APPETITE: A LINEAR REVELATION**\n",
        "\n",
        "Dan Level: 1 (Temple Sweeper) | Time: 45 minutes | Sacred Arts: Linear Regression, Gradient Descent, Loss Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpTCNrRRY3Rm"
      },
      "source": [
        "## üìú THE CHALLENGE\n",
        "\n",
        "Master Pai-Torch sits in contemplative silence beside the temple's sacred feeding bowl, watching Suki the temple cat's daily rituals. \"Young grasshopper,\" the master begins, \"observe how Suki appears at this bowl with mysterious precision. Yet beneath this feline mystery lies a pattern as ancient as the mountains themselves. The wise temple keepers have long known that a cat's hunger grows steadily with each passing hour, following a sacred mathematical harmony.\"\n",
        "\n",
        "\"Your first trial as Temple Sweeper is to decode this sacred relationship,\" Master Pai-Torch continues, stroking their chin thoughtfully. \"Through the mystical arts of linear regression, you must learn to predict when Suki's hunger will compel her to appear at the feeding bowl. Master this simple relationship, and you will have taken your first step toward understanding the deeper mysteries of neural networks. But beware - even the most basic patterns require disciplined practice to master.\"\n",
        "\n",
        "## üéØ THE SACRED OBJECTIVES\n",
        "\n",
        "- [ ] **Tensor Mastery**: Create and manipulate PyTorch tensors for cat feeding data\n",
        "- [ ] **Linear Wisdom**: Implement a neural network with a single linear layer\n",
        "- [ ] **Gradient Discipline**: Master the sacred training loop with proper gradient management\n",
        "- [ ] **Loss Understanding**: Use Mean Squared Error to measure prediction accuracy\n",
        "- [ ] **Pattern Recognition**: Discover the hidden relationship between time and hunger\n",
        "- [ ] **Convergence Patience**: Train until your model achieves temple-worthy accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "EYGnzTxTY3Rn",
        "outputId": "2dd70cf2-aef8-4743-a087-9c1931c94f0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üèÆ The Temple of Neural Networks welcomes you, Grasshopper!\n",
            "PyTorch version: 2.6.0+cu124\n",
            "üê± Suki stirs from her afternoon nap, sensing the approach of learning...\n"
          ]
        }
      ],
      "source": [
        "# üì¶ ALL IMPORTS AND CONFIGURATION\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from typing import Tuple\n",
        "\n",
        "# Global configuration constants\n",
        "DEFAULT_CHAOS_LEVEL = 0.1\n",
        "FEEDING_THRESHOLD = 70  # Hunger level at which Suki appears\n",
        "\n",
        "print(\"üèÆ The Temple of Neural Networks welcomes you, Grasshopper!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(\"üê± Suki stirs from her afternoon nap, sensing the approach of learning...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfENvMI0Y3Rp"
      },
      "source": [
        "## üê± THE SACRED DATA GENERATION SCROLL\n",
        "\n",
        "*Master Pai-Torch gestures toward the feeding bowl*\n",
        "\n",
        "\"Before you can understand the cat, you must first understand the data.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juC-Et0PY3Rp"
      },
      "outputs": [],
      "source": [
        "def generate_cat_feeding_data(n_observations: int = 100, chaos_level: float = 0.1) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Generate observations of Suki's feeding patterns.\n",
        "\n",
        "    Ancient wisdom suggests: hunger_level = 2.5 * hours_since_last_meal + 20\n",
        "    When hunger_level > 70, Suki appears at the food bowl.\n",
        "\n",
        "    Args:\n",
        "        n_observations: Number of Suki sightings to simulate\n",
        "        chaos_level: Amount of feline unpredictability (0.0 = perfectly predictable cat, 1.0 = pure chaos)\n",
        "        sacred_seed: Ensures consistent randomness\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (hours_since_last_meal, hunger_level) as sacred tensors\n",
        "    \"\"\"\n",
        "    # Suki can go 0-30 hours between meals (she's very dramatic)\n",
        "    hours_since_meal = torch.rand(n_observations, 1) * 30\n",
        "\n",
        "    # The sacred relationship known to ancient cat scholars\n",
        "    base_hunger = 20\n",
        "    hunger_per_hour = 2.5\n",
        "\n",
        "    hunger_levels = hunger_per_hour * hours_since_meal.squeeze() + base_hunger\n",
        "\n",
        "    # Add feline chaos (cats are unpredictable creatures)\n",
        "    chaos = torch.randn(n_observations) * chaos_level * hunger_levels.std()\n",
        "    hunger_levels = hunger_levels + chaos\n",
        "\n",
        "    # Even mystical cats have limits\n",
        "    hunger_levels = torch.clamp(hunger_levels, 0, 100)\n",
        "\n",
        "    return hours_since_meal, hunger_levels.unsqueeze(1)\n",
        "\n",
        "def visualize_cat_wisdom(hours: torch.Tensor, hunger: torch.Tensor, predictions: torch.Tensor = None):\n",
        "    \"\"\"Display the sacred patterns of Suki's appetite.\"\"\"\n",
        "    plt.figure(figsize=(12, 7))\n",
        "    plt.scatter(hours.numpy(), hunger.numpy(), alpha=0.6, color='purple',\n",
        "                label='Suki\\'s Actual Hunger Levels')\n",
        "\n",
        "    if predictions is not None:\n",
        "        sorted_indices = torch.argsort(hours.squeeze())\n",
        "        sorted_hours = hours[sorted_indices]\n",
        "        sorted_predictions = predictions[sorted_indices]\n",
        "        plt.plot(sorted_hours.numpy(), sorted_predictions.detach().numpy(),\n",
        "                'gold', linewidth=3, label='Your Mystical Predictions')\n",
        "\n",
        "    plt.axhline(y=FEEDING_THRESHOLD, color='red', linestyle='--', alpha=0.7,\n",
        "                label='Sacred Feeding Threshold (Suki Appears!)')\n",
        "    plt.xlabel('Hours Since Last Meal (feature)')\n",
        "    plt.ylabel('Suki\\'s Hunger Level (target)')\n",
        "    plt.title('The Mysteries of Temple Cat Appetite')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.ylim(0, 100)\n",
        "    plt.show()\n",
        "\n",
        "# Generate the sacred data\n",
        "hours_since_meal, hunger_levels = generate_cat_feeding_data(n_observations=100)\n",
        "\n",
        "print(f\"üìä Generated {len(hours_since_meal)} observations of Suki's feeding patterns\")\n",
        "print(f\"‚è∞ Hours since meal range: {hours_since_meal.min():.1f} to {hours_since_meal.max():.1f}\")\n",
        "print(f\"üçΩÔ∏è Hunger levels range: {hunger_levels.min():.1f} to {hunger_levels.max():.1f}\")\n",
        "\n",
        "# Visualize the sacred patterns\n",
        "visualize_cat_wisdom(hours_since_meal, hunger_levels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kkdux47jY3Rq"
      },
      "source": [
        "## üíÉ FIRST MOVEMENTS: THE NEURAL NETWORK FOUNDATION\n",
        "\n",
        "*Master Pai-Torch nods approvingly*\n",
        "\n",
        "\"Now that you have witnessed the sacred data, it is time to craft your first neural network. Though simple in form, this linear layer contains the essence of all deeper mysteries. Complete the missing sacred techniques below.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaNpsaSyY3Rq"
      },
      "outputs": [],
      "source": [
        "class CatHungerPredictor(nn.Module):\n",
        "    \"\"\"A mystical artifact for understanding feline appetite patterns.\"\"\"\n",
        "\n",
        "    def __init__(self, input_features: int = 1):\n",
        "        super(CatHungerPredictor, self).__init__()\n",
        "        # TODO: Create the Linear layer\n",
        "        # Hint: torch.nn.Linear transforms input energy into output wisdom\n",
        "        # It needs input_features and output_features (how many predictions?)\n",
        "        self.linear = None\n",
        "\n",
        "    def forward(self, features: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Channel your understanding through the mystical network.\"\"\"\n",
        "        # TODO: Pass the input through your Linear layer\n",
        "        # Remember: even cats follow mathematical laws\n",
        "        return None\n",
        "\n",
        "def train(model: nn.Module, features: torch.Tensor, target: torch.Tensor, epochs: int = 4_000) -> list:\n",
        "    \"\"\"\n",
        "    Train the cat hunger prediction model.\n",
        "\n",
        "    Returns:\n",
        "        List of loss values during training\n",
        "    \"\"\"\n",
        "    # TODO: Choose your loss calculation method\n",
        "    # Hint: Mean Squared Error is favored by the ancient masters\n",
        "    criterion = None\n",
        "\n",
        "    # TODO: Choose your parameter updating method\n",
        "    # Hint: SGD (Stochastic Gradient Descent) is the traditional path\n",
        "    optimizer = None\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # TODO: CRITICAL - Clear the gradient spirits from previous cycle\n",
        "        # Hint: The spirits accumulate if not banished properly\n",
        "        # This is the most common mistake in PyTorch training!\n",
        "\n",
        "        # TODO: Forward pass - get predictions\n",
        "        predictions = None\n",
        "\n",
        "        # TODO: Compute the loss\n",
        "        loss = None\n",
        "\n",
        "        # TODO: Backward pass - compute gradients\n",
        "        # Hint: Loss knows how to compute its own gradients\n",
        "\n",
        "        # TODO: Update parameters\n",
        "        # Hint: The optimizer knows how to update using the gradients\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # Report progress to Master Pai-Torch\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "            if loss.item() < 10:\n",
        "                print(\"üí´ The Gradient Spirits smile upon your progress!\")\n",
        "\n",
        "    return losses\n",
        "\n",
        "# Create your first neural network\n",
        "model = CatHungerPredictor(input_features=1)\n",
        "print(\"üß† Your neural network has been born!\")\n",
        "print(f\"Model structure: {model}\")\n",
        "print(f\"Initial parameters: Weight={model.linear.weight.item():.3f}, Bias={model.linear.bias.item():.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjgngVwcY3Rr"
      },
      "source": [
        "## üéØ UNLEASH THE TRAINING RITUAL\n",
        "\n",
        "*Master Pai-Torch places a weathered hand on your shoulder*\n",
        "\n",
        "\"Now comes the sacred moment, grasshopper. Train your network with the feeding data and witness the emergence of wisdom from randomness.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GqAdbcFY3Rs"
      },
      "outputs": [],
      "source": [
        "# Begin the training ritual\n",
        "print(\"üî• Beginning the sacred training ritual...\")\n",
        "print(\"Master Pai-Torch whispers: 'Watch the loss decrease, young one. This is the dance of learning.'\")\n",
        "\n",
        "# TODO: Train your model using the function above\n",
        "# Use: hours_since_meal, hunger_levels, and appropriate epochs/learning_rate\n",
        "loss_history = None\n",
        "\n",
        "# Visualize the training progress\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(loss_history)\n",
        "plt.title('The Sacred Dance of Loss Reduction')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Examine the learned parameters\n",
        "learned_weight = model.linear.weight.item()\n",
        "learned_bias = model.linear.bias.item()\n",
        "print(f\"\\nüéä Training Complete! üéä\")\n",
        "print(f\"Learned relationship: hunger = {learned_weight:.3f} √ó hours + {learned_bias:.3f}\")\n",
        "print(f\"True relationship: hunger = 2.500 √ó hours + 20.000\")\n",
        "print(f\"Weight accuracy: {abs(learned_weight - 2.5):.3f} away from true value\")\n",
        "print(f\"Bias accuracy: {abs(learned_bias - 20):.3f} away from true value\")\n",
        "\n",
        "# Generate predictions and visualize\n",
        "with torch.no_grad():\n",
        "    predictions = model(hours_since_meal)\n",
        "\n",
        "print(\"\\nüîÆ Visualizing your mystical predictions...\")\n",
        "visualize_cat_wisdom(hours_since_meal, hunger_levels, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfL-EV_TY3Rt"
      },
      "source": [
        "## ‚ö° THE TRIALS OF MASTERY\n",
        "\n",
        "*Master Pai-Torch examines your work with ancient eyes*\n",
        "\n",
        "\"Your first steps show promise, grasshopper. But true mastery must be tested through sacred trials.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFXq_hyaY3Rt"
      },
      "outputs": [],
      "source": [
        "# TRIALS OF MASTERY\n",
        "print(\"‚ö° TRIAL 1: BASIC MASTERY\")\n",
        "\n",
        "# Check your progress\n",
        "final_loss = loss_history[-1] if loss_history else float('inf')\n",
        "weight_accuracy = abs(learned_weight - 2.5) < 0.5\n",
        "bias_accuracy = abs(learned_bias - 20) < 5\n",
        "\n",
        "# Check if loss decreases consistently (last loss < first loss by significant margin)\n",
        "loss_decreases = len(loss_history) > 100 and loss_history[-1] < loss_history[99] * 0.9\n",
        "\n",
        "# Check if predictions form a clean line (R¬≤ > 0.8)\n",
        "with torch.no_grad():\n",
        "    predictions = model(hours_since_meal)\n",
        "    y_mean = hunger_levels.mean()\n",
        "    ss_tot = ((hunger_levels - y_mean) ** 2).sum()\n",
        "    ss_res = ((hunger_levels - predictions) ** 2).sum()\n",
        "    r_squared = 1 - (ss_res / ss_tot)\n",
        "    clean_line = r_squared > 0.8\n",
        "\n",
        "# Trial 1 checkboxes\n",
        "loss_check = \"‚úÖ\" if loss_decreases else \"‚ùå\"\n",
        "weight_bias_check = \"‚úÖ\" if (weight_accuracy and bias_accuracy) else \"‚ùå\"\n",
        "line_check = \"‚úÖ\" if clean_line else \"‚ùå\"\n",
        "\n",
        "print(f\"- {loss_check} Loss decreases consistently (no angry Gradient Spirits)\")\n",
        "print(f\"- {weight_bias_check} Model weight approximately 2.5 (¬±0.5), bias around 20 (¬±5)\")\n",
        "print(f\"- {line_check} Predictions form a clean line through the scattered data\")\n",
        "\n",
        "# Trial 2: Understanding Test\n",
        "print(\"\\n‚ö° TRIAL 2: UNDERSTANDING TEST\")\n",
        "\n",
        "# Test prediction shapes\n",
        "test_features = torch.tensor([[5.0], [10.0], [20.0]])\n",
        "with torch.no_grad():\n",
        "    test_predictions = model(test_features)\n",
        "\n",
        "shapes_correct = test_predictions.shape == (3, 1)\n",
        "weight_reasonable = 2.0 <= learned_weight <= 3.0\n",
        "bias_reasonable = 15 <= learned_bias <= 25\n",
        "\n",
        "# Test prediction reasonableness\n",
        "test_pred_values = test_predictions.squeeze().tolist()\n",
        "expected_approx = [2.5 * 5 + 20, 2.5 * 10 + 20, 2.5 * 20 + 20]  # [32.5, 45, 70]\n",
        "predictions_reasonable = all(abs(pred - exp) <= 10 for pred, exp in zip(test_pred_values, expected_approx))\n",
        "\n",
        "# Trial 2 checkboxes\n",
        "shapes_check = \"‚úÖ\" if shapes_correct else \"‚ùå\"\n",
        "weight_param_check = \"‚úÖ\" if weight_reasonable else \"‚ùå\"\n",
        "bias_param_check = \"‚úÖ\" if bias_reasonable else \"‚ùå\"\n",
        "pred_check = \"‚úÖ\" if predictions_reasonable else \"‚ùå\"\n",
        "\n",
        "print(f\"- {shapes_check} Tensor shapes align with the sacred geometry\")\n",
        "print(f\"- {weight_param_check} Weight parameter reflects feline wisdom\")\n",
        "print(f\"- {bias_param_check} Bias parameter captures base hunger levels\")\n",
        "print(f\"- {pred_check} Predictions are reasonable for test inputs\")\n",
        "\n",
        "# Your Performance section\n",
        "print(f\"\\nüìä Your Performance:\")\n",
        "print(f\"- Weight accuracy: {learned_weight:.3f} {'(PASS)' if weight_accuracy else '(FAIL)'}\")\n",
        "print(f\"- Bias accuracy: {learned_bias:.3f} {'(PASS)' if bias_accuracy else '(FAIL)'}\")\n",
        "\n",
        "# Overall success check\n",
        "trial1_passed = loss_decreases and weight_accuracy and bias_accuracy and clean_line\n",
        "trial2_passed = shapes_correct and weight_reasonable and bias_reasonable and predictions_reasonable\n",
        "\n",
        "if trial1_passed and trial2_passed:\n",
        "    print(\"\\nüéâ Master Pai-Torch nods with approval - your understanding grows!\")\n",
        "    print(\"\\nüèÜ Congratulations! You have passed the basic trials of the Temple Sweeper!\")\n",
        "    print(\"üê± Suki purrs approvingly - your neural network has learned her sacred patterns.\")\n",
        "else:\n",
        "    print(\"\\nü§î The path to mastery requires more practice. Consider adjusting your training parameters.\")\n",
        "    print(\"üí° Hint: Try different learning rates, more epochs, or review your code for errors.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wav0AdVGY3Ru"
      },
      "source": [
        "## üå∏ THE FOUR PATHS OF MASTERY: PROGRESSIVE EXTENSIONS\n",
        "\n",
        "*Master Pai-Torch gestures toward four different pathways leading deeper into the temple*\n",
        "\n",
        "\"You have learned the fundamental way, grasshopper. But mastery comes through exploring the branching paths. Each extension will teach you new aspects of the neural arts while building upon your foundation.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4o6-a37sY3Ru"
      },
      "source": [
        "### Extension 1: Cook Oh-Pai-Timizer's Portion Control\n",
        "*\"A good cook knows that batch size affects the final dish!\"*\n",
        "\n",
        "*Cook Oh-Pai-Timizer bustles over, wooden spoon in hand*\n",
        "\n",
        "\"Ah, grasshopper! I see you've mastered feeding one cat at a time. But what happens when you need to predict hunger for multiple cats simultaneously? In my kitchen, efficiency comes from preparing multiple servings at once! Your neural network can learn the same wisdom - processing many observations together.\"\n",
        "\n",
        "**NEW CONCEPTS**: Batch processing, tensor shapes, vectorized operations  \n",
        "**DIFFICULTY**: +15% (still Dan 1, but with batches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrBlbkHaY3Ru"
      },
      "outputs": [],
      "source": [
        "def generate_multi_cat_data(n_cats: int = 5, observations_per_cat: int = 50,\n",
        "                           chaos_level: float = 0.15):\n",
        "    \"\"\"\n",
        "    Generate feeding data for multiple temple cats at once.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (batch_hours, batch_hunger_levels)\n",
        "        Shape: (n_cats * observations_per_cat, 1) for both tensors\n",
        "    \"\"\"\n",
        "    # TODO: Create batched data that your model can process all at once\n",
        "    # Hint: Use torch.cat() to combine data from multiple cats\n",
        "    # Hint: Each cat follows the same hunger pattern but with different chaos\n",
        "\n",
        "    all_hours = []\n",
        "    all_hunger = []\n",
        "\n",
        "    for cat_id in range(n_cats):\n",
        "        # Generate data for this cat (slightly different chaos per cat)\n",
        "        cat_hours, cat_hunger = generate_cat_feeding_data(\n",
        "            n_observations=observations_per_cat,\n",
        "            chaos_level=chaos_level * (1 + cat_id * 0.1),  # Each cat is slightly more chaotic\n",
        "            sacred_seed=42 + cat_id\n",
        "        )\n",
        "        all_hours.append(cat_hours)\n",
        "        all_hunger.append(cat_hunger)\n",
        "\n",
        "    # TODO: Combine all cat data into batched tensors\n",
        "    batch_hours = torch.cat(all_hours, dim=0)\n",
        "    batch_hunger = torch.cat(all_hunger, dim=0)\n",
        "\n",
        "    return batch_hours, batch_hunger\n",
        "\n",
        "# Generate multi-cat data\n",
        "multi_hours, multi_hunger = generate_multi_cat_data(n_cats=5, observations_per_cat=40)\n",
        "\n",
        "print(f\"üê±üê±üê± Generated data for multiple cats:\")\n",
        "print(f\"   Total observations: {len(multi_hours)}\")\n",
        "print(f\"   Hours tensor shape: {multi_hours.shape}\")\n",
        "print(f\"   Hunger tensor shape: {multi_hunger.shape}\")\n",
        "\n",
        "# TRIAL: Feed batched data to your existing model\n",
        "print(\"\\nüçΩÔ∏è Testing batch processing with your trained model...\")\n",
        "with torch.no_grad():\n",
        "    batch_predictions = model(multi_hours)\n",
        "\n",
        "print(f\"‚úÖ Your model successfully processed {len(multi_hours)} observations at once!\")\n",
        "print(f\"   Predictions shape: {batch_predictions.shape}\")\n",
        "\n",
        "# Visualize the multi-cat results\n",
        "visualize_cat_wisdom(multi_hours, multi_hunger, batch_predictions)\n",
        "\n",
        "# SUCCESS: Model processes multiple cats simultaneously, same accuracy\n",
        "batch_loss = nn.MSELoss()(batch_predictions, multi_hunger)\n",
        "print(f\"\\nüìä Batch processing loss: {batch_loss.item():.2f}\")\n",
        "print(\"üéâ Cook Oh-Pai-Timizer beams: 'Efficiency and accuracy - the hallmarks of a master chef!'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-QrIKcVY3Rv"
      },
      "source": [
        "### Extension 2: He-Ao-World's Measurement Mix-up\n",
        "*\"These old eyes sometimes read the measuring scrolls incorrectly...\"*\n",
        "\n",
        "*He-Ao-World shuffles over, looking apologetic*\n",
        "\n",
        "\"Oh dear! I was recording Suki's feeding times and... well, I might have mixed up some of the measurements. Some are in minutes instead of hours, and others might be twice what they should be. The data looks a bit... chaotic now. Perhaps you could help clean it up? I've heard that normalizing data can make neural networks much happier!\"\n",
        "\n",
        "**NEW CONCEPTS**: Data normalization, feature scaling, handling inconsistent units  \n",
        "**DIFFICULTY**: +25% (still Dan 1, but messier data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0BUu_gAY3Rv"
      },
      "outputs": [],
      "source": [
        "def create_messy_data(clean_hours: torch.Tensor, clean_hunger: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Simulate He-Ao-World's measurement accidents.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (messy_hours, messy_hunger) with inconsistent scales\n",
        "    \"\"\"\n",
        "    messy_hours = clean_hours.clone()\n",
        "    messy_hunger = clean_hunger.clone()\n",
        "\n",
        "    # 30% of hours accidentally recorded in minutes instead of hours\n",
        "    minute_mask = torch.rand(len(messy_hours)) < 0.3\n",
        "    messy_hours[minute_mask] = messy_hours[minute_mask] * 60\n",
        "\n",
        "    # 20% of hunger levels accidentally doubled\n",
        "    double_mask = torch.rand(len(messy_hunger)) < 0.2\n",
        "    messy_hunger[double_mask] = messy_hunger[double_mask] * 2\n",
        "\n",
        "    # 10% of both measurements have random scaling errors\n",
        "    error_mask = torch.rand(len(messy_hours)) < 0.1\n",
        "    random_scales = torch.rand(error_mask.sum()) * 3 + 0.5  # Random scale 0.5-3.5\n",
        "    messy_hours[error_mask] = messy_hours[error_mask] * random_scales.unsqueeze(1)\n",
        "\n",
        "    return messy_hours, messy_hunger\n",
        "\n",
        "def normalize_feeding_data(hours_since_meal: torch.Tensor, hunger_levels: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Clean and normalize the feeding data to handle measurement inconsistencies.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (normalized_hours, normalized_hunger, normalization_params)\n",
        "    \"\"\"\n",
        "    # TODO: Implement data normalization\n",
        "    # Hint: (data - mean) / std is a common normalization approach\n",
        "    # Remember: Store the normalization parameters for later use!\n",
        "\n",
        "    # Calculate normalization parameters\n",
        "    hours_mean = hours_since_meal.mean()\n",
        "    hours_std = hours_since_meal.std()\n",
        "    hunger_mean = hunger_levels.mean()\n",
        "    hunger_std = hunger_levels.std()\n",
        "\n",
        "    # Normalize the data\n",
        "    normalized_hours = (hours_since_meal - hours_mean) / (hours_std + 1e-8)  # Small epsilon to avoid division by zero\n",
        "    normalized_hunger = (hunger_levels - hunger_mean) / (hunger_std + 1e-8)\n",
        "\n",
        "    # Store parameters for later denormalization\n",
        "    normalization_params = {\n",
        "        'hours_mean': hours_mean,\n",
        "        'hours_std': hours_std,\n",
        "        'hunger_mean': hunger_mean,\n",
        "        'hunger_std': hunger_std\n",
        "    }\n",
        "\n",
        "    return normalized_hours, normalized_hunger, normalization_params\n",
        "\n",
        "# Create messy data\n",
        "messy_hours, messy_hunger = create_messy_data(hours_since_meal, hunger_levels)\n",
        "\n",
        "print(\"üå™Ô∏è He-Ao-World's measurement chaos:\")\n",
        "print(f\"   Original hours range: {hours_since_meal.min():.1f} to {hours_since_meal.max():.1f}\")\n",
        "print(f\"   Messy hours range: {messy_hours.min():.1f} to {messy_hours.max():.1f}\")\n",
        "print(f\"   Original hunger range: {hunger_levels.min():.1f} to {hunger_levels.max():.1f}\")\n",
        "print(f\"   Messy hunger range: {messy_hunger.min():.1f} to {messy_hunger.max():.1f}\")\n",
        "\n",
        "# Normalize the messy data\n",
        "norm_hours, norm_hunger, norm_params = normalize_feeding_data(messy_hours, messy_hunger)\n",
        "\n",
        "print(\"\\nüßπ After normalization:\")\n",
        "print(f\"   Normalized hours: mean={norm_hours.mean():.3f}, std={norm_hours.std():.3f}\")\n",
        "print(f\"   Normalized hunger: mean={norm_hunger.mean():.3f}, std={norm_hunger.std():.3f}\")\n",
        "\n",
        "# Train a new model on normalized data\n",
        "print(\"\\nüéØ Training on normalized data...\")\n",
        "normalized_model = CatHungerPredictor(input_features=1)\n",
        "normalized_losses = train_cat_predictor(normalized_model, norm_hours, norm_hunger,\n",
        "                                      epochs=800, learning_rate=0.05)\n",
        "\n",
        "# Compare training curves\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(loss_history, label='Original Data')\n",
        "plt.plot(normalized_losses, label='Normalized Data')\n",
        "plt.title('Training Loss Comparison')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "with torch.no_grad():\n",
        "    norm_predictions = normalized_model(norm_hours)\n",
        "    # Denormalize predictions for visualization\n",
        "    denorm_predictions = norm_predictions * norm_params['hunger_std'] + norm_params['hunger_mean']\n",
        "    denorm_hours = norm_hours * norm_params['hours_std'] + norm_params['hours_mean']\n",
        "\n",
        "plt.scatter(denorm_hours.numpy(), messy_hunger.numpy(), alpha=0.5, label='Messy Data')\n",
        "sorted_indices = torch.argsort(denorm_hours.squeeze())\n",
        "sorted_hours = denorm_hours[sorted_indices]\n",
        "sorted_predictions = denorm_predictions[sorted_indices]\n",
        "plt.plot(sorted_hours.numpy(), sorted_predictions.numpy(), 'r-', linewidth=2, label='Normalized Model')\n",
        "plt.title('Normalized Model Performance')\n",
        "plt.xlabel('Hours Since Meal')\n",
        "plt.ylabel('Hunger Level')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüìä Final normalized loss: {normalized_losses[-1]:.4f}\")\n",
        "print(\"üéâ He-Ao-World bows gratefully: 'Your wisdom has turned my chaos into clarity!'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqETo6jdY3Rw"
      },
      "source": [
        "### Extension 3: Master Pai-Torch's Patience Teaching\n",
        "*\"The eager student trains too quickly and learns too little.\"*\n",
        "\n",
        "*Master Pai-Torch sits in contemplative silence*\n",
        "\n",
        "\"Young grasshopper, I observe your training ritual rushes like a mountain stream. But wisdom comes to those who vary their pace. Sometimes we must step boldly, sometimes cautiously, sometimes we must rest entirely. Learn the arts of patience - early stopping when progress ceases, and learning rate scheduling to train with growing wisdom.\"\n",
        "\n",
        "**NEW CONCEPTS**: Learning rate scheduling, early stopping, training patience  \n",
        "**DIFFICULTY**: +35% (still Dan 1, but smarter training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42_2R6t2Y3Rw"
      },
      "outputs": [],
      "source": [
        "def patient_training_ritual(model: nn.Module, features: torch.Tensor, target: torch.Tensor,\n",
        "                          epochs: int = 2000, initial_lr: float = 0.1, patience: int = 100):\n",
        "    \"\"\"\n",
        "    Train with patience and adaptive learning rate.\n",
        "\n",
        "    Args:\n",
        "        patience: Stop training if loss doesn't improve for this many epochs\n",
        "        initial_lr: Starting learning rate\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (trained_model, loss_history, stopped_early, learning_rates)\n",
        "    \"\"\"\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=initial_lr)\n",
        "\n",
        "    # TODO: Implement patient training with learning rate decay\n",
        "    # Hint: Start with lr=0.1, reduce by half every 500 epochs\n",
        "    # Hint: Keep track of best loss and stop if no improvement\n",
        "\n",
        "    losses = []\n",
        "    learning_rates = []\n",
        "    best_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    stopped_early = False\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Learning rate scheduling: reduce by half every 500 epochs\n",
        "        if epoch > 0 and epoch % 500 == 0:\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "            new_lr = current_lr * 0.5\n",
        "            optimizer.param_groups[0]['lr'] = new_lr\n",
        "            print(f\"üìâ Epoch {epoch}: Reducing learning rate to {new_lr:.6f}\")\n",
        "\n",
        "        # Training step\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(features)\n",
        "        loss = criterion(predictions, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        current_loss = loss.item()\n",
        "        losses.append(current_loss)\n",
        "        learning_rates.append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "        # Early stopping logic\n",
        "        if current_loss < best_loss - 1e-6:  # Small threshold for improvement\n",
        "            best_loss = current_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Check if we should stop early\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"üõë Early stopping at epoch {epoch+1}: No improvement for {patience} epochs\")\n",
        "            stopped_early = True\n",
        "            break\n",
        "\n",
        "        # Progress reporting\n",
        "        if (epoch + 1) % 200 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {current_loss:.4f}, LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
        "\n",
        "    return model, losses, stopped_early, learning_rates\n",
        "\n",
        "# Compare patient training vs. rushed training\n",
        "print(\"üèÉ First, let's see rushed training (high learning rate, no patience):\")\n",
        "rushed_model = CatHungerPredictor(input_features=1)\n",
        "rushed_losses = train_cat_predictor(rushed_model, hours_since_meal, hunger_levels,\n",
        "                                  epochs=1000, learning_rate=0.1)\n",
        "\n",
        "print(\"\\nüßò Now, let's try patient training:\")\n",
        "patient_model = CatHungerPredictor(input_features=1)\n",
        "patient_model, patient_losses, stopped_early, learning_rates = patient_training_ritual(\n",
        "    patient_model, hours_since_meal, hunger_levels,\n",
        "    epochs=2000, initial_lr=0.1, patience=150\n",
        ")\n",
        "\n",
        "# Visualize the comparison\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Loss comparison\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(rushed_losses, label='Rushed Training', alpha=0.7)\n",
        "plt.plot(patient_losses, label='Patient Training', alpha=0.7)\n",
        "plt.title('Loss Comparison')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Learning rate schedule\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(learning_rates)\n",
        "plt.title('Learning Rate Schedule')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.yscale('log')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Final predictions comparison\n",
        "plt.subplot(1, 3, 3)\n",
        "with torch.no_grad():\n",
        "    rushed_pred = rushed_model(hours_since_meal)\n",
        "    patient_pred = patient_model(hours_since_meal)\n",
        "\n",
        "plt.scatter(hours_since_meal.numpy(), hunger_levels.numpy(), alpha=0.5, label='Data')\n",
        "sorted_indices = torch.argsort(hours_since_meal.squeeze())\n",
        "sorted_hours = hours_since_meal[sorted_indices]\n",
        "plt.plot(sorted_hours.numpy(), rushed_pred[sorted_indices].numpy(), 'r--', label='Rushed', linewidth=2)\n",
        "plt.plot(sorted_hours.numpy(), patient_pred[sorted_indices].numpy(), 'g-', label='Patient', linewidth=2)\n",
        "plt.title('Final Predictions Comparison')\n",
        "plt.xlabel('Hours Since Meal')\n",
        "plt.ylabel('Hunger Level')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Results summary\n",
        "print(f\"\\nüìä TRAINING COMPARISON:\")\n",
        "print(f\"   Rushed final loss: {rushed_losses[-1]:.4f}\")\n",
        "print(f\"   Patient final loss: {patient_losses[-1]:.4f}\")\n",
        "print(f\"   Patient training epochs: {len(patient_losses)}\")\n",
        "print(f\"   Stopped early: {stopped_early}\")\n",
        "print(f\"   Improvement: {((rushed_losses[-1] - patient_losses[-1]) / rushed_losses[-1] * 100):.1f}%\")\n",
        "\n",
        "print(\"\\nüéâ Master Pai-Torch smiles: 'The patient student achieves deeper understanding with less effort.'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6W8XFbzY3Rx"
      },
      "source": [
        "### Extension 4: He-Ao-World's Mysterious Incidents\n",
        "*\"Some days, the most unexpected things happen in the temple...\"*\n",
        "\n",
        "*He-Ao-World approaches nervously, wringing hands*\n",
        "\n",
        "\"Oh dear, grasshopper... I must confess something troubling. Some days, the most unusual incidents occur during my cleaning rounds. A loud crash from the kitchen sends Suki running to hide, despite being hungry. Or a visiting monk drops treats, making her appear even when she's not particularly hungry. These... outlying events... they don't follow the normal pattern, and I fear they may confuse your linear wisdom.\"\n",
        "\n",
        "**NEW CONCEPTS**: Outliers, robust training, linear regression limitations  \n",
        "**DIFFICULTY**: +45% (still Dan 1, but revealing model weaknesses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6-HIwDzY3Rx"
      },
      "outputs": [],
      "source": [
        "def generate_data_with_outliers(n_observations: int = 200, outlier_fraction: float = 0.1,\n",
        "                               chaos_level: float = 0.1, sacred_seed: int = 42):\n",
        "    \"\"\"\n",
        "    Generate feeding data with mysterious outlying incidents.\n",
        "\n",
        "    Args:\n",
        "        n_observations: Total number of observations\n",
        "        outlier_fraction: Fraction of data that will be outliers (0.0 to 1.0)\n",
        "        chaos_level: Normal noise level for regular data\n",
        "        sacred_seed: For reproducible randomness\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (hours, hunger_levels, is_outlier_mask)\n",
        "    \"\"\"\n",
        "    torch.manual_seed(sacred_seed)\n",
        "\n",
        "    # Generate normal data first\n",
        "    hours, hunger = generate_cat_feeding_data(n_observations, chaos_level, sacred_seed)\n",
        "\n",
        "    # Determine which points will be outliers\n",
        "    n_outliers = int(n_observations * outlier_fraction)\n",
        "    outlier_indices = torch.randperm(n_observations)[:n_outliers]\n",
        "    is_outlier = torch.zeros(n_observations, dtype=torch.bool)\n",
        "    is_outlier[outlier_indices] = True\n",
        "\n",
        "    # Create outliers: dramatic deviations from normal pattern\n",
        "    for i in outlier_indices:\n",
        "        incident_type = torch.randint(0, 3, (1,)).item()\n",
        "\n",
        "        if incident_type == 0:\n",
        "            # Kitchen crash: Suki hides despite hunger\n",
        "            # High hunger but behavior like low hunger\n",
        "            hunger[i] = torch.rand(1) * 30 + 10  # Force low hunger reading\n",
        "\n",
        "        elif incident_type == 1:\n",
        "            # Monk drops treats: Suki appears despite not being hungry\n",
        "            # Low hunger but behavior like high hunger\n",
        "            hunger[i] = torch.rand(1) * 30 + 70  # Force high hunger reading\n",
        "\n",
        "        else:\n",
        "            # Temple bell rings: Completely random behavior\n",
        "            # Completely random hunger level\n",
        "            hunger[i] = torch.rand(1) * 100\n",
        "\n",
        "    return hours, hunger, is_outlier\n",
        "\n",
        "def visualize_outlier_impact(hours: torch.Tensor, hunger: torch.Tensor, is_outlier: torch.Tensor,\n",
        "                           clean_model: nn.Module, outlier_model: nn.Module):\n",
        "    \"\"\"\n",
        "    Visualize how outliers affect linear regression predictions.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Plot 1: Data with outliers highlighted\n",
        "    plt.subplot(2, 2, 1)\n",
        "    normal_mask = ~is_outlier\n",
        "    plt.scatter(hours[normal_mask].numpy(), hunger[normal_mask].numpy(),\n",
        "               alpha=0.6, color='purple', label='Normal Behavior', s=50)\n",
        "    plt.scatter(hours[is_outlier].numpy(), hunger[is_outlier].numpy(),\n",
        "               alpha=0.8, color='red', label='Mysterious Incidents', s=100, marker='x')\n",
        "    plt.xlabel('Hours Since Last Meal')\n",
        "    plt.ylabel('Hunger Level')\n",
        "    plt.title('Temple Data: Normal vs Outlying Incidents')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 2: Model comparisons\n",
        "    plt.subplot(2, 2, 2)\n",
        "    with torch.no_grad():\n",
        "        clean_pred = clean_model(hours)\n",
        "        outlier_pred = outlier_model(hours)\n",
        "\n",
        "    plt.scatter(hours[normal_mask].numpy(), hunger[normal_mask].numpy(),\n",
        "               alpha=0.4, color='purple', label='Normal Data')\n",
        "    plt.scatter(hours[is_outlier].numpy(), hunger[is_outlier].numpy(),\n",
        "               alpha=0.8, color='red', label='Outliers', s=100, marker='x')\n",
        "\n",
        "    # Plot prediction lines\n",
        "    sorted_indices = torch.argsort(hours.squeeze())\n",
        "    sorted_hours = hours[sorted_indices]\n",
        "    plt.plot(sorted_hours.numpy(), clean_pred[sorted_indices].numpy(),\n",
        "             'g-', linewidth=3, label='Clean Data Model')\n",
        "    plt.plot(sorted_hours.numpy(), outlier_pred[sorted_indices].numpy(),\n",
        "             'r--', linewidth=3, label='Outlier-Affected Model')\n",
        "\n",
        "    plt.xlabel('Hours Since Last Meal')\n",
        "    plt.ylabel('Hunger Level')\n",
        "    plt.title('Model Predictions: Clean vs Outlier-Affected')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 3: Residual analysis\n",
        "    plt.subplot(2, 2, 3)\n",
        "    clean_residuals = (hunger - clean_pred.squeeze()).numpy()\n",
        "    outlier_residuals = (hunger - outlier_pred.squeeze()).numpy()\n",
        "\n",
        "    plt.scatter(hours[normal_mask].numpy(), clean_residuals[normal_mask],\n",
        "               alpha=0.6, color='green', label='Clean Model Residuals')\n",
        "    plt.scatter(hours[is_outlier].numpy(), clean_residuals[is_outlier],\n",
        "               alpha=0.8, color='red', label='Outlier Residuals', s=100, marker='x')\n",
        "    plt.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
        "    plt.xlabel('Hours Since Last Meal')\n",
        "    plt.ylabel('Residual (Actual - Predicted)')\n",
        "    plt.title('Residual Analysis: Clean Model')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 4: Parameter comparison\n",
        "    plt.subplot(2, 2, 4)\n",
        "    models = ['Clean Data', 'With Outliers']\n",
        "    weights = [clean_model.linear.weight.item(), outlier_model.linear.weight.item()]\n",
        "    biases = [clean_model.linear.bias.item(), outlier_model.linear.bias.item()]\n",
        "\n",
        "    x = range(len(models))\n",
        "    width = 0.35\n",
        "    plt.bar([i - width/2 for i in x], weights, width, label='Weight (slope)', alpha=0.7)\n",
        "    plt.bar([i + width/2 for i in x], [b/10 for b in biases], width, label='Bias/10 (intercept)', alpha=0.7)\n",
        "    plt.axhline(y=2.5, color='red', linestyle='--', alpha=0.7, label='True Weight=2.5')\n",
        "    plt.axhline(y=2.0, color='orange', linestyle='--', alpha=0.7, label='True Bias/10=2.0')\n",
        "    plt.xlabel('Model Type')\n",
        "    plt.ylabel('Parameter Value')\n",
        "    plt.title('Parameter Comparison')\n",
        "    plt.xticks(x, models)\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def analyze_outlier_impact(clean_model: nn.Module, outlier_model: nn.Module,\n",
        "                          clean_hours: torch.Tensor, clean_hunger: torch.Tensor,\n",
        "                          outlier_hours: torch.Tensor, outlier_hunger: torch.Tensor,\n",
        "                          is_outlier: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Quantify how outliers affect model performance and parameters.\n",
        "    \"\"\"\n",
        "    print(f\"\\\\nüìä OUTLIER IMPACT ANALYSIS:\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # Parameter comparison\n",
        "    clean_weight = clean_model.linear.weight.item()\n",
        "    clean_bias = clean_model.linear.bias.item()\n",
        "    outlier_weight = outlier_model.linear.weight.item()\n",
        "    outlier_bias = outlier_model.linear.bias.item()\n",
        "\n",
        "    print(f\"\\\\nüéØ PARAMETER COMPARISON:\")\n",
        "    print(f\"   True relationship: hunger = 2.5 √ó hours + 20\")\n",
        "    print(f\"   Clean model:      hunger = {clean_weight:.3f} √ó hours + {clean_bias:.3f}\")\n",
        "    print(f\"   Outlier model:    hunger = {outlier_weight:.3f} √ó hours + {outlier_bias:.3f}\")\n",
        "    print(f\"   Weight shift:     {abs(outlier_weight - clean_weight):.3f}\")\n",
        "    print(f\"   Bias shift:       {abs(outlier_bias - clean_bias):.3f}\")\n",
        "\n",
        "    # Performance on clean data\n",
        "    with torch.no_grad():\n",
        "        clean_pred_clean = clean_model(clean_hours)\n",
        "        outlier_pred_clean = outlier_model(clean_hours)\n",
        "\n",
        "        clean_mse = nn.MSELoss()(clean_pred_clean, clean_hunger).item()\n",
        "        outlier_mse = nn.MSELoss()(outlier_pred_clean, clean_hunger).item()\n",
        "\n",
        "    print(f\"\\\\nüéØ PERFORMANCE ON CLEAN DATA:\")\n",
        "    print(f\"   Clean model MSE:   {clean_mse:.2f}\")\n",
        "    print(f\"   Outlier model MSE: {outlier_mse:.2f}\")\n",
        "    print(f\"   Performance drop:  {((outlier_mse - clean_mse) / clean_mse * 100):.1f}%\")\n",
        "\n",
        "    # Outlier influence\n",
        "    normal_mask = ~is_outlier\n",
        "    outlier_fraction = is_outlier.sum().item() / len(is_outlier)\n",
        "\n",
        "    print(f\"\\\\nüö® OUTLIER INFLUENCE:\")\n",
        "    print(f\"   Outlier fraction: {outlier_fraction:.1%}\")\n",
        "    print(f\"   Normal points: {normal_mask.sum().item()}\")\n",
        "    print(f\"   Outlier points: {is_outlier.sum().item()}\")\n",
        "\n",
        "    # Residual analysis\n",
        "    with torch.no_grad():\n",
        "        clean_residuals = (clean_hunger - clean_model(clean_hours)).abs()\n",
        "        outlier_residuals = (outlier_hunger - outlier_model(outlier_hours)).abs()\n",
        "\n",
        "        normal_residuals = outlier_residuals[normal_mask]\n",
        "        outlier_point_residuals = outlier_residuals[is_outlier]\n",
        "\n",
        "    print(f\"\\\\nüìà RESIDUAL ANALYSIS:\")\n",
        "    print(f\"   Normal points avg residual: {normal_residuals.mean().item():.2f}\")\n",
        "    print(f\"   Outlier points avg residual: {outlier_point_residuals.mean().item():.2f}\")\n",
        "    print(f\"   Outlier residuals {(outlier_point_residuals.mean() / normal_residuals.mean()).item():.1f}x larger\")\n",
        "\n",
        "# Generate clean data for comparison\n",
        "print(\"üßò First, let's establish a baseline with clean data:\")\n",
        "clean_hours, clean_hunger = generate_cat_feeding_data(n_observations=200, chaos_level=0.1)\n",
        "\n",
        "# Train model on clean data\n",
        "clean_model = CatHungerPredictor(input_features=1)\n",
        "clean_losses = train_cat_predictor(clean_model, clean_hours, clean_hunger,\n",
        "                                 epochs=1000, learning_rate=0.01)\n",
        "\n",
        "print(\"\\\\nüå™Ô∏è Now, let's see what happens with He-Ao-World's mysterious incidents:\")\n",
        "\n",
        "# Generate data with different outlier levels\n",
        "outlier_levels = [0.05, 0.1, 0.2]  # 5%, 10%, 20% outliers\n",
        "outlier_models = []\n",
        "outlier_data = []\n",
        "\n",
        "for outlier_frac in outlier_levels:\n",
        "    print(f\"\\\\nüé≠ Testing with {outlier_frac:.0%} outliers...\")\n",
        "\n",
        "    # Generate outlier data\n",
        "    outlier_hours, outlier_hunger, is_outlier = generate_data_with_outliers(\n",
        "        n_observations=200, outlier_fraction=outlier_frac, chaos_level=0.1\n",
        "    )\n",
        "\n",
        "    # Train model on outlier data\n",
        "    outlier_model = CatHungerPredictor(input_features=1)\n",
        "    outlier_losses = train_cat_predictor(outlier_model, outlier_hours, outlier_hunger,\n",
        "                                       epochs=1000, learning_rate=0.01)\n",
        "\n",
        "    outlier_models.append(outlier_model)\n",
        "    outlier_data.append((outlier_hours, outlier_hunger, is_outlier))\n",
        "\n",
        "    print(f\"   Outlier incidents: {is_outlier.sum().item()}\")\n",
        "    print(f\"   Final training loss: {outlier_losses[-1]:.2f}\")\n",
        "\n",
        "# Detailed analysis for 10% outliers\n",
        "print(\"\\\\nüîç DETAILED ANALYSIS: 10% Outliers\")\n",
        "selected_model = outlier_models[1]  # 10% outliers\n",
        "selected_hours, selected_hunger, selected_outliers = outlier_data[1]\n",
        "\n",
        "# Visualize the impact\n",
        "visualize_outlier_impact(selected_hours, selected_hunger, selected_outliers,\n",
        "                        clean_model, selected_model)\n",
        "\n",
        "# Quantitative analysis\n",
        "analyze_outlier_impact(clean_model, selected_model,\n",
        "                      clean_hours, clean_hunger,\n",
        "                      selected_hours, selected_hunger, selected_outliers)\n",
        "\n",
        "print(\"\\\\nüß† PHILOSOPHICAL REFLECTION:\")\n",
        "print(\"He-Ao-World sighs: 'I see now that my clumsiness reveals a deeper truth...'\")\n",
        "print(\"\\\\nüí° KEY INSIGHTS:\")\n",
        "print(\"   ‚Ä¢ Linear regression is sensitive to outliers - they can drastically shift the line\")\n",
        "print(\"   ‚Ä¢ A few extreme points can dominate many normal observations\")\n",
        "print(\"   ‚Ä¢ Real-world data always contains unexpected events\")\n",
        "print(\"   ‚Ä¢ Simple models struggle with data that doesn't follow their assumptions\")\n",
        "print(\"\\\\nüé≠ MASTER'S WISDOM:\")\n",
        "print(\"Master Pai-Torch nods gravely: 'The straight path works well in a perfect world,\")\n",
        "print(\"but life is full of unexpected turns. This limitation will guide you toward\")\n",
        "print(\"more robust methods in your future training.'\")\n",
        "\n",
        "# SUCCESS CRITERIA\n",
        "print(\"\\\\n‚ö° MASTERY TRIAL:\")\n",
        "print(\"üéØ Can you identify which incidents are outliers by looking at the residuals?\")\n",
        "print(\"üéØ Do you understand why the outlier model performs worse on clean data?\")\n",
        "print(\"üéØ Can you explain why linear regression is fundamentally limited by outliers?\")\n",
        "\n",
        "print(\"\\\\n‚ú® Understanding these limitations is the first step toward mastery!\")\n",
        "print(\"üöÄ In future dans, you'll learn robust techniques that handle outliers better.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4vFGAOhY3Ry"
      },
      "source": [
        "## üî• CORRECTING YOUR FORM: A STANCE IMBALANCE\n",
        "\n",
        "*Master Pai-Torch observes your training ritual with ancient eyes*\n",
        "\n",
        "\"Your eager mind races ahead of your disciplined form, grasshopper. See how your gradient flow stance wavers? A previous disciple left this flawed training ritual behind. The form has become unsteady - can you restore proper technique?\"\n",
        "\n",
        "*The master gestures toward a scroll containing corrupted training code*\n",
        "\n",
        "\"Study this broken ritual carefully. The Gradient Spirits grow angry when not properly dismissed, and the loss may never converge. Your task is to identify the flaw and restore harmony to the training process.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hP9chxSOY3Ry"
      },
      "outputs": [],
      "source": [
        "def unsteady_training(model, features, target, epochs=1000):\n",
        "    \"\"\"This training stance has lost its balance - your form needs correction! ü•ã\"\"\"\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Forward pass\n",
        "        predictions = model(features)\n",
        "        loss = criterion(predictions, target)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(f'Epoch {epoch}: Loss = {loss.item():.4f}')\n",
        "\n",
        "    return model, losses\n",
        "\n",
        "# Test the unsteady training\n",
        "print(\"ü•ã Testing the unsteady training ritual...\")\n",
        "print(\"Master Pai-Torch warns: 'Watch carefully - something is amiss with this technique!'\")\n",
        "\n",
        "broken_model = CatHungerPredictor(input_features=1)\n",
        "broken_model, broken_losses = unsteady_training(broken_model, hours_since_meal, hunger_levels, epochs=500)\n",
        "\n",
        "# Compare with proper training\n",
        "print(\"\\nüßò Now witness the proper training stance:\")\n",
        "proper_model = CatHungerPredictor(input_features=1)\n",
        "proper_losses = train_cat_predictor(proper_model, hours_since_meal, hunger_levels, epochs=500)\n",
        "\n",
        "# Visualize the difference\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(broken_losses, label='Unsteady Training', color='red', alpha=0.7)\n",
        "plt.plot(proper_losses, label='Proper Training', color='green', alpha=0.7)\n",
        "plt.title('Training Loss Comparison')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.yscale('log')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(broken_losses[-100:], label='Unsteady (last 100 epochs)', color='red')\n",
        "plt.plot(proper_losses[-100:], label='Proper (last 100 epochs)', color='green')\n",
        "plt.title('Final Training Behavior')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "with torch.no_grad():\n",
        "    broken_pred = broken_model(hours_since_meal)\n",
        "    proper_pred = proper_model(hours_since_meal)\n",
        "\n",
        "plt.scatter(hours_since_meal.numpy(), hunger_levels.numpy(), alpha=0.5, label='True Data')\n",
        "sorted_indices = torch.argsort(hours_since_meal.squeeze())\n",
        "sorted_hours = hours_since_meal[sorted_indices]\n",
        "plt.plot(sorted_hours.numpy(), broken_pred[sorted_indices].numpy(), 'r--',\n",
        "         label='Unsteady Predictions', linewidth=2)\n",
        "plt.plot(sorted_hours.numpy(), proper_pred[sorted_indices].numpy(), 'g-',\n",
        "         label='Proper Predictions', linewidth=2)\n",
        "plt.title('Final Predictions')\n",
        "plt.xlabel('Hours Since Meal')\n",
        "plt.ylabel('Hunger Level')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüìä DEBUGGING RESULTS:\")\n",
        "print(f\"   Unsteady final loss: {broken_losses[-1]:.4f}\")\n",
        "print(f\"   Proper final loss: {proper_losses[-1]:.4f}\")\n",
        "print(f\"   Loss difference: {broken_losses[-1] - proper_losses[-1]:.4f}\")\n",
        "\n",
        "# DEBUGGING CHALLENGE: Can you spot the critical error?\n",
        "print(\"\\nüîç DEBUGGING CHALLENGE:\")\n",
        "print(\"‚ùì Can you identify the critical error in the unsteady_training function?\")\n",
        "print(\"üí° HINT: The Gradient Spirits are not being properly dismissed between cycles\")\n",
        "print(\"üß† MASTER'S WISDOM: 'The undisciplined mind accumulates old thoughts,'\")\n",
        "print(\"   'just as the untrained gradient accumulates old directions.'\")\n",
        "print(\"\\nüéØ SOLUTION: The missing line is optimizer.zero_grad() before loss.backward()\")\n",
        "print(\"   Without this, gradients accumulate from previous iterations, causing instability!\")\n",
        "print(\"\\nüéâ Understanding this fundamental mistake is your first step toward PyTorch mastery!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPbQKJKFY3Rz"
      },
      "source": [
        "## üèÜ COMPLETION CEREMONY\n",
        "\n",
        "*Master Pai-Torch rises and bows respectfully*\n",
        "\n",
        "\"Congratulations, young grasshopper. You have successfully completed your first kata in the Temple of Neural Networks. Through Suki's simple feeding patterns, you have learned the fundamental mysteries that underlie all neural arts:\n",
        "\n",
        "**Sacred Knowledge Acquired:**\n",
        "- **Tensor Mastery**: You can create and manipulate PyTorch tensors with confidence\n",
        "- **Linear Wisdom**: You understand how neural networks transform input to output\n",
        "- **Gradient Discipline**: You have mastered the sacred training loop and gradient management\n",
        "- **Loss Understanding**: You can measure and minimize prediction errors\n",
        "- **Convergence Patience**: You know how to train models with various strategies\n",
        "- **Decision Boundaries**: You understand the difference between prediction and action\n",
        "\n",
        "**The Path Forward:**\n",
        "Your journey has just begun. The linear relationship you mastered here is the foundation for all deeper mysteries. As you progress to Dan 2, you will learn to handle more complex patterns, multiple layers, and sophisticated techniques.\n",
        "\n",
        "**Final Wisdom:**\n",
        "Remember always: every complex neural network, no matter how sophisticated, is built upon the simple principles you practiced here. The gradient flows, the loss decreases, and wisdom emerges from the dance between prediction and reality.\n",
        "\n",
        "üê± *Suki purrs approvingly from her perch, as if to say: \"You are ready for greater challenges, young neural warrior.\"*\n",
        "\n",
        "üèÆ **May your gradients flow smoothly and your losses converge swiftly!** üèÆ\""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
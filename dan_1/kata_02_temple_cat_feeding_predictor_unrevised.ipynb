{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ruliana/pytorch-katas/blob/main/dan_1/kata_02_temple_cat_feeding_predictor_unrevised.ipynb)\n",
    "\n",
    "## üèÆ The Ancient Scroll Unfurls üèÆ\n",
    "\n",
    "THE SACRED HUNGER ORACLE: MASTERING SUKI'S FEEDING PROPHECY\n",
    "\n",
    "Dan Level: 1 (Temple Sweeper) | Time: 45 minutes | Sacred Arts: Linear Regression, Gradient Descent, Training Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìú THE CHALLENGE\n",
    "\n",
    "In the quiet hours before dawn, as you sweep the temple courtyard, you notice something peculiar. **Suki**, the sacred temple cat, appears at the feeding bowl with uncanny precision - never too early, never too late, always exactly when their hunger reaches the perfect threshold.\n",
    "\n",
    "**Master Pai-Torch** emerges from the shadows, observing your curious gaze.\n",
    "\n",
    "*\"Ah, young grasshopper, you have discovered the first hidden technique. The wise observe patterns where others see chaos. Tell me, what do you see in the sacred cat's behavior?\"*\n",
    "\n",
    "You share your observations: \"Master, Suki always appears when they've been without food for a certain time. It's as if there's a mathematical relationship between hours and hunger.\"\n",
    "\n",
    "*Master Pai-Torch nods slowly.* \"Indeed. The ancient temple keepers knew this secret - **hunger grows linearly with time**. But the true art lies not in knowing this truth, but in teaching a neural network to discover it. This is your first lesson in the sacred art of **gradient descent**.\"\n",
    "\n",
    "He gestures toward the feeding bowl. \"Your task is to create a mystical artifact - a neural network that can predict Suki's exact hunger level based on hours since their last meal. Master this, and you will understand the fundamental flow of gradients through the sacred tensor realms.\"\n",
    "\n",
    "## üéØ THE SACRED OBJECTIVES\n",
    "\n",
    "- [ ] Create a linear neural network that learns the relationship between time and hunger\n",
    "- [ ] Implement proper gradient descent training with loss computation\n",
    "- [ ] Achieve convergence with final loss below 50\n",
    "- [ ] Understand how gradients flow backward through your network\n",
    "- [ ] Visualize the learned relationship and validate predictions\n",
    "- [ ] Master the sacred ritual of `optimizer.zero_grad()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé≤ THE SACRED IMPORTS\n",
    "\n",
    "First, we gather the mystical tools needed for our neural arts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "# For reproducible mystical experiences\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"üîÆ The sacred tools are ready, grasshopper!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üê± THE SACRED DATA GENERATION SCROLL\n",
    "\n",
    "Master Pai-Torch whispers the ancient secret: *\"The sacred relationship follows the pattern: `hunger_level = 2.5 * hours_since_last_meal + 20`. When hunger exceeds 70, Suki appears at the bowl.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cat_feeding_data(n_observations: int = 100, chaos_level: float = 0.1,\n",
    "                             sacred_seed: int = 42) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Generate observations of Suki's feeding patterns.\n",
    "\n",
    "    Ancient wisdom suggests: hunger_level = 2.5 * hours_since_last_meal + 20\n",
    "    When hunger_level > 70, Suki appears at the food bowl.\n",
    "\n",
    "    Args:\n",
    "        n_observations: Number of Suki sightings to simulate\n",
    "        chaos_level: Amount of feline unpredictability (0.0 = perfectly predictable cat, 1.0 = pure chaos)\n",
    "        sacred_seed: Ensures consistent randomness for reproducible wisdom\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (hours_since_last_meal, hunger_level) as sacred tensors\n",
    "    \"\"\"\n",
    "    torch.manual_seed(sacred_seed)\n",
    "    np.random.seed(sacred_seed)\n",
    "\n",
    "    # Suki can go 0-30 hours between meals (she's very dramatic)\n",
    "    hours_since_meal = torch.rand(n_observations, 1) * 30\n",
    "\n",
    "    # The sacred relationship known to ancient cat scholars\n",
    "    base_hunger = 20\n",
    "    hunger_per_hour = 2.5\n",
    "\n",
    "    hunger_levels = hunger_per_hour * hours_since_meal.squeeze() + base_hunger\n",
    "\n",
    "    # Add feline chaos (cats are unpredictable creatures)\n",
    "    chaos = torch.randn(n_observations) * chaos_level * hunger_levels.std()\n",
    "    hunger_levels = hunger_levels + chaos\n",
    "\n",
    "    # Even mystical cats have limits\n",
    "    hunger_levels = torch.clamp(hunger_levels, 0, 100)\n",
    "\n",
    "    return hours_since_meal, hunger_levels.unsqueeze(1)\n",
    "\n",
    "def visualize_cat_wisdom(hours: torch.Tensor, hunger: torch.Tensor,\n",
    "                        predictions: torch.Tensor = None):\n",
    "    \"\"\"Display the sacred patterns of Suki's appetite.\"\"\"\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.scatter(hours.numpy(), hunger.numpy(), alpha=0.6, color='purple',\n",
    "                label='Suki\\'s Actual Hunger Levels')\n",
    "\n",
    "    if predictions is not None:\n",
    "        sorted_indices = torch.argsort(hours.squeeze())\n",
    "        sorted_hours = hours[sorted_indices]\n",
    "        sorted_predictions = predictions[sorted_indices]\n",
    "        plt.plot(sorted_hours.numpy(), sorted_predictions.detach().numpy(),\n",
    "                'gold', linewidth=3, label='Your Mystical Predictions')\n",
    "\n",
    "    plt.axhline(y=70, color='red', linestyle='--', alpha=0.7,\n",
    "                label='Sacred Feeding Threshold (Suki Appears!)')\n",
    "    plt.xlabel('Hours Since Last Meal')\n",
    "    plt.ylabel('Suki\\'s Hunger Level')\n",
    "    plt.title('üê± The Mysteries of Temple Cat Appetite üê±')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.ylim(0, 100)\n",
    "    plt.show()\n",
    "\n",
    "# Generate the sacred data\n",
    "hours, hunger = generate_cat_feeding_data(n_observations=150, chaos_level=0.1)\n",
    "visualize_cat_wisdom(hours, hunger)\n",
    "\n",
    "print(f\"üìä Generated {len(hours)} observations of Suki's feeding patterns\")\n",
    "print(f\"üïê Hour range: {hours.min():.1f} to {hours.max():.1f}\")\n",
    "print(f\"üçΩÔ∏è Hunger range: {hunger.min():.1f} to {hunger.max():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíÉ FIRST MOVEMENTS: THE HUNGER PREDICTION ARTIFACT\n",
    "\n",
    "Master Pai-Torch gestures toward an empty scroll. *\"Now, young grasshopper, create the sacred artifact that will learn Suki's patterns. A simple linear network - one that transforms hours into hunger through the mystical art of matrix multiplication.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatHungerPredictor(nn.Module):\n",
    "    \"\"\"A mystical artifact for understanding feline appetite patterns.\"\"\"\n",
    "\n",
    "    def __init__(self, input_features: int = 1):\n",
    "        super(CatHungerPredictor, self).__init__()\n",
    "        # TODO: Create the Linear layer\n",
    "        # Hint: torch.nn.Linear transforms input energy into output wisdom\n",
    "        # It needs to know: input_features -> output_features (1 for hunger level)\n",
    "        self.linear = None\n",
    "\n",
    "    def forward(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Channel your understanding through the mystical network.\"\"\"\n",
    "        # TODO: Pass the input through your Linear layer\n",
    "        # Remember: even cats follow mathematical laws\n",
    "        return None\n",
    "\n",
    "# Create your mystical artifact\n",
    "model = CatHungerPredictor(input_features=1)\n",
    "print(\"üèõÔ∏è Your hunger prediction artifact has been forged!\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())} sacred numbers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåä THE SACRED TRAINING RITUAL\n",
    "\n",
    "Master Pai-Torch closes his eyes in concentration. *\"Now comes the most crucial lesson, grasshopper. The network must learn through the sacred flow of gradients. Each cycle, it makes a prediction, measures its error, and adjusts its understanding. This is the essence of all neural wisdom.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hunger_predictor(model: nn.Module, features: torch.Tensor, target: torch.Tensor,\n",
    "                          epochs: int = 1000, learning_rate: float = 0.01) -> list:\n",
    "    \"\"\"\n",
    "    Train the cat hunger prediction model through the sacred ritual of gradient descent.\n",
    "\n",
    "    Args:\n",
    "        model: Your mystical hunger predictor\n",
    "        features: Hours since last meal (input)\n",
    "        target: Actual hunger levels (what we want to predict)\n",
    "        epochs: Number of training cycles\n",
    "        learning_rate: How big steps to take during learning\n",
    "\n",
    "    Returns:\n",
    "        List of loss values during training\n",
    "    \"\"\"\n",
    "    # TODO: Choose your loss calculation method\n",
    "    # Hint: Mean Squared Error is favored by the ancient masters\n",
    "    # It measures the average squared difference between predictions and truth\n",
    "    criterion = None\n",
    "\n",
    "    # TODO: Choose your parameter updating method\n",
    "    # Hint: SGD (Stochastic Gradient Descent) is the traditional path\n",
    "    # It needs to know: which parameters to update and how fast to learn\n",
    "    optimizer = None\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # TODO: CRITICAL - Clear the gradient spirits from previous cycle\n",
    "        # Hint: The spirits accumulate if not banished properly\n",
    "        # Use: optimizer.zero_grad()\n",
    "\n",
    "        # TODO: Forward pass - get predictions from your model\n",
    "        # Hint: Pass the features through your model\n",
    "        predictions = None\n",
    "\n",
    "        # TODO: Compute the loss\n",
    "        # Hint: Use your criterion to compare predictions with target\n",
    "        loss = None\n",
    "\n",
    "        # TODO: Backward pass - compute gradients\n",
    "        # Hint: loss.backward() summons the gradient spirits\n",
    "\n",
    "        # TODO: Update parameters\n",
    "        # Hint: optimizer.step() applies the gradient wisdom\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Report progress to Master Pai-Torch\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "            if loss.item() < 10:\n",
    "                print(\"üí´ The Gradient Spirits smile upon your progress!\")\n",
    "\n",
    "    return losses\n",
    "\n",
    "# Begin the sacred training ritual\n",
    "print(\"üßò Beginning the sacred training ritual...\")\n",
    "loss_history = train_hunger_predictor(model, hours, hunger, epochs=1000, learning_rate=0.01)\n",
    "\n",
    "# Visualize the learning journey\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(loss_history)\n",
    "plt.title('üåä The Sacred Loss Journey')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚ú® Training complete! Final loss: {loss_history[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÆ REVEALING THE LEARNED WISDOM\n",
    "\n",
    "Master Pai-Torch nods approvingly. *\"Now, grasshopper, let us see what your network has learned. The true test is not just low loss, but understanding the sacred relationship itself.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with your trained model\n",
    "model.eval()  # Set to evaluation mode\n",
    "with torch.no_grad():\n",
    "    predictions = model(hours)\n",
    "\n",
    "# Visualize the learned relationship\n",
    "visualize_cat_wisdom(hours, hunger, predictions)\n",
    "\n",
    "# Examine the learned parameters\n",
    "learned_weight = model.linear.weight.item()\n",
    "learned_bias = model.linear.bias.item()\n",
    "\n",
    "print(f\"üéØ Your network learned:\")\n",
    "print(f\"   Weight (hunger per hour): {learned_weight:.3f}\")\n",
    "print(f\"   Bias (base hunger): {learned_bias:.3f}\")\n",
    "print(f\"\\nüìö The ancient truth:\")\n",
    "print(f\"   True weight: 2.5\")\n",
    "print(f\"   True bias: 20\")\n",
    "print(f\"\\nüéâ Accuracy: Weight is {abs(learned_weight - 2.5):.3f} away from truth\")\n",
    "print(f\"           Bias is {abs(learned_bias - 20):.3f} away from truth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° THE TRIALS OF MASTERY\n",
    "\n",
    "Master Pai-Torch strokes his beard thoughtfully. *\"Before you can advance, you must prove your understanding through the sacred trials.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_your_wisdom(model):\n",
    "    \"\"\"Master Pai-Torch's evaluation of your understanding.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Test 1: Shape consistency\n",
    "    test_features = torch.tensor([[5.0], [10.0], [20.0]])\n",
    "    with torch.no_grad():\n",
    "        predictions = model(test_features)\n",
    "    assert predictions.shape == (3, 1), f\"Expected shape (3, 1), got {predictions.shape}\"\n",
    "    print(\"‚úÖ Shape test passed - your tensors align with the sacred geometry!\")\n",
    "\n",
    "    # Test 2: Parameter validation\n",
    "    weight = model.linear.weight.item()\n",
    "    bias = model.linear.bias.item()\n",
    "    \n",
    "    assert 2.0 <= weight <= 3.0, f\"Weight {weight:.2f} seems off - cats are more predictable!\"\n",
    "    assert 15 <= bias <= 25, f\"Bias {bias:.2f} - even well-fed cats have base hunger!\"\n",
    "    print(\"‚úÖ Parameter test passed - your network understands cat nature!\")\n",
    "\n",
    "    # Test 3: Logical predictions\n",
    "    with torch.no_grad():\n",
    "        pred_5h = model(torch.tensor([[5.0]])).item()\n",
    "        pred_10h = model(torch.tensor([[10.0]])).item()\n",
    "        pred_20h = model(torch.tensor([[20.0]])).item()\n",
    "    \n",
    "    assert pred_5h < pred_10h < pred_20h, \"Hunger should increase with time!\"\n",
    "    print(\"‚úÖ Logic test passed - longer waits mean hungrier cats!\")\n",
    "\n",
    "    print(\"\\nüéâ Master Pai-Torch nods with approval - your understanding grows!\")\n",
    "    print(f\"   Predicted hunger after 5 hours: {pred_5h:.1f}\")\n",
    "    print(f\"   Predicted hunger after 10 hours: {pred_10h:.1f}\")\n",
    "    print(f\"   Predicted hunger after 20 hours: {pred_20h:.1f}\")\n",
    "\n",
    "# Test your wisdom\n",
    "test_your_wisdom(model)\n",
    "\n",
    "# Final mastery check\n",
    "final_loss = loss_history[-1]\n",
    "print(f\"\\nüìä MASTERY EVALUATION:\")\n",
    "print(f\"   Final Loss: {final_loss:.4f} {'‚úÖ' if final_loss < 50 else '‚ùå'} (Target: < 50)\")\n",
    "print(f\"   Weight Accuracy: {abs(learned_weight - 2.5):.3f} {'‚úÖ' if abs(learned_weight - 2.5) < 0.5 else '‚ùå'} (Target: < 0.5)\")\n",
    "print(f\"   Bias Accuracy: {abs(learned_bias - 20):.3f} {'‚úÖ' if abs(learned_bias - 20) < 5 else '‚ùå'} (Target: < 5)\")\n",
    "\n",
    "if final_loss < 50 and abs(learned_weight - 2.5) < 0.5 and abs(learned_bias - 20) < 5:\n",
    "    print(\"\\nüèÜ CONGRATULATIONS! You have mastered the first sacred art!\")\n",
    "    print(\"üê± Suki purrs approvingly - your network understands cat wisdom!\")\n",
    "else:\n",
    "    print(\"\\nüîÑ Not quite there yet, grasshopper. Review your training ritual.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üå∏ THE FOUR PATHS OF MASTERY: PROGRESSIVE EXTENSIONS\n",
    "\n",
    "Master Pai-Torch gestures toward four different paths leading from the courtyard. *\"Your foundation is strong, but true mastery requires walking different paths. Each will teach you new aspects of the sacred arts.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension 1: Cook Oh-Pai-Timizer's Batch Cooking Wisdom\n",
    "*\"A good cook knows that batch size affects the final dish!\"*\n",
    "\n",
    "Cook Oh-Pai-Timizer bustles over, wooden spoon in hand.\n",
    "\n",
    "*\"Ah, grasshopper! I see you've mastered feeding one cat at a time. But what happens when you need to predict hunger for multiple cats simultaneously? In my kitchen, efficiency comes from preparing multiple servings at once!\"*\n",
    "\n",
    "**NEW CONCEPTS:** Batch processing, tensor shapes, vectorized operations  \n",
    "**DIFFICULTY:** +15% (still Dan 1, but with batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multi_cat_data(n_cats: int = 5, observations_per_cat: int = 50, \n",
    "                           sacred_seed: int = 42) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Generate feeding data for multiple temple cats at once.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (batch_hours, batch_hunger_levels)\n",
    "        Shape: (n_cats * observations_per_cat, 1) for both tensors\n",
    "    \"\"\"\n",
    "    torch.manual_seed(sacred_seed)\n",
    "    \n",
    "    all_hours = []\n",
    "    all_hunger = []\n",
    "    \n",
    "    for cat_id in range(n_cats):\n",
    "        # Each cat has slightly different eating patterns\n",
    "        cat_chaos = 0.05 + (cat_id * 0.02)  # Some cats are more predictable\n",
    "        hours, hunger = generate_cat_feeding_data(\n",
    "            n_observations=observations_per_cat, \n",
    "            chaos_level=cat_chaos,\n",
    "            sacred_seed=sacred_seed + cat_id\n",
    "        )\n",
    "        all_hours.append(hours)\n",
    "        all_hunger.append(hunger)\n",
    "    \n",
    "    # Combine all cats into one large batch\n",
    "    batch_hours = torch.cat(all_hours, dim=0)\n",
    "    batch_hunger = torch.cat(all_hunger, dim=0)\n",
    "    \n",
    "    return batch_hours, batch_hunger\n",
    "\n",
    "# Generate multi-cat data\n",
    "batch_hours, batch_hunger = generate_multi_cat_data(n_cats=5, observations_per_cat=50)\n",
    "\n",
    "print(f\"üê± Generated data for 5 temple cats\")\n",
    "print(f\"üìä Batch shape: {batch_hours.shape[0]} total observations\")\n",
    "\n",
    "# Test your existing model on batch data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    batch_predictions = model(batch_hours)\n",
    "\n",
    "# Visualize batch predictions\n",
    "visualize_cat_wisdom(batch_hours, batch_hunger, batch_predictions)\n",
    "\n",
    "# Calculate batch performance\n",
    "batch_loss = nn.MSELoss()(batch_predictions, batch_hunger)\n",
    "print(f\"\\nüçΩÔ∏è Batch prediction loss: {batch_loss.item():.4f}\")\n",
    "print(f\"üéØ SUCCESS: Your model processes multiple cats simultaneously!\")\n",
    "print(f\"   Batch processing is a fundamental skill for efficient neural networks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension 2: He-Ao-World's Measurement Mix-up\n",
    "*\"These old eyes sometimes read the measuring scrolls incorrectly...\"*\n",
    "\n",
    "He-Ao-World shuffles over, looking apologetic.\n",
    "\n",
    "*\"Oh dear! I was recording Suki's feeding times and... well, I might have mixed up some of the measurements. Some are in minutes instead of hours, and others might be twice what they should be. The data looks a bit... chaotic now.\"*\n",
    "\n",
    "**NEW CONCEPTS:** Data normalization, feature scaling, handling inconsistent units  \n",
    "**DIFFICULTY:** +25% (still Dan 1, but messier data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_messy_data(clean_hours: torch.Tensor, clean_hunger: torch.Tensor,\n",
    "                     contamination_rate: float = 0.3) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    He-Ao-World's 'accidents' create measurement inconsistencies.\n",
    "    \n",
    "    Args:\n",
    "        contamination_rate: Fraction of data to corrupt\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (messy_hours, messy_hunger)\n",
    "    \"\"\"\n",
    "    messy_hours = clean_hours.clone()\n",
    "    messy_hunger = clean_hunger.clone()\n",
    "    \n",
    "    n_contaminated = int(len(messy_hours) * contamination_rate)\n",
    "    contaminated_indices = torch.randperm(len(messy_hours))[:n_contaminated]\n",
    "    \n",
    "    for idx in contaminated_indices:\n",
    "        accident_type = torch.randint(0, 3, (1,)).item()\n",
    "        \n",
    "        if accident_type == 0:\n",
    "            # Measured in minutes instead of hours\n",
    "            messy_hours[idx] = messy_hours[idx] * 60\n",
    "        elif accident_type == 1:\n",
    "            # Double measurement error\n",
    "            messy_hours[idx] = messy_hours[idx] * 2\n",
    "        else:\n",
    "            # Random extra noise\n",
    "            messy_hours[idx] = messy_hours[idx] + torch.randn(1) * 5\n",
    "    \n",
    "    return messy_hours, messy_hunger\n",
    "\n",
    "def normalize_feeding_data(hours_since_meal: torch.Tensor, \n",
    "                          hunger_levels: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, dict]:\n",
    "    \"\"\"\n",
    "    Clean and normalize the feeding data to handle measurement inconsistencies.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (normalized_hours, normalized_hunger, normalization_params)\n",
    "    \"\"\"\n",
    "    # Calculate normalization parameters\n",
    "    hours_mean = hours_since_meal.mean()\n",
    "    hours_std = hours_since_meal.std()\n",
    "    hunger_mean = hunger_levels.mean()\n",
    "    hunger_std = hunger_levels.std()\n",
    "    \n",
    "    # Normalize (subtract mean, divide by std)\n",
    "    normalized_hours = (hours_since_meal - hours_mean) / hours_std\n",
    "    normalized_hunger = (hunger_levels - hunger_mean) / hunger_std\n",
    "    \n",
    "    # Store parameters for denormalization later\n",
    "    params = {\n",
    "        'hours_mean': hours_mean,\n",
    "        'hours_std': hours_std,\n",
    "        'hunger_mean': hunger_mean,\n",
    "        'hunger_std': hunger_std\n",
    "    }\n",
    "    \n",
    "    return normalized_hours, normalized_hunger, params\n",
    "\n",
    "# Create messy data\n",
    "messy_hours, messy_hunger = create_messy_data(hours, hunger, contamination_rate=0.3)\n",
    "\n",
    "print(\"ü§¶ He-Ao-World's measurement mishaps:\")\n",
    "print(f\"   Original hour range: {hours.min():.1f} to {hours.max():.1f}\")\n",
    "print(f\"   Messy hour range: {messy_hours.min():.1f} to {messy_hours.max():.1f}\")\n",
    "\n",
    "# Visualize the chaos\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(hours.numpy(), hunger.numpy(), alpha=0.6, color='blue', label='Clean Data')\n",
    "plt.title('Original Clean Data')\n",
    "plt.xlabel('Hours')\n",
    "plt.ylabel('Hunger')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(messy_hours.numpy(), messy_hunger.numpy(), alpha=0.6, color='red', label='Messy Data')\n",
    "plt.title('He-Ao-World\\'s Messy Data')\n",
    "plt.xlabel('Hours (inconsistent units!)')\n",
    "plt.ylabel('Hunger')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Normalize the messy data\n",
    "norm_hours, norm_hunger, norm_params = normalize_feeding_data(messy_hours, messy_hunger)\n",
    "\n",
    "# Train a new model on normalized data\n",
    "normalized_model = CatHungerPredictor(input_features=1)\n",
    "normalized_losses = train_hunger_predictor(normalized_model, norm_hours, norm_hunger, \n",
    "                                         epochs=800, learning_rate=0.01)\n",
    "\n",
    "print(f\"\\nüßπ Normalized training complete!\")\n",
    "print(f\"   Final loss with normalization: {normalized_losses[-1]:.4f}\")\n",
    "print(f\"   Original messy data would have loss: {nn.MSELoss()(model(messy_hours), messy_hunger).item():.4f}\")\n",
    "print(f\"\\nüéØ SUCCESS: Normalization makes messy data trainable!\")\n",
    "print(f\"   This is a crucial skill for handling real-world data inconsistencies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension 3: Master Pai-Torch's Patience Teaching\n",
    "*\"The eager student trains too quickly and learns too little.\"*\n",
    "\n",
    "Master Pai-Torch sits in contemplative silence.\n",
    "\n",
    "*\"Young grasshopper, I observe your training ritual rushes like a mountain stream. But wisdom comes to those who vary their pace. Sometimes we must step boldly, sometimes cautiously, sometimes we must rest entirely.\"*\n",
    "\n",
    "**NEW CONCEPTS:** Learning rate scheduling, early stopping, training patience  \n",
    "**DIFFICULTY:** +35% (still Dan 1, but smarter training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patient_training_ritual(model: nn.Module, features: torch.Tensor, target: torch.Tensor,\n",
    "                           epochs: int = 2000, initial_lr: float = 0.1, \n",
    "                           patience: int = 100) -> Tuple[list, bool]:\n",
    "    \"\"\"\n",
    "    Train with patience and adaptive learning rate.\n",
    "    \n",
    "    Args:\n",
    "        patience: Stop training if loss doesn't improve for this many epochs\n",
    "        initial_lr: Starting learning rate\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (loss_history, stopped_early)\n",
    "    \"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=initial_lr)\n",
    "    \n",
    "    # Learning rate scheduler - reduces LR when loss plateaus\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=50, verbose=True\n",
    "    )\n",
    "    \n",
    "    losses = []\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(features)\n",
    "        loss = criterion(predictions, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        current_loss = loss.item()\n",
    "        losses.append(current_loss)\n",
    "        \n",
    "        # Update learning rate based on loss\n",
    "        scheduler.step(current_loss)\n",
    "        \n",
    "        # Check for improvement\n",
    "        if current_loss < best_loss:\n",
    "            best_loss = current_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\n‚è∞ Early stopping at epoch {epoch+1} - no improvement for {patience} epochs\")\n",
    "            print(f\"   Best loss achieved: {best_loss:.4f}\")\n",
    "            return losses, True\n",
    "        \n",
    "        # Progress reporting\n",
    "        if (epoch + 1) % 200 == 0:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {current_loss:.4f}, LR: {current_lr:.6f}')\n",
    "    \n",
    "    return losses, False\n",
    "\n",
    "# Compare patient vs rushed training\n",
    "print(\"üèÉ Training with rush (original method):\")\n",
    "rushed_model = CatHungerPredictor(input_features=1)\n",
    "rushed_losses = train_hunger_predictor(rushed_model, hours, hunger, epochs=1000, learning_rate=0.01)\n",
    "\n",
    "print(\"\\nüßò Training with patience (adaptive method):\")\n",
    "patient_model = CatHungerPredictor(input_features=1)\n",
    "patient_losses, stopped_early = patient_training_ritual(patient_model, hours, hunger,\n",
    "                                                       epochs=2000, initial_lr=0.1, patience=100)\n",
    "\n",
    "# Compare results\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(rushed_losses, label='Rushed Training', color='red')\n",
    "plt.plot(patient_losses, label='Patient Training', color='blue')\n",
    "plt.title('Training Comparison')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Show final predictions\n",
    "rushed_model.eval()\n",
    "patient_model.eval()\n",
    "with torch.no_grad():\n",
    "    rushed_pred = rushed_model(hours)\n",
    "    patient_pred = patient_model(hours)\n",
    "\n",
    "plt.scatter(hours.numpy(), hunger.numpy(), alpha=0.4, color='gray', label='True Data')\n",
    "plt.scatter(hours.numpy(), rushed_pred.numpy(), alpha=0.6, color='red', label='Rushed', s=10)\n",
    "plt.scatter(hours.numpy(), patient_pred.numpy(), alpha=0.6, color='blue', label='Patient', s=10)\n",
    "plt.title('Prediction Comparison')\n",
    "plt.xlabel('Hours')\n",
    "plt.ylabel('Hunger')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä PATIENCE COMPARISON:\")\n",
    "print(f\"   Rushed final loss: {rushed_losses[-1]:.4f}\")\n",
    "print(f\"   Patient final loss: {patient_losses[-1]:.4f}\")\n",
    "print(f\"   Epochs used: {len(rushed_losses)} vs {len(patient_losses)}\")\n",
    "print(f\"   Stopped early: {stopped_early}\")\n",
    "print(f\"\\nüéØ SUCCESS: Patient training often achieves better results!\")\n",
    "print(f\"   Learning rate adaptation and early stopping are essential skills.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension 4: Suki's Feeding Threshold Mystery\n",
    "*\"Understanding when the cat appears is as important as predicting hunger.\"*\n",
    "\n",
    "Suki sits majestically, then meows once.\n",
    "\n",
    "Master Pai-Torch translates: *\"The sacred cat says your linear wisdom is sound, but the true test is knowing when hunger becomes action. At what point does prediction become decision?\"*\n",
    "\n",
    "**NEW CONCEPTS:** Threshold analysis, decision boundaries, model interpretation  \n",
    "**DIFFICULTY:** +45% (still Dan 1, but thinking beyond prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feeding_threshold(model: nn.Module, features: torch.Tensor, target: torch.Tensor,\n",
    "                            threshold_candidates: list = [60, 65, 70, 75, 80]) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze how well your model predicts when Suki will actually appear.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of {threshold: accuracy_score}\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(features)\n",
    "    \n",
    "    threshold_results = {}\n",
    "    \n",
    "    for threshold in threshold_candidates:\n",
    "        # Model's binary prediction: will Suki appear?\n",
    "        pred_appears = (predictions > threshold).float()\n",
    "        \n",
    "        # Ground truth: does Suki actually appear?\n",
    "        true_appears = (target > threshold).float()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        correct_predictions = (pred_appears == true_appears).float()\n",
    "        accuracy = correct_predictions.mean().item()\n",
    "        \n",
    "        threshold_results[threshold] = accuracy\n",
    "    \n",
    "    return threshold_results\n",
    "\n",
    "def visualize_decision_boundary(model: nn.Module, features: torch.Tensor, \n",
    "                               target: torch.Tensor, best_threshold: float):\n",
    "    \"\"\"\n",
    "    Show where your model draws the line between \"hungry\" and \"will appear\"\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(features)\n",
    "    \n",
    "    # Create decision regions\n",
    "    pred_appears = predictions > best_threshold\n",
    "    true_appears = target > best_threshold\n",
    "    \n",
    "    # Classify predictions\n",
    "    true_positives = (pred_appears & true_appears).squeeze()\n",
    "    false_positives = (pred_appears & ~true_appears).squeeze()\n",
    "    true_negatives = (~pred_appears & ~true_appears).squeeze()\n",
    "    false_negatives = (~pred_appears & true_appears).squeeze()\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Plot the different prediction types\n",
    "    plt.scatter(features[true_positives].numpy(), target[true_positives].numpy(), \n",
    "               color='green', alpha=0.7, label='True Positive (Correctly predicted appearance)', s=50)\n",
    "    plt.scatter(features[false_positives].numpy(), target[false_positives].numpy(), \n",
    "               color='orange', alpha=0.7, label='False Positive (Predicted appearance wrongly)', s=50)\n",
    "    plt.scatter(features[true_negatives].numpy(), target[true_negatives].numpy(), \n",
    "               color='lightblue', alpha=0.7, label='True Negative (Correctly predicted no appearance)', s=50)\n",
    "    plt.scatter(features[false_negatives].numpy(), target[false_negatives].numpy(), \n",
    "               color='red', alpha=0.7, label='False Negative (Missed appearance)', s=50)\n",
    "    \n",
    "    # Plot model predictions as a line\n",
    "    sorted_indices = torch.argsort(features.squeeze())\n",
    "    sorted_features = features[sorted_indices]\n",
    "    sorted_predictions = predictions[sorted_indices]\n",
    "    plt.plot(sorted_features.numpy(), sorted_predictions.numpy(), 'black', linewidth=2, label='Model Predictions')\n",
    "    \n",
    "    # Plot decision threshold\n",
    "    plt.axhline(y=best_threshold, color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Decision Threshold = {best_threshold}')\n",
    "    \n",
    "    plt.xlabel('Hours Since Last Meal')\n",
    "    plt.ylabel('Hunger Level')\n",
    "    plt.title('üê± Decision Boundary Analysis: When Will Suki Appear?')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze different thresholds\n",
    "threshold_results = analyze_feeding_threshold(model, hours, hunger, \n",
    "                                            threshold_candidates=[60, 65, 70, 75, 80])\n",
    "\n",
    "# Find the best threshold\n",
    "best_threshold = max(threshold_results.keys(), key=lambda k: threshold_results[k])\n",
    "best_accuracy = threshold_results[best_threshold]\n",
    "\n",
    "print(\"üéØ THRESHOLD ANALYSIS RESULTS:\")\n",
    "for threshold, accuracy in threshold_results.items():\n",
    "    marker = \"‚≠ê\" if threshold == best_threshold else \"  \"\n",
    "    print(f\"{marker} Threshold {threshold}: {accuracy:.3f} accuracy\")\n",
    "\n",
    "print(f\"\\nüèÜ Best threshold: {best_threshold} with {best_accuracy:.3f} accuracy\")\n",
    "\n",
    "# Visualize the decision boundary\n",
    "visualize_decision_boundary(model, hours, hunger, best_threshold)\n",
    "\n",
    "# Calculate detailed metrics\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(hours)\n",
    "    \n",
    "pred_appears = predictions > best_threshold\n",
    "true_appears = hunger > best_threshold\n",
    "\n",
    "true_positives = (pred_appears & true_appears).sum().item()\n",
    "false_positives = (pred_appears & ~true_appears).sum().item()\n",
    "true_negatives = (~pred_appears & ~true_appears).sum().item()\n",
    "false_negatives = (~pred_appears & true_appears).sum().item()\n",
    "\n",
    "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "\n",
    "print(f\"\\nüìä DETAILED METRICS:\")\n",
    "print(f\"   Precision: {precision:.3f} (When we predict Suki will appear, how often are we right?)\")\n",
    "print(f\"   Recall: {recall:.3f} (When Suki actually appears, how often do we catch it?)\")\n",
    "print(f\"   True Positives: {true_positives} (Correctly predicted appearances)\")\n",
    "print(f\"   False Positives: {false_positives} (False alarms)\")\n",
    "print(f\"   True Negatives: {true_negatives} (Correctly predicted no appearance)\")\n",
    "print(f\"   False Negatives: {false_negatives} (Missed appearances)\")\n",
    "\n",
    "print(f\"\\nüéØ SUCCESS CRITERIA:\")\n",
    "print(f\"   Accuracy > 0.8: {'‚úÖ' if best_accuracy > 0.8 else '‚ùå'} ({best_accuracy:.3f})\")\n",
    "print(f\"   Precision > 0.7: {'‚úÖ' if precision > 0.7 else '‚ùå'} ({precision:.3f})\")\n",
    "print(f\"   Recall > 0.7: {'‚úÖ' if recall > 0.7 else '‚ùå'} ({recall:.3f})\")\n",
    "\n",
    "if best_accuracy > 0.8 and precision > 0.7 and recall > 0.7:\n",
    "    print(\"\\nüèÜ MASTERY ACHIEVED!\")\n",
    "    print(\"üê± You understand that prediction and decision-making are different skills!\")\n",
    "    print(\"üéØ You've learned to optimize thresholds for practical applications!\")\n",
    "else:\n",
    "    print(\"\\nüîÑ Good progress! Understanding decision boundaries is advanced knowledge.\")\n",
    "    print(\"   Try adjusting your model training or exploring different thresholds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî• CORRECTING YOUR FORM: A STANCE IMBALANCE\n",
    "\n",
    "Master Pai-Torch observes your training ritual with a careful eye. *\"Your eager mind races ahead of your disciplined form, grasshopper. See how your gradient flow stance wavers?\"*\n",
    "\n",
    "A previous disciple left this flawed training ritual. Your form has become unsteady - can you restore proper technique?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsteady_training(model: nn.Module, features: torch.Tensor, target: torch.Tensor, \n",
    "                     epochs: int = 1000) -> list:\n",
    "    \"\"\"This training stance has lost its balance - your form needs correction! ü•ã\"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        predictions = model(features)\n",
    "        loss = criterion(predictions, target)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch {epoch}: Loss = {loss.item():.4f}')\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Test the flawed training\n",
    "print(\"ü•ã Testing the previous disciple's flawed training technique...\")\n",
    "flawed_model = CatHungerPredictor(input_features=1)\n",
    "flawed_losses = unsteady_training(flawed_model, hours, hunger, epochs=500)\n",
    "\n",
    "# Visualize the problem\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(flawed_losses, color='red', linewidth=2, label='Flawed Training')\n",
    "plt.title('üî• The Flawed Training Ritual')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüö® DEBUGGING CHALLENGE:\")\n",
    "print(f\"   The loss seems to be {'climbing' if flawed_losses[-1] > flawed_losses[0] else 'unstable'}!\")\n",
    "print(f\"   Final loss: {flawed_losses[-1]:.4f}\")\n",
    "print(f\"   This is much worse than your proper training: {loss_history[-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nüßê MASTER'S WISDOM:\")\n",
    "print(f\"   'The undisciplined mind accumulates old thoughts,'\")\n",
    "print(f\"   'just as the untrained gradient accumulates old directions.'\")\n",
    "print(f\"\\nüîç HINT: Look carefully at the training loop.\")\n",
    "print(f\"   What crucial step is missing between epochs?\")\n",
    "print(f\"   The Gradient Spirits are not being properly dismissed!\")\n",
    "\n",
    "# The solution (commented out - let students figure it out)\n",
    "# THE MISSING LINE: optimizer.zero_grad() before loss.backward()\n",
    "# Without this, gradients accumulate across epochs, causing instability\n",
    "\n",
    "print(f\"\\nüéØ YOUR MISSION:\")\n",
    "print(f\"   Can you spot the critical error in the 'unsteady_training' function?\")\n",
    "print(f\"   Fix it and compare the training curves!\")\n",
    "print(f\"   Understanding this error is crucial for all future neural network training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÆ THE SACRED COMPLETION\n",
    "\n",
    "Master Pai-Torch bows deeply as you complete the trials.\n",
    "\n",
    "*\"Well done, grasshopper. You have learned the first sacred art - the Linear Transformation. You understand how gradients flow backward through your network, updating the sacred parameters that encode wisdom. This foundation will serve you well in all future trials.\"*\n",
    "\n",
    "**üéì WHAT YOU HAVE MASTERED:**\n",
    "- Linear neural networks and forward passes\n",
    "- Gradient descent and backpropagation\n",
    "- The sacred ritual of `optimizer.zero_grad()`\n",
    "- Loss functions and convergence\n",
    "- Batch processing and data normalization\n",
    "- Learning rate scheduling and early stopping\n",
    "- Decision boundaries and threshold optimization\n",
    "- Debugging gradient accumulation problems\n",
    "\n",
    "**üîÆ LOOKING AHEAD:**\n",
    "In your next trials, you will learn about multiple layers, activation functions, and more complex architectures. But remember - all neural wisdom builds upon these linear foundations.\n",
    "\n",
    "**üê± SUKI'S FINAL WISDOM:**\n",
    "*Meow.* (Translation: \"The journey of a thousand neural networks begins with a single linear layer.\")\n",
    "\n",
    "**Continue your journey with Dan 2 to learn the arts of the Temple Guardian!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
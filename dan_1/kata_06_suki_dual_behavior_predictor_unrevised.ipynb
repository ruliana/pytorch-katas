{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ruliana/pytorch-katas/blob/main/dan_1/kata_06_suki_dual_behavior_predictor_unrevised.ipynb)\n",
    "\n",
    "## üèÆ The Ancient Scroll Unfurls üèÆ\n",
    "\n",
    "# THE TWIN MYSTERIES OF SUKI'S NATURE: CLASSIFICATION MEETS REGRESSION\n",
    "Dan Level: 1 (Temple Sweeper) | Time: 45 minutes | Sacred Arts: Multi-output learning, Classification + Regression, Dual loss functions\n",
    "\n",
    "## üìú THE CHALLENGE\n",
    "\n",
    "Master Pai-Torch sits in contemplative silence, observing Suki's graceful movements around the temple courtyard. \"Grasshopper,\" the ancient master finally speaks, \"you have learned to predict single truths - when Suki will eat, whether doors will stick, which ceremonies are occurring. But observe closely: Suki's behavior contains not one mystery, but two intertwined enigmas that dance together like yin and yang.\" The master's eyes gleam with ancient wisdom. \"She exists in distinct modes - the focused hunter tracking temple mice, and the serene napper basking in sunbeams. Yet within each mode, her energy pulses with varying intensity, from barely stirring to explosive action.\"\n",
    "\n",
    "\"The ultimate Dan 1 challenge awaits you: create a single neural network that can simultaneously classify Suki's behavioral mode AND predict her activity intensity level. You must learn to weave together the discrete art of classification with the continuous flow of regression, understanding when to use each approach for different aspects of the same phenomenon. This is the sacred dual nature of machine learning - knowing both what category something belongs to, and how much of that category it embodies.\"\n",
    "\n",
    "## üéØ THE SACRED OBJECTIVES\n",
    "\n",
    "- [ ] **Master Multi-Output Networks**: Build a single model that produces two different types of predictions\n",
    "- [ ] **Understand Classification vs Regression**: Learn when each approach serves different aspects of the same problem\n",
    "- [ ] **Combine Different Loss Functions**: Calculate separate losses for classification and regression outputs\n",
    "- [ ] **Interpret Dual Predictions**: Understand how to read and validate both categorical and continuous outputs\n",
    "- [ ] **Visualize Complex Relationships**: Create plots that show both discrete categories and continuous values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ FIRST CELL - ALL IMPORTS AND CONFIGURATION\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "# Set reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Global configuration constants\n",
    "DEFAULT_CHAOS_LEVEL = 0.15\n",
    "SACRED_SEED = 42\n",
    "HUNTING_MODE = 0\n",
    "NAPPING_MODE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üê± THE SACRED DATA GENERATION SCROLL\n",
    "\n",
    "*Master Pai-Torch gestures toward Suki, who is currently stretched luxuriously in a sunbeam*\n",
    "\n",
    "\"Observe, Grasshopper. Suki's behavior follows ancient patterns that even she may not fully understand. Her activity level depends on the time since her last meal and the temple's ambient energy, but her mode - hunting versus napping - follows deeper rhythms that shift like the temple's spiritual tides.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_suki_dual_behavior_data(n_observations: int = 200, chaos_level: float = 0.15,\n",
    "                                   sacred_seed: int = 42) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Generate observations of Suki's dual nature: behavioral mode and activity intensity.\n",
    "\n",
    "    Ancient wisdom reveals:\n",
    "    - Mode depends on: spiritual_energy threshold (hunting if > 0.4, napping otherwise)\n",
    "    - Intensity follows: base_intensity + hours_factor * hours_since_meal + spiritual_boost\n",
    "    - Hunting mode: base_intensity = 30, hours_factor = 3.0, spiritual_boost = spiritual_energy * 40\n",
    "    - Napping mode: base_intensity = 10, hours_factor = 1.5, spiritual_boost = (1-spiritual_energy) * 25\n",
    "\n",
    "    Args:\n",
    "        n_observations: Number of Suki behavior observations\n",
    "        chaos_level: Amount of feline unpredictability (0.0 = perfectly predictable, 1.0 = pure chaos)\n",
    "        sacred_seed: Ensures consistent mystical randomness\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (input_features, behavioral_modes, activity_intensities)\n",
    "        - input_features: [hours_since_meal, spiritual_energy] (n_observations, 2)\n",
    "        - behavioral_modes: 0=hunting, 1=napping (n_observations,) as long tensor\n",
    "        - activity_intensities: 0-100 energy level (n_observations, 1) as float tensor\n",
    "    \"\"\"\n",
    "    torch.manual_seed(sacred_seed)\n",
    "    \n",
    "    # Suki's input features\n",
    "    hours_since_meal = torch.rand(n_observations) * 8  # 0-8 hours range\n",
    "    spiritual_energy = torch.rand(n_observations)      # 0-1 temple energy level\n",
    "    \n",
    "    # Combine features for input\n",
    "    input_features = torch.stack([hours_since_meal, spiritual_energy], dim=1)\n",
    "    \n",
    "    # Determine behavioral mode (classification target)\n",
    "    # Hunting mode when spiritual energy > 0.4, napping otherwise\n",
    "    behavioral_modes = (spiritual_energy > 0.4).long()  # 0=hunting, 1=napping\n",
    "    \n",
    "    # Calculate activity intensity (regression target)\n",
    "    activity_intensities = torch.zeros(n_observations)\n",
    "    \n",
    "    # Hunting mode calculations\n",
    "    hunting_mask = behavioral_modes == 0\n",
    "    activity_intensities[hunting_mask] = (\n",
    "        30 +  # base hunting intensity\n",
    "        3.0 * hours_since_meal[hunting_mask] +  # hunger drives activity\n",
    "        spiritual_energy[hunting_mask] * 40  # spiritual boost for hunters\n",
    "    )\n",
    "    \n",
    "    # Napping mode calculations\n",
    "    napping_mask = behavioral_modes == 1\n",
    "    activity_intensities[napping_mask] = (\n",
    "        10 +  # base napping intensity\n",
    "        1.5 * hours_since_meal[napping_mask] +  # mild hunger effect\n",
    "        (1 - spiritual_energy[napping_mask]) * 25  # low energy = deeper naps\n",
    "    )\n",
    "    \n",
    "    # Add feline chaos to intensity (cats are wonderfully unpredictable)\n",
    "    chaos = torch.randn(n_observations) * chaos_level * activity_intensities.std()\n",
    "    activity_intensities = activity_intensities + chaos\n",
    "    \n",
    "    # Keep intensities within reasonable bounds\n",
    "    activity_intensities = torch.clamp(activity_intensities, 0, 100)\n",
    "    \n",
    "    return input_features, behavioral_modes, activity_intensities.unsqueeze(1)\n",
    "\n",
    "def visualize_suki_mysteries(features: torch.Tensor, modes: torch.Tensor, \n",
    "                           intensities: torch.Tensor, predictions: Tuple[torch.Tensor, torch.Tensor] = None):\n",
    "    \"\"\"Display the twin mysteries of Suki's behavioral patterns.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    hours = features[:, 0].numpy()\n",
    "    spiritual = features[:, 1].numpy()\n",
    "    modes_np = modes.numpy()\n",
    "    intensities_np = intensities.squeeze().numpy()\n",
    "    \n",
    "    # Plot 1: Mode Classification (discrete categories)\n",
    "    hunting_mask = modes_np == 0\n",
    "    napping_mask = modes_np == 1\n",
    "    \n",
    "    ax1.scatter(hours[hunting_mask], spiritual[hunting_mask], \n",
    "               c='red', alpha=0.6, label='Hunting Mode', s=60)\n",
    "    ax1.scatter(hours[napping_mask], spiritual[napping_mask], \n",
    "               c='blue', alpha=0.6, label='Napping Mode', s=60)\n",
    "    \n",
    "    ax1.axhline(y=0.4, color='purple', linestyle='--', alpha=0.7,\n",
    "               label='Mode Threshold (Spiritual Energy = 0.4)')\n",
    "    ax1.set_xlabel('Hours Since Last Meal')\n",
    "    ax1.set_ylabel('Temple Spiritual Energy')\n",
    "    ax1.set_title('Suki\\'s Behavioral Mode Classification')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Activity Intensity Regression (continuous values)\n",
    "    scatter = ax2.scatter(hours, spiritual, c=intensities_np, \n",
    "                         cmap='viridis', alpha=0.7, s=60)\n",
    "    plt.colorbar(scatter, ax=ax2, label='Activity Intensity (0-100)')\n",
    "    \n",
    "    if predictions is not None:\n",
    "        pred_modes, pred_intensities = predictions\n",
    "        # Show prediction accuracy with different markers\n",
    "        correct_mode = (torch.argmax(pred_modes, dim=1) == modes)\n",
    "        ax2.scatter(hours[~correct_mode.numpy()], spiritual[~correct_mode.numpy()], \n",
    "                   marker='x', c='red', s=100, alpha=0.8, label='Mode Prediction Error')\n",
    "        \n",
    "    ax2.set_xlabel('Hours Since Last Meal')\n",
    "    ax2.set_ylabel('Temple Spiritual Energy')\n",
    "    ax2.set_title('Suki\\'s Activity Intensity Regression')\n",
    "    if predictions is not None:\n",
    "        ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate and visualize the sacred data\n",
    "features, modes, intensities = generate_suki_dual_behavior_data()\n",
    "print(f\"Generated {len(features)} observations of Suki's dual nature\")\n",
    "print(f\"Input features shape: {features.shape} (hours_since_meal, spiritual_energy)\")\n",
    "print(f\"Behavioral modes shape: {modes.shape} (0=hunting, 1=napping)\")\n",
    "print(f\"Activity intensities shape: {intensities.shape} (0-100 energy level)\")\n",
    "print(f\"\\nMode distribution: {torch.bincount(modes)} [hunting, napping]\")\n",
    "print(f\"Intensity range: {intensities.min():.1f} - {intensities.max():.1f}\")\n",
    "\n",
    "visualize_suki_mysteries(features, modes, intensities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíÉ FIRST MOVEMENTS: THE DUAL-OUTPUT NEURAL NETWORK\n",
    "\n",
    "*Master Pai-Torch raises a weathered hand*\n",
    "\n",
    "\"Now comes the true test, Grasshopper. You must forge a single neural network that speaks two languages - the binary whispers of classification and the flowing songs of regression. Like a master calligrapher who can write both poetry and accounting ledgers with the same brush, your model must produce both categorical truths and continuous wisdom.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SukiDualBehaviorPredictor(nn.Module):\n",
    "    \"\"\"A mystical artifact that understands both the discrete and continuous nature of Suki.\"\"\"\n",
    "\n",
    "    def __init__(self, input_features: int = 2, hidden_size: int = 16):\n",
    "        super(SukiDualBehaviorPredictor, self).__init__()\n",
    "        \n",
    "        # TODO: Create the shared hidden layer\n",
    "        # Hint: This layer learns features useful for both tasks\n",
    "        self.shared_layer = None\n",
    "        \n",
    "        # TODO: Create the classification head (mode prediction)\n",
    "        # Hint: Output should have 2 neurons for binary classification (hunting vs napping)\n",
    "        self.mode_classifier = None\n",
    "        \n",
    "        # TODO: Create the regression head (intensity prediction)\n",
    "        # Hint: Output should have 1 neuron for continuous intensity values\n",
    "        self.intensity_regressor = None\n",
    "\n",
    "    def forward(self, features: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Channel dual wisdom through the mystical network.\"\"\"\n",
    "        # TODO: Pass input through shared hidden layer with ReLU activation\n",
    "        # Hint: F.relu() applies the activation function\n",
    "        hidden = None\n",
    "        \n",
    "        # TODO: Get mode classification logits (raw outputs)\n",
    "        # Hint: Don't apply softmax here - let the loss function handle it\n",
    "        mode_logits = None\n",
    "        \n",
    "        # TODO: Get intensity regression output\n",
    "        # Hint: No activation needed for regression output\n",
    "        intensity_output = None\n",
    "        \n",
    "        return mode_logits, intensity_output\n",
    "\n",
    "def train_dual_model(model: nn.Module, features: torch.Tensor, mode_targets: torch.Tensor, \n",
    "                    intensity_targets: torch.Tensor, epochs: int = 1500, learning_rate: float = 0.01) -> dict:\n",
    "    \"\"\"\n",
    "    Train the dual-output model with separate losses for classification and regression.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing training history for both tasks\n",
    "    \"\"\"\n",
    "    # TODO: Choose classification loss function\n",
    "    # Hint: CrossEntropyLoss is perfect for multi-class classification\n",
    "    classification_criterion = None\n",
    "    \n",
    "    # TODO: Choose regression loss function\n",
    "    # Hint: MSELoss works well for continuous value prediction\n",
    "    regression_criterion = None\n",
    "\n",
    "    # TODO: Choose optimizer for all model parameters\n",
    "    # Hint: Adam optimizer often works well for multi-task learning\n",
    "    optimizer = None\n",
    "\n",
    "    # Track training progress\n",
    "    history = {\n",
    "        'classification_losses': [],\n",
    "        'regression_losses': [],\n",
    "        'total_losses': [],\n",
    "        'mode_accuracies': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # TODO: CRITICAL - Clear gradients from previous iteration\n",
    "        # Hint: Always banish the old gradient spirits first\n",
    "        \n",
    "        # TODO: Forward pass - get both predictions\n",
    "        mode_logits, intensity_predictions = None\n",
    "\n",
    "        # TODO: Calculate classification loss\n",
    "        # Hint: Compare mode_logits with mode_targets\n",
    "        classification_loss = None\n",
    "\n",
    "        # TODO: Calculate regression loss\n",
    "        # Hint: Compare intensity_predictions with intensity_targets\n",
    "        regression_loss = None\n",
    "\n",
    "        # TODO: Combine both losses (equal weighting for now)\n",
    "        # Hint: total_loss = classification_loss + regression_loss\n",
    "        total_loss = None\n",
    "\n",
    "        # TODO: Backward pass - compute gradients\n",
    "        \n",
    "        # TODO: Update parameters\n",
    "\n",
    "        # Track progress\n",
    "        history['classification_losses'].append(classification_loss.item())\n",
    "        history['regression_losses'].append(regression_loss.item())\n",
    "        history['total_losses'].append(total_loss.item())\n",
    "        \n",
    "        # Calculate mode prediction accuracy\n",
    "        with torch.no_grad():\n",
    "            predicted_modes = torch.argmax(mode_logits, dim=1)\n",
    "            accuracy = (predicted_modes == mode_targets).float().mean().item()\n",
    "            history['mode_accuracies'].append(accuracy)\n",
    "\n",
    "        # Report progress to Master Pai-Torch\n",
    "        if (epoch + 1) % 300 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}]:')\n",
    "            print(f'  Classification Loss: {classification_loss.item():.4f}')\n",
    "            print(f'  Regression Loss: {regression_loss.item():.4f}')\n",
    "            print(f'  Total Loss: {total_loss.item():.4f}')\n",
    "            print(f'  Mode Accuracy: {accuracy:.1%}')\n",
    "            if accuracy > 0.85 and regression_loss.item() < 100:\n",
    "                print(\"  üí´ The dual spirits of learning dance in harmony!\")\n",
    "            print()\n",
    "\n",
    "    return history\n",
    "\n",
    "def plot_dual_training_progress(history: dict):\n",
    "    \"\"\"Visualize the journey of dual learning.\"\"\"\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    epochs = range(1, len(history['total_losses']) + 1)\n",
    "    \n",
    "    # Total loss\n",
    "    ax1.plot(epochs, history['total_losses'], 'purple', linewidth=2)\n",
    "    ax1.set_title('Total Combined Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Classification loss\n",
    "    ax2.plot(epochs, history['classification_losses'], 'red', linewidth=2)\n",
    "    ax2.set_title('Classification Loss (Mode Prediction)')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Cross-Entropy Loss')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Regression loss\n",
    "    ax3.plot(epochs, history['regression_losses'], 'blue', linewidth=2)\n",
    "    ax3.set_title('Regression Loss (Intensity Prediction)')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('MSE Loss')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Mode accuracy\n",
    "    ax4.plot(epochs, [acc * 100 for acc in history['mode_accuracies']], 'green', linewidth=2)\n",
    "    ax4.axhline(y=85, color='gold', linestyle='--', alpha=0.7, label='Mastery Threshold (85%)')\n",
    "    ax4.set_title('Mode Classification Accuracy')\n",
    "    ax4.set_xlabel('Epoch')\n",
    "    ax4.set_ylabel('Accuracy (%)')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° THE TRIALS OF MASTERY\n",
    "\n",
    "*Master Pai-Torch's eyes gleam with anticipation*\n",
    "\n",
    "\"The moment of truth approaches, Grasshopper. Your dual-natured network must prove its worth through four sacred trials. Only when both the discrete and continuous spirits bow to your will shall you achieve true mastery.\"\n",
    "\n",
    "### Trial 1: Basic Dual Mastery\n",
    "- [ ] Classification loss decreases consistently (the category spirits align)\n",
    "- [ ] Regression loss decreases consistently (the intensity flows harmoniously)\n",
    "- [ ] Final mode classification accuracy above 85% (Suki approves of your categorical wisdom)\n",
    "- [ ] Final intensity regression MSE below 100 (your continuous predictions flow true)\n",
    "- [ ] Model produces outputs of correct shapes for both tasks\n",
    "\n",
    "### Trial 2: Understanding Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_your_dual_wisdom(model):\n",
    "    \"\"\"Master Pai-Torch's evaluation of your dual understanding.\"\"\"\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    # Test with known scenarios\n",
    "    test_features = torch.tensor([\n",
    "        [2.0, 0.8],  # Short hunger, high spiritual energy ‚Üí should be hunting mode, high intensity\n",
    "        [6.0, 0.2],  # Long hunger, low spiritual energy ‚Üí should be napping mode, moderate intensity\n",
    "        [1.0, 0.6],  # Very short hunger, medium-high energy ‚Üí hunting mode, medium intensity\n",
    "        [4.0, 0.1]   # Medium hunger, very low energy ‚Üí napping mode, low intensity\n",
    "    ])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        mode_logits, intensity_predictions = model(test_features)\n",
    "    \n",
    "    # Check output shapes\n",
    "    assert mode_logits.shape == (4, 2), f\"Mode logits shape should be (4, 2), got {mode_logits.shape}\"\n",
    "    assert intensity_predictions.shape == (4, 1), f\"Intensity shape should be (4, 1), got {intensity_predictions.shape}\"\n",
    "    \n",
    "    # Convert logits to probabilities and predictions\n",
    "    mode_probabilities = F.softmax(mode_logits, dim=1)\n",
    "    predicted_modes = torch.argmax(mode_logits, dim=1)\n",
    "    \n",
    "    print(\"üîÆ Test Scenario Analysis:\")\n",
    "    scenarios = [\n",
    "        \"Short hunger (2h), High energy (0.8)\",\n",
    "        \"Long hunger (6h), Low energy (0.2)\", \n",
    "        \"Very short hunger (1h), Medium-high energy (0.6)\",\n",
    "        \"Medium hunger (4h), Very low energy (0.1)\"\n",
    "    ]\n",
    "    \n",
    "    for i, scenario in enumerate(scenarios):\n",
    "        mode_pred = \"Hunting\" if predicted_modes[i] == 0 else \"Napping\"\n",
    "        mode_conf = mode_probabilities[i, predicted_modes[i]].item()\n",
    "        intensity_pred = intensity_predictions[i, 0].item()\n",
    "        \n",
    "        print(f\"  {scenario}:\")\n",
    "        print(f\"    Mode: {mode_pred} (confidence: {mode_conf:.1%})\")\n",
    "        print(f\"    Intensity: {intensity_pred:.1f}\")\n",
    "    \n",
    "    # Logical consistency checks\n",
    "    expected_modes = [0, 1, 0, 1]  # Based on spiritual energy thresholds\n",
    "    mode_accuracy = (predicted_modes == torch.tensor(expected_modes)).float().mean()\n",
    "    \n",
    "    assert mode_accuracy >= 0.75, f\"Mode prediction accuracy {mode_accuracy:.1%} - the categorical spirits need more training!\"\n",
    "    assert torch.all(intensity_predictions >= 0), \"Intensity predictions must be non-negative!\"\n",
    "    assert torch.all(intensity_predictions <= 150), \"Intensity predictions seem unreasonably high!\"\n",
    "    \n",
    "    print(f\"\\n‚ú® Logical Consistency: {mode_accuracy:.1%} of modes match expected patterns\")\n",
    "    print(\"üéâ Master Pai-Torch nods with deep satisfaction - your dual understanding blooms like a lotus!\")\n",
    "    \n",
    "    model.train()  # Return to training mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üå∏ THE FOUR PATHS OF MASTERY: PROGRESSIVE EXTENSIONS\n",
    "\n",
    "*Master Pai-Torch settles into a meditative posture*\n",
    "\n",
    "\"Your foundation grows strong, but true mastery requires exploration of the deeper mysteries. Four paths stretch before you, each revealing new aspects of the dual nature of learning.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension 1: Cook Oh-Pai-Timizer's Recipe Balance\n",
    "*\"A master chef knows that different dishes require different seasonings!\"*\n",
    "\n",
    "*Cook Oh-Pai-Timizer bustles over, carrying two different-sized ladles*\n",
    "\n",
    "\"Ah, Grasshopper! I see you've learned to cook both savory and sweet dishes simultaneously. But notice - in my kitchen, I don't add equal amounts of salt and sugar to every recipe! Sometimes the main dish needs more attention, sometimes the dessert. Your dual model is like a complex meal - each part may need different amounts of attention.\"\n",
    "\n",
    "**NEW CONCEPTS**: Loss weighting, balancing different task importance, hyperparameter tuning\n",
    "**DIFFICULTY**: +15% (still Dan 1, but more nuanced training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_balanced_losses(model: nn.Module, features: torch.Tensor, mode_targets: torch.Tensor,\n",
    "                              intensity_targets: torch.Tensor, classification_weight: float = 1.0,\n",
    "                              regression_weight: float = 0.01, epochs: int = 1500) -> dict:\n",
    "    \"\"\"\n",
    "    Train with weighted losses to balance the importance of classification vs regression.\n",
    "    \n",
    "    Args:\n",
    "        classification_weight: How much to emphasize mode prediction accuracy\n",
    "        regression_weight: How much to emphasize intensity prediction accuracy\n",
    "        \n",
    "    Returns:\n",
    "        Training history with weighted losses\n",
    "    \"\"\"\n",
    "    # TODO: Implement weighted training\n",
    "    # Hint: total_loss = classification_weight * classification_loss + regression_weight * regression_loss\n",
    "    # Try different weight combinations: (1.0, 0.01), (2.0, 0.02), (0.5, 0.005)\n",
    "    pass\n",
    "\n",
    "def compare_loss_weightings():\n",
    "    \"\"\"Compare different loss weighting strategies.\"\"\"\n",
    "    # TODO: Train multiple models with different loss weightings\n",
    "    # Compare final accuracies and regression errors\n",
    "    # Which weighting gives the best balance?\n",
    "    pass\n",
    "\n",
    "# TRIAL: Find the optimal loss weighting for your dual-task model\n",
    "# SUCCESS: Achieve balanced performance where both tasks perform well\n",
    "# INSIGHT: Learn that classification and regression losses have different scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension 2: He-Ao-World's Observation Uncertainty\n",
    "*\"These old eyes sometimes see things that might not be quite right...\"*\n",
    "\n",
    "*He-Ao-World shuffles over, looking slightly embarrassed*\n",
    "\n",
    "\"Oh dear! I've been observing Suki for months now, but I'm starting to worry... Some of my observations might be a bit uncertain. Sometimes I write down 'hunting' when Suki might have been transitioning between modes. And my intensity measurements? Well, let's just say these old eyes aren't as sharp as they used to be for fine distinctions.\"\n",
    "\n",
    "**NEW CONCEPTS**: Model confidence, prediction uncertainty, probabilistic outputs\n",
    "**DIFFICULTY**: +25% (still Dan 1, but with uncertainty quantification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_prediction_confidence(model: nn.Module, features: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Analyze how confident your model is in its dual predictions.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with confidence metrics for both tasks\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        mode_logits, intensity_predictions = model(features)\n",
    "        \n",
    "        # TODO: Calculate mode prediction confidence\n",
    "        # Hint: Use softmax to get probabilities, then look at the max probability\n",
    "        mode_probabilities = None\n",
    "        mode_confidence = None  # Maximum probability for each prediction\n",
    "        \n",
    "        # TODO: Identify uncertain predictions\n",
    "        # Hint: Low confidence means the model is unsure\n",
    "        uncertain_threshold = 0.6  # Below this confidence, we're uncertain\n",
    "        uncertain_predictions = None\n",
    "        \n",
    "        return {\n",
    "            'mode_confidences': mode_confidence,\n",
    "            'uncertain_count': uncertain_predictions.sum().item(),\n",
    "            'average_confidence': mode_confidence.mean().item()\n",
    "        }\n",
    "\n",
    "def visualize_uncertainty(features: torch.Tensor, modes: torch.Tensor, \n",
    "                         intensities: torch.Tensor, model: nn.Module):\n",
    "    \"\"\"Show where your model is most and least confident.\"\"\"\n",
    "    # TODO: Create a visualization showing:\n",
    "    # - Original data points\n",
    "    # - Color-coded by model confidence\n",
    "    # - Mark uncertain predictions with different symbols\n",
    "    pass\n",
    "\n",
    "# TRIAL: Identify which types of Suki behavior are hardest to predict\n",
    "# SUCCESS: Understand when your model is confident vs uncertain\n",
    "# MASTERY: Learn that knowing what you don't know is as important as knowing what you do know"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension 3: Master Pai-Torch's Architecture Wisdom\n",
    "*\"The structure of understanding shapes the nature of wisdom.\"*\n",
    "\n",
    "*Master Pai-Torch draws patterns in the temple sand*\n",
    "\n",
    "\"Observe, Grasshopper. Your current network shares all learning between both tasks - a single hidden layer serves both classification and regression. But consider: might some knowledge be specific to each task? Perhaps the hunting-detection spirits require different wisdom than the intensity-measuring spirits. Sometimes, partial separation leads to greater harmony.\"\n",
    "\n",
    "**NEW CONCEPTS**: Multi-task architecture design, shared vs task-specific layers, architectural choices\n",
    "**DIFFICULTY**: +35% (still Dan 1, but exploring architecture variations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedSukiPredictor(nn.Module):\n",
    "    \"\"\"An advanced dual-output model with both shared and task-specific layers.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_features: int = 2, shared_size: int = 12, task_specific_size: int = 8):\n",
    "        super(EnhancedSukiPredictor, self).__init__()\n",
    "        \n",
    "        # TODO: Create shared representation layer\n",
    "        # Hint: This captures common patterns useful for both tasks\n",
    "        self.shared_layer = None\n",
    "        \n",
    "        # TODO: Create task-specific hidden layers\n",
    "        # Hint: Mode classification might benefit from different features than intensity regression\n",
    "        self.mode_specific_layer = None\n",
    "        self.intensity_specific_layer = None\n",
    "        \n",
    "        # TODO: Create output layers\n",
    "        self.mode_output = None\n",
    "        self.intensity_output = None\n",
    "    \n",
    "    def forward(self, features: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # TODO: Implement forward pass with both shared and task-specific processing\n",
    "        # Architecture: input ‚Üí shared ‚Üí [mode_specific, intensity_specific] ‚Üí outputs\n",
    "        pass\n",
    "\n",
    "def compare_architectures():\n",
    "    \"\"\"Compare simple shared vs enhanced shared+specific architectures.\"\"\"\n",
    "    # TODO: Train both architectures and compare:\n",
    "    # - Training speed\n",
    "    # - Final accuracy/error\n",
    "    # - Model complexity (parameter count)\n",
    "    # Which architecture works better for Suki's dual nature?\n",
    "    pass\n",
    "\n",
    "# TRIAL: Design and compare different multi-task architectures\n",
    "# SUCCESS: Understand the trade-offs between shared and task-specific processing\n",
    "# INSIGHT: Learn that architecture design is itself a form of inductive bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension 4: Suki's Temporal Behavior Mystery\n",
    "*\"Time flows like a river, and Suki's nature flows with it.\"*\n",
    "\n",
    "*Suki suddenly appears, sits perfectly still for exactly 3.7 seconds, then gracefully leaps onto Master Pai-Torch's shoulder*\n",
    "\n",
    "*Master Pai-Torch smiles knowingly* \"The sacred cat reminds us that her behavior is not just about the present moment, but about the flow of time itself. What if her current mode influences how her intensity changes? What if hunting behavior creates momentum that affects future intensity, while napping creates a different temporal pattern?\"\n",
    "\n",
    "**NEW CONCEPTS**: Sequence modeling, temporal dependencies, state-dependent dynamics\n",
    "**DIFFICULTY**: +45% (advanced Dan 1, introducing time-series concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_temporal_suki_data(sequence_length: int = 50, n_sequences: int = 20):\n",
    "    \"\"\"\n",
    "    Generate time-series data showing how Suki's mode affects her intensity patterns.\n",
    "    \n",
    "    Returns:\n",
    "        Sequences where current mode influences future intensity changes\n",
    "    \"\"\"\n",
    "    # TODO: Create sequences where:\n",
    "    # - Hunting mode: intensity tends to build up over time\n",
    "    # - Napping mode: intensity tends to decay over time\n",
    "    # - Mode transitions create intensity jumps\n",
    "    pass\n",
    "\n",
    "def analyze_temporal_patterns(sequences):\n",
    "    \"\"\"\n",
    "    Discover how mode affects intensity evolution over time.\n",
    "    \"\"\"\n",
    "    # TODO: Analyze:\n",
    "    # - How does intensity change during hunting sequences?\n",
    "    # - How does intensity change during napping sequences?\n",
    "    # - What happens at mode transitions?\n",
    "    pass\n",
    "\n",
    "def visualize_temporal_wisdom(sequences):\n",
    "    \"\"\"Show how Suki's modes create different temporal dynamics.\"\"\"\n",
    "    # TODO: Create time-series plots showing:\n",
    "    # - Mode over time (discrete)\n",
    "    # - Intensity over time (continuous)\n",
    "    # - How they interact across different sequences\n",
    "    pass\n",
    "\n",
    "# TRIAL: Discover and model the temporal relationships in Suki's behavior\n",
    "# SUCCESS: Understand how classification and regression can be temporally coupled\n",
    "# MASTERY: Recognize that static models miss the dynamic dance of real behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî• CORRECTING YOUR FORM: A STANCE IMBALANCE\n",
    "\n",
    "*Master Pai-Torch observes your training ritual with a careful eye*\n",
    "\n",
    "\"Your eager mind races ahead of your disciplined form, Grasshopper. See how your dual-task stance wavers? The spirits of classification and regression must dance in harmony, but I observe discord in your technique.\"\n",
    "\n",
    "*A previous disciple left this flawed dual training ritual. The form has lost its balance - can you restore proper technique?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsteady_dual_training(model, features, mode_targets, intensity_targets, epochs=800):\n",
    "    \"\"\"This dual training stance has lost its balance - your form needs correction! ü•ã\"\"\"\n",
    "    classification_criterion = nn.CrossEntropyLoss()\n",
    "    regression_criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass - get both predictions\n",
    "        mode_logits, intensity_predictions = model(features)\n",
    "        \n",
    "        # Calculate losses\n",
    "        classification_loss = classification_criterion(mode_logits, mode_targets)\n",
    "        regression_loss = regression_criterion(intensity_predictions, intensity_targets)\n",
    "        total_loss = classification_loss + regression_loss\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 200 == 0:\n",
    "            print(f'Epoch {epoch}: Total Loss = {total_loss.item():.4f}')\n",
    "            print(f'  Classification: {classification_loss.item():.4f}')\n",
    "            print(f'  Regression: {regression_loss.item():.4f}')\n",
    "\n",
    "    return model\n",
    "\n",
    "# DEBUGGING CHALLENGE: Can you spot the critical error in this dual training ritual?\n",
    "# HINT: The Gradient Spirits from both tasks are accumulating and interfering with each other\n",
    "# MASTER'S WISDOM: \"In dual-task learning, the undisciplined gradient accumulates old wisdom from both classification and regression, creating confusion in the parameter updates. The wise practitioner clears the slate before each new learning cycle.\"\n",
    "\n",
    "# What happens when you run this code? What symptoms do you observe?\n",
    "# How would you fix this to restore proper dual-task training form?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì THE GRADUATION CEREMONY\n",
    "\n",
    "*Master Pai-Torch rises slowly, a satisfied gleam in ancient eyes*\n",
    "\n",
    "\"Exceptional work, Grasshopper. You have achieved something truly remarkable - the ability to see both the forest and the trees, the category and the quantity, the discrete and the continuous. Your neural network now speaks two languages fluently, understanding both 'what type' and 'how much' of Suki's mystical nature.\"\n",
    "\n",
    "*Suki purrs approvingly and demonstrates a perfect hunting pounce followed immediately by a serene nap*\n",
    "\n",
    "\"The sacred cat herself approves. You have learned that real-world problems rarely ask just one question - they demand understanding of multiple interrelated aspects simultaneously. Classification and regression are not competitors, but dance partners in the grand ballet of machine learning.\"\n",
    "\n",
    "### üèÜ MASTERY ACHIEVED\n",
    "\n",
    "**Core Skills Mastered:**\n",
    "- ‚úÖ Multi-output neural network architecture\n",
    "- ‚úÖ Combining classification and regression in a single model\n",
    "- ‚úÖ Different loss functions for different output types\n",
    "- ‚úÖ Dual performance evaluation (accuracy + MSE)\n",
    "- ‚úÖ Understanding when to use discrete vs continuous prediction\n",
    "\n",
    "**Advanced Techniques Explored:**\n",
    "- ‚úÖ Loss weighting and task balancing\n",
    "- ‚úÖ Prediction confidence and uncertainty\n",
    "- ‚úÖ Shared vs task-specific architectures\n",
    "- ‚úÖ Temporal dependencies in multi-task learning\n",
    "\n",
    "**Sacred Wisdom Gained:**\n",
    "*\"The master understands that reality is neither purely categorical nor purely continuous, but a harmonious blend of both. True intelligence lies not in choosing between classification and regression, but in knowing when and how to use each for different aspects of the same phenomenon.\"*\n",
    "\n",
    "### üöÄ THE PATH FORWARD\n",
    "\n",
    "Your journey as a Temple Sweeper reaches its pinnacle, but the path to mastery continues:\n",
    "\n",
    "- **Dan 2 (Temple Guardian)**: Learn to protect against overfitting in multi-task scenarios\n",
    "- **Dan 3 (Weapon Master)**: Explore specialized architectures for complex multi-output problems\n",
    "- **Dan 4 (Combat Innovator)**: Design custom loss functions for novel multi-task challenges\n",
    "- **Dan 5 (Mystic Arts Master)**: Create generative models that produce structured multi-modal outputs\n",
    "\n",
    "*Master Pai-Torch bows respectfully*\n",
    "\n",
    "\"Go forth, newly-promoted Guardian candidate. The temple doors await your protection, and with them, new mysteries of regularization, validation, and robust learning. May the dual spirits of classification and regression guide your continued journey!\"\n",
    "\n",
    "*Suki meows once - a sound that somehow conveys both categorical certainty and regression-worthy intensity*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
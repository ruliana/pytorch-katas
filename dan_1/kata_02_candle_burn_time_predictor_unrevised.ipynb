{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ruliana/pytorch-katas/blob/main/dan_1/kata_02_candle_burn_time_predictor_unrevised.ipynb)\n",
    "\n",
    "## 🏮 The Ancient Scroll Unfurls 🏮\n",
    "\n",
    "THE MYSTERIES OF ANCIENT PRAYER CANDLES: A MULTI-VARIABLE REVELATION\n",
    "\n",
    "Dan Level: 1 (Temple Sweeper) | Time: 45 minutes | Sacred Arts: Multi-variable Linear Regression, Feature Scaling, Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📜 THE CHALLENGE\n",
    "\n",
    "The temple's ancient prayer candles burn at different rates depending on wax composition, room temperature, and humidity levels. Each candle is sacred, handcrafted by generations of monks, and understanding their burn patterns is crucial for timing evening prayers. Master Ao-Tougrad approaches you with a cryptic observation: \"Young grasshopper, you have learned to predict simple patterns with single variables. But the true mysteries of this temple require understanding multiple flows simultaneously.\"\n",
    "\n",
    "\"Understanding multiple flows leads to understanding gradient flows,\" Master Ao-Tougrad whispers before disappearing into the shadows. Your task is to create a multi-variable linear model that can predict candle burn times by analyzing wax composition (hardness percentage), room temperature, and humidity levels. This ancient knowledge will help the temple maintain its sacred rhythms while teaching you the fundamental art of multi-dimensional pattern recognition.\n",
    "\n",
    "## 🎯 THE SACRED OBJECTIVES\n",
    "\n",
    "- [ ] Master multi-variable linear regression with PyTorch\n",
    "- [ ] Understand how multiple input features combine to create predictions\n",
    "- [ ] Practice gradient descent with multiple parameters\n",
    "- [ ] Learn to interpret model weights across different features\n",
    "- [ ] Visualize multi-dimensional relationships in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 FIRST CELL - ALL IMPORTS AND CONFIGURATION\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Set reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Global configuration constants\n",
    "DEFAULT_CHAOS_LEVEL = 0.1\n",
    "SACRED_SEED = 42\n",
    "N_FEATURES = 3  # wax_hardness, temperature, humidity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🕯️ THE SACRED CANDLE DATA GENERATION SCROLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candle_burn_data(n_candles: int = 100, chaos_level: float = 0.1, \n",
    "                              sacred_seed: int = 42) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Generate observations of temple candle burning patterns.\n",
    "    \n",
    "    Ancient wisdom reveals the sacred formula:\n",
    "    burn_time = 2.5 * wax_hardness + 0.3 * temperature - 0.8 * humidity + 45\n",
    "    \n",
    "    Where:\n",
    "    - wax_hardness: 20-80% (harder wax burns longer)\n",
    "    - temperature: 15-30°C (warmer rooms reduce burn time)\n",
    "    - humidity: 30-70% (higher humidity reduces burn time)\n",
    "    - burn_time: in minutes\n",
    "    \n",
    "    Args:\n",
    "        n_candles: Number of candle observations to simulate\n",
    "        chaos_level: Amount of environmental unpredictability (0.0 = perfect conditions)\n",
    "        sacred_seed: Ensures consistent randomness for reproducible wisdom\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (candle_features, burn_times) as sacred tensors\n",
    "        candle_features shape: (n_candles, 3) [wax_hardness, temperature, humidity]\n",
    "        burn_times shape: (n_candles, 1)\n",
    "    \"\"\"\n",
    "    torch.manual_seed(sacred_seed)\n",
    "    \n",
    "    # Generate temple candle characteristics\n",
    "    wax_hardness = torch.rand(n_candles) * 60 + 20  # 20-80% hardness\n",
    "    temperature = torch.rand(n_candles) * 15 + 15   # 15-30°C\n",
    "    humidity = torch.rand(n_candles) * 40 + 30      # 30-70% humidity\n",
    "    \n",
    "    # Combine features into a single tensor\n",
    "    candle_features = torch.stack([wax_hardness, temperature, humidity], dim=1)\n",
    "    \n",
    "    # The ancient formula discovered by temple monks\n",
    "    true_weights = torch.tensor([2.5, 0.3, -0.8])  # hardness, temp, humidity coefficients\n",
    "    true_bias = 45.0\n",
    "    \n",
    "    # Calculate perfect burn times\n",
    "    perfect_burn_times = torch.matmul(candle_features, true_weights) + true_bias\n",
    "    \n",
    "    # Add environmental chaos (drafts, altitude, spiritual energy fluctuations)\n",
    "    chaos = torch.randn(n_candles) * chaos_level * perfect_burn_times.std()\n",
    "    burn_times = perfect_burn_times + chaos\n",
    "    \n",
    "    # Even mystical candles have physical limits\n",
    "    burn_times = torch.clamp(burn_times, 30, 200)  # 30-200 minutes\n",
    "    \n",
    "    return candle_features, burn_times.unsqueeze(1)\n",
    "\n",
    "def visualize_candle_wisdom(features: torch.Tensor, burn_times: torch.Tensor, \n",
    "                           predictions: torch.Tensor = None):\n",
    "    \"\"\"\n",
    "    Display the sacred patterns of candle burning across multiple dimensions.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    feature_names = ['Wax Hardness (%)', 'Temperature (°C)', 'Humidity (%)']\n",
    "    \n",
    "    # Plot each feature vs burn time\n",
    "    for i, feature_name in enumerate(feature_names):\n",
    "        row, col = i // 2, i % 2\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        # Actual data\n",
    "        ax.scatter(features[:, i].numpy(), burn_times.numpy(), \n",
    "                  alpha=0.6, color='orange', label='Actual Burn Times')\n",
    "        \n",
    "        # Predictions if available\n",
    "        if predictions is not None:\n",
    "            ax.scatter(features[:, i].numpy(), predictions.detach().numpy(), \n",
    "                      alpha=0.6, color='gold', marker='x', s=50, \n",
    "                      label='Predicted Burn Times')\n",
    "        \n",
    "        ax.set_xlabel(feature_name)\n",
    "        ax.set_ylabel('Burn Time (minutes)')\n",
    "        ax.set_title(f'Temple Candles: {feature_name} vs Burn Time')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Feature correlation heatmap\n",
    "    ax = axes[1, 1]\n",
    "    feature_data = features.numpy()\n",
    "    correlation_matrix = np.corrcoef(feature_data.T)\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "                xticklabels=feature_names, yticklabels=feature_names, ax=ax)\n",
    "    ax.set_title('Feature Correlations in Temple Candles')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate and visualize the sacred candle data\n",
    "candle_features, burn_times = generate_candle_burn_data(n_candles=150, chaos_level=0.15)\n",
    "print(f\"Generated data for {len(candle_features)} temple candles\")\n",
    "print(f\"Feature tensor shape: {candle_features.shape}\")\n",
    "print(f\"Burn times tensor shape: {burn_times.shape}\")\n",
    "print(f\"\\nFeature statistics:\")\n",
    "print(f\"Wax hardness: {candle_features[:, 0].min():.1f}% - {candle_features[:, 0].max():.1f}%\")\n",
    "print(f\"Temperature: {candle_features[:, 1].min():.1f}°C - {candle_features[:, 1].max():.1f}°C\")\n",
    "print(f\"Humidity: {candle_features[:, 2].min():.1f}% - {candle_features[:, 2].max():.1f}%\")\n",
    "print(f\"Burn times: {burn_times.min():.1f} - {burn_times.max():.1f} minutes\")\n",
    "\n",
    "visualize_candle_wisdom(candle_features, burn_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🕯️ FIRST MOVEMENTS - THE MULTI-VARIABLE PREDICTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CandleBurnPredictor(nn.Module):\n",
    "    \"\"\"A mystical artifact for understanding multi-dimensional candle burning patterns.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_features: int = 3):\n",
    "        super(CandleBurnPredictor, self).__init__()\n",
    "        # TODO: Create a Linear layer that can handle multiple input features\n",
    "        # Hint: input_features=3 (wax, temperature, humidity) -> output_features=1 (burn time)\n",
    "        self.linear = None\n",
    "    \n",
    "    def forward(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Channel your understanding through the multi-dimensional network.\"\"\"\n",
    "        # TODO: Pass the multi-feature input through your Linear layer\n",
    "        # Remember: this handles multiple features automatically!\n",
    "        return None\n",
    "\n",
    "def train_multi_variable(model: nn.Module, features: torch.Tensor, target: torch.Tensor,\n",
    "                        epochs: int = 1000, learning_rate: float = 0.01) -> list:\n",
    "    \"\"\"\n",
    "    Train the multi-variable candle burn prediction model.\n",
    "    \n",
    "    Returns:\n",
    "        List of loss values during training\n",
    "    \"\"\"\n",
    "    # TODO: Choose your loss calculation method\n",
    "    # Hint: MSE Loss works well for multi-variable regression too!\n",
    "    criterion = None\n",
    "    \n",
    "    # TODO: Choose your parameter updating method\n",
    "    # Hint: SGD will update ALL parameters (weights + bias) automatically\n",
    "    optimizer = None\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # TODO: CRITICAL - Clear the gradient spirits from previous cycle\n",
    "        # Hint: With multiple features, this is even more important!\n",
    "        \n",
    "        # TODO: Forward pass - get predictions from all features\n",
    "        predictions = None\n",
    "        \n",
    "        # TODO: Compute the loss\n",
    "        loss = None\n",
    "        \n",
    "        # TODO: Backward pass - compute gradients for all parameters\n",
    "        \n",
    "        # TODO: Update all parameters (weights and bias)\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Report progress to Master Ao-Tougrad\n",
    "        if (epoch + 1) % 200 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "            if loss.item() < 20:\n",
    "                print(\"🌟 Master Ao-Tougrad nods approvingly at your multi-dimensional wisdom!\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "def analyze_learned_patterns(model: CandleBurnPredictor):\n",
    "    \"\"\"Examine what the model learned about candle burning.\"\"\"\n",
    "    weights = model.linear.weight.data.squeeze()\n",
    "    bias = model.linear.bias.data.item()\n",
    "    \n",
    "    feature_names = ['Wax Hardness', 'Temperature', 'Humidity']\n",
    "    \n",
    "    print(\"\\n🔥 LEARNED CANDLE BURNING WISDOM:\")\n",
    "    print(f\"Base burn time (bias): {bias:.2f} minutes\")\n",
    "    print(\"\\nFeature importance:\")\n",
    "    for i, (name, weight) in enumerate(zip(feature_names, weights)):\n",
    "        effect = \"increases\" if weight > 0 else \"decreases\"\n",
    "        print(f\"  {name}: {weight:.3f} (each unit {effect} burn time)\")\n",
    "    \n",
    "    # Compare to true values\n",
    "    true_weights = torch.tensor([2.5, 0.3, -0.8])\n",
    "    true_bias = 45.0\n",
    "    \n",
    "    print(\"\\n🎯 COMPARISON TO ANCIENT WISDOM:\")\n",
    "    print(f\"True bias: {true_bias:.2f}, Learned bias: {bias:.2f}\")\n",
    "    for i, (name, true_w, learned_w) in enumerate(zip(feature_names, true_weights, weights)):\n",
    "        print(f\"  {name}: True={true_w:.3f}, Learned={learned_w:.3f}\")\n",
    "\n",
    "# Create and examine the model structure\n",
    "model = CandleBurnPredictor(input_features=3)\n",
    "print(\"Model structure:\")\n",
    "print(model)\n",
    "print(f\"\\nNumber of parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚡ THE TRIALS OF MASTERY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial 1: Basic Multi-Variable Mastery\n",
    "- [ ] Loss decreases consistently across all features\n",
    "- [ ] Final loss below 30 (the candles burn predictably)\n",
    "- [ ] Model weights approximately: [2.5, 0.3, -0.8] (±0.3 each)\n",
    "- [ ] Model bias approximately 45 (±5)\n",
    "- [ ] Predictions align well with actual burn times across all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your multi-variable model\n",
    "print(\"🕯️ Beginning the sacred multi-variable training ritual...\")\n",
    "losses = train_multi_variable(model, candle_features, burn_times, epochs=1500, learning_rate=0.001)\n",
    "\n",
    "# Analyze what the model learned\n",
    "analyze_learned_patterns(model)\n",
    "\n",
    "# Generate predictions and visualize\n",
    "with torch.no_grad():\n",
    "    predictions = model(candle_features)\n",
    "\n",
    "print(f\"\\n📊 FINAL PERFORMANCE:\")\n",
    "print(f\"Final loss: {losses[-1]:.4f}\")\n",
    "print(f\"Mean absolute error: {torch.mean(torch.abs(predictions - burn_times)):.2f} minutes\")\n",
    "\n",
    "# Visualize results\n",
    "visualize_candle_wisdom(candle_features, burn_times, predictions)\n",
    "\n",
    "# Plot training progress\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses, color='orange', linewidth=2)\n",
    "plt.title('Multi-Variable Training Progress: Loss Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial 2: Understanding Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_your_wisdom(model):\n",
    "    \"\"\"Master Ao-Tougrad's evaluation of your multi-dimensional understanding.\"\"\"\n",
    "    # Test with specific candle configurations\n",
    "    test_features = torch.tensor([\n",
    "        [50.0, 20.0, 50.0],  # Medium hardness, cool room, medium humidity\n",
    "        [80.0, 15.0, 30.0],  # Hard wax, cold room, low humidity (should burn longest)\n",
    "        [20.0, 30.0, 70.0]   # Soft wax, warm room, high humidity (should burn shortest)\n",
    "    ])\n",
    "    \n",
    "    predictions = model(test_features)\n",
    "    \n",
    "    # Shape validation\n",
    "    assert predictions.shape == (3, 1), f\"Expected shape (3, 1), got {predictions.shape}\"\n",
    "    \n",
    "    # Parameter validation\n",
    "    weights = model.linear.weight.data.squeeze()\n",
    "    bias = model.linear.bias.data.item()\n",
    "    \n",
    "    # Check if learned weights are close to true values\n",
    "    expected_weights = torch.tensor([2.5, 0.3, -0.8])\n",
    "    expected_bias = 45.0\n",
    "    \n",
    "    for i, (learned, expected) in enumerate(zip(weights, expected_weights)):\n",
    "        assert abs(learned - expected) < 0.5, f\"Weight {i} = {learned:.3f}, expected ~{expected:.3f}\"\n",
    "    \n",
    "    assert abs(bias - expected_bias) < 8, f\"Bias = {bias:.2f}, expected ~{expected_bias:.2f}\"\n",
    "    \n",
    "    # Logical validation - hardest wax in cold, dry conditions should burn longest\n",
    "    longest_burn = predictions[1].item()  # Hard wax, cold, dry\n",
    "    shortest_burn = predictions[2].item()  # Soft wax, warm, humid\n",
    "    \n",
    "    assert longest_burn > shortest_burn, \"Hard wax in cold, dry conditions should burn longer!\"\n",
    "    \n",
    "    print(\"🎉 Master Ao-Tougrad emerges from the shadows with approval!\")\n",
    "    print(\"   'You have grasped the flow of multiple gradients, young grasshopper.'\")\n",
    "    print(f\"\\n📊 Test predictions:\")\n",
    "    print(f\"  Medium conditions: {predictions[0].item():.1f} minutes\")\n",
    "    print(f\"  Optimal conditions: {predictions[1].item():.1f} minutes\")\n",
    "    print(f\"  Poor conditions: {predictions[2].item():.1f} minutes\")\n",
    "\n",
    "# Run the wisdom test\n",
    "test_your_wisdom(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🌸 THE FOUR PATHS OF MASTERY: PROGRESSIVE EXTENSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension 1: Cook Oh-Pai-Timizer's Feature Scaling Wisdom\n",
    "*\"In cooking, balancing flavors requires understanding their relative strengths!\"*\n",
    "\n",
    "*Cook Oh-Pai-Timizer approaches with measuring spoons*\n",
    "\n",
    "\"Ah, grasshopper! I see your model learns well, but notice how wax hardness ranges from 20-80 while temperature only goes 15-30? It's like adding a tablespoon of salt versus a teaspoon of pepper - the amounts are different but both affect the final dish! Your gradient descent might struggle because some features dominate others simply due to their scale.\"\n",
    "\n",
    "**NEW CONCEPTS**: Feature normalization, standardization, gradient descent stability  \n",
    "**DIFFICULTY**: +15% (still Dan 1, but with preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(features: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Normalize features to have zero mean and unit variance.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (normalized_features, feature_means, feature_stds)\n",
    "    \"\"\"\n",
    "    # TODO: Implement feature normalization\n",
    "    # Hint: normalized = (features - mean) / std\n",
    "    # Remember: Store means and stds for denormalizing predictions later!\n",
    "    \n",
    "    feature_means = None\n",
    "    feature_stds = None\n",
    "    normalized_features = None\n",
    "    \n",
    "    return normalized_features, feature_means, feature_stds\n",
    "\n",
    "def compare_training_with_without_normalization():\n",
    "    \"\"\"Compare training speed and stability with and without feature normalization.\"\"\"\n",
    "    \n",
    "    # Train without normalization (original data)\n",
    "    model_raw = CandleBurnPredictor(input_features=3)\n",
    "    print(\"Training without normalization...\")\n",
    "    losses_raw = train_multi_variable(model_raw, candle_features, burn_times, \n",
    "                                     epochs=1000, learning_rate=0.001)\n",
    "    \n",
    "    # Train with normalization\n",
    "    normalized_features, means, stds = normalize_features(candle_features)\n",
    "    model_norm = CandleBurnPredictor(input_features=3)\n",
    "    print(\"\\nTraining with normalization...\")\n",
    "    losses_norm = train_multi_variable(model_norm, normalized_features, burn_times, \n",
    "                                      epochs=1000, learning_rate=0.01)  # Can use higher LR!\n",
    "    \n",
    "    # Compare results\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(losses_raw, label='Without Normalization', color='red', alpha=0.7)\n",
    "    plt.plot(losses_norm, label='With Normalization', color='blue', alpha=0.7)\n",
    "    plt.title('Training Comparison: Loss Over Time')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(losses_raw[-200:], label='Without Normalization', color='red', alpha=0.7)\n",
    "    plt.plot(losses_norm[-200:], label='With Normalization', color='blue', alpha=0.7)\n",
    "    plt.title('Final 200 Epochs (Convergence Detail)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n🍜 Cook Oh-Pai-Timizer's Analysis:\")\n",
    "    print(f\"  Final loss without normalization: {losses_raw[-1]:.4f}\")\n",
    "    print(f\"  Final loss with normalization: {losses_norm[-1]:.4f}\")\n",
    "    print(f\"  Improvement: {((losses_raw[-1] - losses_norm[-1]) / losses_raw[-1] * 100):.1f}%\")\n",
    "\n",
    "# TRIAL: Compare normalized vs non-normalized training\n",
    "# SUCCESS: Normalized training converges faster and more stably\n",
    "compare_training_with_without_normalization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension 2: He-Ao-World's Measurement Mishap\n",
    "*\"These old eyes have been recording measurements for decades, but...\"*\n",
    "\n",
    "*He-Ao-World shuffles over looking particularly apologetic*\n",
    "\n",
    "\"Oh dear! I've been helping record the candle data, but I'm afraid I've made some... inconsistencies. Some temperature readings are in Fahrenheit instead of Celsius, some humidity measurements might be absolute instead of relative, and I may have double-counted some wax hardness values. The data is messier now, but perhaps this teaches us about real-world conditions?\"\n",
    "\n",
    "**NEW CONCEPTS**: Outlier detection, robust training, data validation  \n",
    "**DIFFICULTY**: +25% (still Dan 1, but with noisy data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduce_measurement_chaos(features: torch.Tensor, burn_times: torch.Tensor, \n",
    "                               chaos_probability: float = 0.2) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Introduce realistic measurement errors that He-Ao-World might cause.\n",
    "    \n",
    "    Args:\n",
    "        chaos_probability: Fraction of measurements that contain errors\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (corrupted_features, corrupted_burn_times)\n",
    "    \"\"\"\n",
    "    corrupted_features = features.clone()\n",
    "    corrupted_burn_times = burn_times.clone()\n",
    "    \n",
    "    n_samples = len(features)\n",
    "    n_corrupted = int(n_samples * chaos_probability)\n",
    "    \n",
    "    # Randomly select samples to corrupt\n",
    "    corrupted_indices = torch.randperm(n_samples)[:n_corrupted]\n",
    "    \n",
    "    for idx in corrupted_indices:\n",
    "        # Different types of measurement errors He-Ao-World might make\n",
    "        error_type = torch.randint(0, 4, (1,)).item()\n",
    "        \n",
    "        if error_type == 0:  # Temperature in Fahrenheit instead of Celsius\n",
    "            celsius_temp = corrupted_features[idx, 1]\n",
    "            fahrenheit_temp = celsius_temp * 9/5 + 32\n",
    "            corrupted_features[idx, 1] = fahrenheit_temp\n",
    "        elif error_type == 1:  # Double-recorded wax hardness\n",
    "            corrupted_features[idx, 0] *= 2\n",
    "        elif error_type == 2:  # Humidity as absolute instead of relative\n",
    "            corrupted_features[idx, 2] *= 0.3  # Simulate conversion error\n",
    "        else:  # Burn time measurement error\n",
    "            corrupted_burn_times[idx] *= torch.normal(1.0, 0.3, (1,))  # ±30% error\n",
    "    \n",
    "    return corrupted_features, corrupted_burn_times\n",
    "\n",
    "def detect_outliers(features: torch.Tensor, burn_times: torch.Tensor, \n",
    "                   threshold: float = 3.0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Detect outliers using the Z-score method.\n",
    "    \n",
    "    Returns:\n",
    "        Boolean tensor indicating which samples are outliers\n",
    "    \"\"\"\n",
    "    # TODO: Implement outlier detection\n",
    "    # Hint: Calculate Z-scores for each feature and burn time\n",
    "    # Hint: Z-score = (value - mean) / std\n",
    "    # A sample is an outlier if ANY feature has |Z-score| > threshold\n",
    "    \n",
    "    outliers = torch.zeros(len(features), dtype=torch.bool)\n",
    "    \n",
    "    # Check each feature column\n",
    "    for i in range(features.shape[1]):\n",
    "        # TODO: Calculate Z-scores for feature i\n",
    "        # TODO: Mark samples with |Z-score| > threshold as outliers\n",
    "        pass\n",
    "    \n",
    "    # TODO: Also check burn times for outliers\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "def robust_training_comparison():\n",
    "    \"\"\"Compare training on clean vs corrupted data, with and without outlier removal.\"\"\"\n",
    "    \n",
    "    # Generate corrupted data\n",
    "    corrupted_features, corrupted_burn_times = introduce_measurement_chaos(\n",
    "        candle_features, burn_times, chaos_probability=0.15\n",
    "    )\n",
    "    \n",
    "    # Detect outliers\n",
    "    outliers = detect_outliers(corrupted_features, corrupted_burn_times)\n",
    "    clean_mask = ~outliers\n",
    "    \n",
    "    print(f\"🔍 He-Ao-World's Measurement Analysis:\")\n",
    "    print(f\"  Original samples: {len(candle_features)}\")\n",
    "    print(f\"  Detected outliers: {outliers.sum().item()}\")\n",
    "    print(f\"  Clean samples remaining: {clean_mask.sum().item()}\")\n",
    "    \n",
    "    # Train three models: clean, corrupted, and cleaned\n",
    "    models = {\n",
    "        'Original Clean': (candle_features, burn_times),\n",
    "        'With Corruption': (corrupted_features, corrupted_burn_times),\n",
    "        'Outliers Removed': (corrupted_features[clean_mask], corrupted_burn_times[clean_mask])\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, (feats, targets) in models.items():\n",
    "        model = CandleBurnPredictor(input_features=3)\n",
    "        print(f\"\\nTraining on {name} data...\")\n",
    "        losses = train_multi_variable(model, feats, targets, epochs=1000, learning_rate=0.001)\n",
    "        results[name] = {'model': model, 'losses': losses}\n",
    "    \n",
    "    # Visualize training comparison\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    for name, result in results.items():\n",
    "        plt.plot(result['losses'], label=name, alpha=0.8)\n",
    "    plt.title('Training Loss Comparison')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Show data distribution comparison\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.hist(candle_features[:, 1].numpy(), alpha=0.5, label='Original Temp', bins=20)\n",
    "    plt.hist(corrupted_features[:, 1].numpy(), alpha=0.5, label='Corrupted Temp', bins=20)\n",
    "    plt.title('Temperature Distribution: Before/After Corruption')\n",
    "    plt.xlabel('Temperature')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.scatter(candle_features[:, 0], burn_times, alpha=0.5, label='Original', s=20)\n",
    "    plt.scatter(corrupted_features[outliers, 0], corrupted_burn_times[outliers], \n",
    "               alpha=0.8, label='Outliers', s=30, color='red', marker='x')\n",
    "    plt.title('Outliers in Wax Hardness vs Burn Time')\n",
    "    plt.xlabel('Wax Hardness')\n",
    "    plt.ylabel('Burn Time')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    final_losses = [result['losses'][-1] for result in results.values()]\n",
    "    plt.bar(results.keys(), final_losses, color=['green', 'red', 'blue'], alpha=0.7)\n",
    "    plt.title('Final Training Loss Comparison')\n",
    "    plt.ylabel('Final Loss')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n🧹 He-Ao-World's Wisdom:\")\n",
    "    print(f\"  'Sometimes the best way to clean data is to recognize what doesn't belong.'\")\n",
    "    for name, result in results.items():\n",
    "        print(f\"  {name}: Final loss = {result['losses'][-1]:.4f}\")\n",
    "\n",
    "# TRIAL: Handle corrupted measurements and outliers\n",
    "# SUCCESS: Model trained on cleaned data performs better than corrupted data\n",
    "robust_training_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension 3: Master Pai-Torch's Gradient Wisdom\n",
    "*\"The path of learning is not always straight, young grasshopper.\"*\n",
    "\n",
    "*Master Pai-Torch materializes beside you in contemplative silence*\n",
    "\n",
    "\"I observe that you train your networks with steady, unchanging steps. But consider the mountain climber - they take bold strides on gentle slopes, careful steps on steep terrain, and pause to rest when tired. The wise student learns to adjust their pace based on the terrain of the loss landscape.\"\n",
    "\n",
    "**NEW CONCEPTS**: Learning rate scheduling, momentum, adaptive optimization  \n",
    "**DIFFICULTY**: +35% (still Dan 1, but with advanced optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_optimization_comparison():\n",
    "    \"\"\"Compare different optimization strategies for multi-variable regression.\"\"\"\n",
    "    \n",
    "    # Different optimization strategies\n",
    "    optimizers_config = {\n",
    "        'SGD (Basic)': {'optimizer': 'sgd', 'lr': 0.001, 'momentum': 0},\n",
    "        'SGD + Momentum': {'optimizer': 'sgd', 'lr': 0.001, 'momentum': 0.9},\n",
    "        'Adam (Adaptive)': {'optimizer': 'adam', 'lr': 0.01},\n",
    "        'SGD + Schedule': {'optimizer': 'sgd_schedule', 'lr': 0.01, 'momentum': 0.9}\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, config in optimizers_config.items():\n",
    "        print(f\"\\nTraining with {name}...\")\n",
    "        model = CandleBurnPredictor(input_features=3)\n",
    "        \n",
    "        # TODO: Create different optimizers based on config\n",
    "        if config['optimizer'] == 'sgd':\n",
    "            optimizer = None  # TODO: optim.SGD with momentum\n",
    "        elif config['optimizer'] == 'adam':\n",
    "            optimizer = None  # TODO: optim.Adam\n",
    "        elif config['optimizer'] == 'sgd_schedule':\n",
    "            optimizer = None  # TODO: optim.SGD with momentum\n",
    "            scheduler = None  # TODO: optim.lr_scheduler.StepLR\n",
    "        \n",
    "        # Training loop with advanced optimization\n",
    "        criterion = nn.MSELoss()\n",
    "        losses = []\n",
    "        learning_rates = []\n",
    "        \n",
    "        for epoch in range(1000):\n",
    "            # TODO: Standard training step\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(candle_features)\n",
    "            loss = criterion(predictions, burn_times)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # TODO: Update learning rate if using scheduler\n",
    "            if config['optimizer'] == 'sgd_schedule':\n",
    "                pass  # TODO: scheduler.step()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            learning_rates.append(optimizer.param_groups[0]['lr'])\n",
    "            \n",
    "            if (epoch + 1) % 200 == 0:\n",
    "                print(f'Epoch [{epoch+1}/1000], Loss: {loss.item():.4f}, LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "        \n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'losses': losses,\n",
    "            'learning_rates': learning_rates\n",
    "        }\n",
    "    \n",
    "    # Visualize optimization comparison\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss curves\n",
    "    ax = axes[0, 0]\n",
    "    for name, result in results.items():\n",
    "        ax.plot(result['losses'], label=name, alpha=0.8)\n",
    "    ax.set_title('Training Loss Comparison')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning rate schedules\n",
    "    ax = axes[0, 1]\n",
    "    for name, result in results.items():\n",
    "        ax.plot(result['learning_rates'], label=name, alpha=0.8)\n",
    "    ax.set_title('Learning Rate Over Time')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Learning Rate')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Final loss comparison\n",
    "    ax = axes[1, 0]\n",
    "    final_losses = [result['losses'][-1] for result in results.values()]\n",
    "    bars = ax.bar(results.keys(), final_losses, color=['red', 'orange', 'green', 'blue'], alpha=0.7)\n",
    "    ax.set_title('Final Loss Comparison')\n",
    "    ax.set_ylabel('Final Loss')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Convergence speed (epochs to reach 90% of final loss)\n",
    "    ax = axes[1, 1]\n",
    "    convergence_epochs = []\n",
    "    for name, result in results.items():\n",
    "        final_loss = result['losses'][-1]\n",
    "        target_loss = final_loss * 1.1  # 10% above final loss\n",
    "        convergence_epoch = next((i for i, loss in enumerate(result['losses']) if loss <= target_loss), 999)\n",
    "        convergence_epochs.append(convergence_epoch)\n",
    "    \n",
    "    ax.bar(results.keys(), convergence_epochs, color=['red', 'orange', 'green', 'blue'], alpha=0.7)\n",
    "    ax.set_title('Convergence Speed (Epochs to 90% of Final Loss)')\n",
    "    ax.set_ylabel('Epochs')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n🧙 Master Pai-Torch's Analysis:\")\n",
    "    print(f\"  'Each optimization path teaches different lessons about the gradient landscape.'\")\n",
    "    for name, result in results.items():\n",
    "        print(f\"  {name}: Final loss = {result['losses'][-1]:.4f}, Convergence = {convergence_epochs[list(results.keys()).index(name)]} epochs\")\n",
    "\n",
    "# TRIAL: Compare different optimization strategies\n",
    "# SUCCESS: Understand how different optimizers affect training dynamics\n",
    "advanced_optimization_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension 4: Suki's Multi-Dimensional Purring Oracle\n",
    "*\"The temple cat understands patterns that span multiple dimensions.\"*\n",
    "\n",
    "*Suki sits majestically, then performs an elaborate sequence of meows*\n",
    "\n",
    "*Master Pai-Torch translates: \"The sacred cat says your linear wisdom across multiple dimensions is sound, but true understanding comes from seeing how all variables dance together in harmony. Can you predict not just burn time, but also understand which combination of conditions creates the most sacred flames?\"*\n",
    "\n",
    "**NEW CONCEPTS**: Multi-dimensional visualization, feature interaction analysis, model interpretation  \n",
    "**DIFFICULTY**: +45% (still Dan 1, but with deep analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_interactions(model: CandleBurnPredictor):\n",
    "    \"\"\"Analyze how different feature combinations affect candle burn time.\"\"\"\n",
    "    \n",
    "    # Create a grid of feature combinations\n",
    "    wax_range = torch.linspace(20, 80, 20)\n",
    "    temp_range = torch.linspace(15, 30, 20)\n",
    "    humidity_range = torch.linspace(30, 70, 20)\n",
    "    \n",
    "    # TODO: Generate predictions for all combinations\n",
    "    # Hint: Use torch.meshgrid to create all combinations\n",
    "    # Hint: Reshape to create a feature matrix for prediction\n",
    "    \n",
    "    # Find optimal conditions\n",
    "    print(\"🔍 Suki's Multi-Dimensional Analysis:\")\n",
    "    \n",
    "    # TODO: Find the feature combination that gives maximum burn time\n",
    "    # TODO: Find the feature combination that gives minimum burn time\n",
    "    # TODO: Analyze how each feature affects the others\n",
    "    \n",
    "    pass\n",
    "\n",
    "def create_sacred_flame_predictor(model: CandleBurnPredictor):\n",
    "    \"\"\"Create an interactive tool to predict candle burn time for any combination.\"\"\"\n",
    "    \n",
    "    def predict_burn_time(wax_hardness: float, temperature: float, humidity: float) -> float:\n",
    "        \"\"\"Predict burn time for specific candle conditions.\"\"\"\n",
    "        features = torch.tensor([[wax_hardness, temperature, humidity]])\n",
    "        with torch.no_grad():\n",
    "            prediction = model(features)\n",
    "        return prediction.item()\n",
    "    \n",
    "    print(\"🕯️ SACRED FLAME PREDICTOR\")\n",
    "    print(\"Enter candle conditions to predict burn time:\")\n",
    "    \n",
    "    # Test some interesting combinations\n",
    "    test_conditions = [\n",
    "        (80, 15, 30, \"Optimal for long meditation\"),\n",
    "        (20, 30, 70, \"Quick evening prayers\"),\n",
    "        (50, 22, 50, \"Balanced conditions\"),\n",
    "        (70, 20, 40, \"Winter temple setting\"),\n",
    "        (40, 28, 60, \"Summer temple setting\")\n",
    "    ]\n",
    "    \n",
    "    for wax, temp, hum, description in test_conditions:\n",
    "        burn_time = predict_burn_time(wax, temp, hum)\n",
    "        print(f\"  {description}: {burn_time:.1f} minutes\")\n",
    "        print(f\"    (Wax: {wax}%, Temp: {temp}°C, Humidity: {hum}%)\")\n",
    "    \n",
    "    return predict_burn_time\n",
    "\n",
    "def visualize_multi_dimensional_wisdom(model: CandleBurnPredictor):\n",
    "    \"\"\"Create advanced visualizations of the multi-dimensional relationship.\"\"\"\n",
    "    \n",
    "    # Create 3D visualization of feature interactions\n",
    "    fig = plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # 3D scatter plot of actual data\n",
    "    ax1 = fig.add_subplot(221, projection='3d')\n",
    "    scatter = ax1.scatter(candle_features[:, 0], candle_features[:, 1], candle_features[:, 2], \n",
    "                         c=burn_times.squeeze(), cmap='viridis', alpha=0.6)\n",
    "    ax1.set_xlabel('Wax Hardness (%)')\n",
    "    ax1.set_ylabel('Temperature (°C)')\n",
    "    ax1.set_zlabel('Humidity (%)')\n",
    "    ax1.set_title('3D Candle Data (Color = Burn Time)')\n",
    "    plt.colorbar(scatter, ax=ax1, shrink=0.5)\n",
    "    \n",
    "    # Heatmap of burn time vs two features (fixing third)\n",
    "    ax2 = fig.add_subplot(222)\n",
    "    \n",
    "    # TODO: Create a heatmap showing burn time for different wax/temperature combinations\n",
    "    # Fix humidity at 50% and vary wax hardness and temperature\n",
    "    \n",
    "    # Feature importance visualization\n",
    "    ax3 = fig.add_subplot(223)\n",
    "    weights = model.linear.weight.data.squeeze().abs()\n",
    "    feature_names = ['Wax Hardness', 'Temperature', 'Humidity']\n",
    "    bars = ax3.bar(feature_names, weights, color=['brown', 'red', 'blue'], alpha=0.7)\n",
    "    ax3.set_title('Feature Importance (Absolute Weight Values)')\n",
    "    ax3.set_ylabel('Absolute Weight')\n",
    "    \n",
    "    # Residual analysis\n",
    "    ax4 = fig.add_subplot(224)\n",
    "    with torch.no_grad():\n",
    "        predictions = model(candle_features)\n",
    "    residuals = (predictions - burn_times).squeeze()\n",
    "    ax4.scatter(predictions.squeeze(), residuals, alpha=0.6)\n",
    "    ax4.axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
    "    ax4.set_xlabel('Predicted Burn Time')\n",
    "    ax4.set_ylabel('Residuals (Predicted - Actual)')\n",
    "    ax4.set_title('Residual Analysis')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n🐱 Suki's Final Wisdom:\")\n",
    "    print(\"  *Purrs approvingly while demonstrating multi-dimensional understanding*\")\n",
    "    print(f\"  Model R² score: {1 - (residuals.var() / burn_times.var()):.3f}\")\n",
    "    print(f\"  Mean absolute error: {torch.mean(torch.abs(residuals)):.2f} minutes\")\n",
    "\n",
    "# TRIAL: Analyze multi-dimensional feature relationships\n",
    "# SUCCESS: Understand how all features work together to predict burn time\n",
    "analyze_feature_interactions(model)\n",
    "predictor = create_sacred_flame_predictor(model)\n",
    "visualize_multi_dimensional_wisdom(model)\n",
    "\n",
    "# MASTERY: Create your own candle condition and predict its burn time\n",
    "print(\"\\n🎓 MASTERY CHALLENGE:\")\n",
    "print(\"Create your own candle condition and predict its burn time using the predictor function!\")\n",
    "print(\"Example: predictor(60, 25, 45)\")\n",
    "\n",
    "# Test your understanding\n",
    "custom_prediction = predictor(60, 25, 45)\n",
    "print(f\"Your custom candle (60% wax, 25°C, 45% humidity): {custom_prediction:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔥 CORRECTING YOUR FORM: A STANCE IMBALANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Master Pai-Torch observes your multi-variable training ritual with a careful eye. \"Your eager mind grasps the complexity of multiple dimensions, grasshopper, but I sense a disturbance in your gradient flow. Your stance wavers when handling the sacred multi-dimensional tensors.\"\n",
    "\n",
    "A previous disciple left this flawed multi-variable training ritual. The form has become unsteady across multiple dimensions - can you restore proper technique?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsteady_multi_variable_training(model, features, target, epochs=1000):\n",
    "    \"\"\"This multi-variable training stance has lost its balance - your form needs correction! 🥋\"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass with multiple features\n",
    "        predictions = model(features)\n",
    "        loss = criterion(predictions, target)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 200 == 0:\n",
    "            print(f'Epoch {epoch}: Loss = {loss.item():.4f}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Test the flawed training - notice how the loss behaves strangely\n",
    "print(\"🚨 Testing the unsteady training ritual...\")\n",
    "flawed_model = CandleBurnPredictor(input_features=3)\n",
    "flawed_model = unsteady_multi_variable_training(flawed_model, candle_features, burn_times)\n",
    "\n",
    "print(\"\\n🧙 Master Pai-Torch's Guidance:\")\n",
    "print(\"'The undisciplined mind accumulates old thoughts across all dimensions,'\")\n",
    "print(\"'just as the untrained gradient accumulates old directions from multiple features.'\")\n",
    "print(\"\\n🔍 DEBUGGING CHALLENGE:\")\n",
    "print(\"Can you spot the critical error in this multi-variable training ritual?\")\n",
    "print(\"HINT: The Gradient Spirits from ALL features are not being properly dismissed between cycles\")\n",
    "print(\"HINT: In multi-variable regression, accumulated gradients affect ALL weight updates\")\n",
    "print(\"\\n💡 SOLUTION: Add the missing gradient clearing step and observe the difference!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ruliana/pytorch-katas/blob/main/dan_1/temple_scroll_authentication_unrevised.ipynb)\n",
    "\n",
    "## üèÆ The Ancient Scroll Unfurls üèÆ\n",
    "\n",
    "**THE MYSTERY OF THE STOLEN SACRED SCROLLS**\n",
    "\n",
    "Dan Level: 1 (Temple Sweeper) | Time: 60 minutes | Sacred Arts: Tensor Manipulation, Binary Classification, Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìú THE MASTER'S CHALLENGE\n",
    "\n",
    "Young Grasshopper, a crisis has befallen our sacred temple!\n",
    "\n",
    "At dawn, Master Pai-Torch discovered that several ancient scrolls had vanished from the Scroll Repository. These scrolls contain the most sacred teachings of tensor manipulation, passed down through generations of neural network masters.\n",
    "\n",
    "\"The thief is clever,\" Master Pai-Torch whispers, stroking their long beard. \"They have replaced our sacred scrolls with forgeries. But the wise student knows that authentic scrolls carry the subtle marks of true wisdom - patterns invisible to the untrained eye, yet as clear as mountain streams to those who understand the flow of data.\"\n",
    "\n",
    "Suki, the temple cat, sits nearby, occasionally pawing at scattered scroll fragments. Master Pai-Torch nods knowingly. \"Even our sacred cat senses the deception. Observe how Suki's behavior differs when examining authentic fragments versus the forgeries.\"\n",
    "\n",
    "The cat's ears perk up at specific scroll measurements - width, length, ink density, and age markings. Her purring patterns seem to correlate with the authenticity of each fragment.\n",
    "\n",
    "Your sacred duty: Create a mystical classifier that can distinguish between authentic ancient scrolls and cunning forgeries by analyzing their physical characteristics.\n",
    "\n",
    "## üéØ THE SACRED OBJECTIVES\n",
    "\n",
    "- [ ] Master the art of tensor creation and manipulation\n",
    "- [ ] Learn to preprocess sacred data for neural consumption\n",
    "- [ ] Forge your first binary classification model using torch.nn.Linear\n",
    "- [ ] Perform the Sacred Ritual of Training: forward ‚Üí loss ‚Üí backward ‚Üí optimize\n",
    "- [ ] Achieve scroll authentication accuracy worthy of Master Pai-Torch's approval (>85%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç THE SACRED DATA GENERATION SCROLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the sacred seed for reproducible wisdom\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_scroll_data(n_scrolls: int = 500, authenticity_balance: float = 0.6, \n",
    "                        noise_level: float = 0.1, sacred_seed: int = 42) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Generate sacred scroll data based on ancient temple measurements.\n",
    "    \n",
    "    Master Pai-Torch's ancient wisdom reveals that authentic scrolls follow these patterns:\n",
    "    - Width: 15-25 cm (authentic) vs 10-20 cm (forgeries)\n",
    "    - Length: 40-60 cm (authentic) vs 30-50 cm (forgeries)  \n",
    "    - Ink_density: 0.7-0.9 (authentic) vs 0.4-0.7 (forgeries)\n",
    "    - Age_markings: 0.8-1.0 (authentic) vs 0.2-0.6 (forgeries)\n",
    "    \n",
    "    Args:\n",
    "        n_scrolls: Total number of scrolls to generate\n",
    "        authenticity_balance: Fraction of authentic scrolls (0.6 = 60% authentic)\n",
    "        noise_level: Amount of measurement uncertainty\n",
    "        sacred_seed: For reproducible mystical results\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (scroll_features, authenticity_labels)\n",
    "    \"\"\"\n",
    "    torch.manual_seed(sacred_seed)\n",
    "    np.random.seed(sacred_seed)\n",
    "    \n",
    "    n_authentic = int(n_scrolls * authenticity_balance)\n",
    "    n_forgeries = n_scrolls - n_authentic\n",
    "    \n",
    "    # Generate authentic scrolls (label = 1)\n",
    "    authentic_width = torch.uniform(15, 25, (n_authentic, 1))\n",
    "    authentic_length = torch.uniform(40, 60, (n_authentic, 1))\n",
    "    authentic_ink = torch.uniform(0.7, 0.9, (n_authentic, 1))\n",
    "    authentic_age = torch.uniform(0.8, 1.0, (n_authentic, 1))\n",
    "    \n",
    "    authentic_features = torch.cat([authentic_width, authentic_length, authentic_ink, authentic_age], dim=1)\n",
    "    authentic_labels = torch.ones(n_authentic, 1)\n",
    "    \n",
    "    # Generate forgeries (label = 0)\n",
    "    forgery_width = torch.uniform(10, 20, (n_forgeries, 1))\n",
    "    forgery_length = torch.uniform(30, 50, (n_forgeries, 1))\n",
    "    forgery_ink = torch.uniform(0.4, 0.7, (n_forgeries, 1))\n",
    "    forgery_age = torch.uniform(0.2, 0.6, (n_forgeries, 1))\n",
    "    \n",
    "    forgery_features = torch.cat([forgery_width, forgery_length, forgery_ink, forgery_age], dim=1)\n",
    "    forgery_labels = torch.zeros(n_forgeries, 1)\n",
    "    \n",
    "    # Combine all scrolls\n",
    "    all_features = torch.cat([authentic_features, forgery_features], dim=0)\n",
    "    all_labels = torch.cat([authentic_labels, forgery_labels], dim=0)\n",
    "    \n",
    "    # Add mystical noise (measurement uncertainty)\n",
    "    noise = torch.randn_like(all_features) * noise_level\n",
    "    all_features = all_features + noise\n",
    "    \n",
    "    # Ensure measurements stay within reasonable bounds\n",
    "    all_features = torch.clamp(all_features, 0, 100)\n",
    "    \n",
    "    # Shuffle the sacred data\n",
    "    indices = torch.randperm(n_scrolls)\n",
    "    all_features = all_features[indices]\n",
    "    all_labels = all_labels[indices]\n",
    "    \n",
    "    return all_features, all_labels\n",
    "\n",
    "def visualize_scroll_mysteries(features: torch.Tensor, labels: torch.Tensor):\n",
    "    \"\"\"Reveal the hidden patterns in scroll authenticity that even Suki can sense.\"\"\"\n",
    "    \n",
    "    feature_names = ['Width (cm)', 'Length (cm)', 'Ink Density', 'Age Markings']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('üîç The Sacred Scroll Authentication Patterns üîç', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Convert to numpy for visualization\n",
    "    features_np = features.numpy()\n",
    "    labels_np = labels.numpy().flatten()\n",
    "    \n",
    "    for i, (ax, feature_name) in enumerate(zip(axes.flat, feature_names)):\n",
    "        # Separate authentic and forgery data\n",
    "        authentic_data = features_np[labels_np == 1, i]\n",
    "        forgery_data = features_np[labels_np == 0, i]\n",
    "        \n",
    "        # Create histograms\n",
    "        ax.hist(authentic_data, bins=20, alpha=0.7, color='gold', label='Authentic Scrolls', density=True)\n",
    "        ax.hist(forgery_data, bins=20, alpha=0.7, color='red', label='Forgeries', density=True)\n",
    "        \n",
    "        ax.set_xlabel(feature_name)\n",
    "        ax.set_ylabel('Density')\n",
    "        ax.set_title(f'{feature_name} Distribution')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show correlation matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    correlation_matrix = np.corrcoef(features_np.T)\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "                xticklabels=feature_names, yticklabels=feature_names)\n",
    "    plt.title('üåê Sacred Feature Correlations üåê')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üìä Sacred Statistics:\")\n",
    "    print(f\"   Total scrolls: {len(features)}\")\n",
    "    print(f\"   Authentic scrolls: {int(labels.sum().item())} ({labels.mean().item():.1%})\")\n",
    "    print(f\"   Suspected forgeries: {len(features) - int(labels.sum().item())} ({1-labels.mean().item():.1%})\")\n",
    "    print(f\"\")\n",
    "    print(f\"‚ú® Master Pai-Torch nods: 'The patterns are clear to those who know how to see.'\")\n",
    "\n",
    "# Generate the sacred data\n",
    "features, labels = generate_scroll_data(n_scrolls=500, authenticity_balance=0.6, noise_level=0.1)\n",
    "visualize_scroll_mysteries(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÆ FIRST MOVEMENTS: THE SACRED CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScrollAuthenticator(nn.Module):\n",
    "    \"\"\"A mystical artifact for determining scroll authenticity, blessed by Master Pai-Torch.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_features: int = 4):\n",
    "        super(ScrollAuthenticator, self).__init__()\n",
    "        # TODO: Create the Sacred Classification Layer\n",
    "        # Hint: For binary classification, output should be 1 dimension\n",
    "        # Hint: torch.nn.Linear(input_dim, output_dim)\n",
    "        self.authentication_layer = None\n",
    "        \n",
    "        # TODO: Create the Sacred Activation Function\n",
    "        # Hint: For binary classification, sigmoid transforms output to probability\n",
    "        self.sacred_activation = None\n",
    "    \n",
    "    def divine_authenticity(self, scroll_features: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Channel the ancient wisdom to determine scroll authenticity.\"\"\"\n",
    "        # TODO: Pass features through the authentication layer\n",
    "        raw_prediction = None\n",
    "        \n",
    "        # TODO: Apply sacred activation to get probability\n",
    "        authenticity_probability = None\n",
    "        \n",
    "        return authenticity_probability\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"The sacred forward pass - PyTorch expects this method name.\"\"\"\n",
    "        return self.divine_authenticity(x)\n",
    "\n",
    "def preprocess_scroll_data(features: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Prepare the scroll data for mystical analysis.\n",
    "    \n",
    "    Master Pai-Torch wisdom: \"Raw data is like unpolished jade - \n",
    "    it must be refined before its true value can be revealed.\"\n",
    "    \"\"\"\n",
    "    # TODO: Normalize the features using mean and standard deviation\n",
    "    # Hint: normalized = (data - mean) / std\n",
    "    # Hint: Use torch.mean() and torch.std() with dim=0 to get per-feature statistics\n",
    "    \n",
    "    feature_means = None\n",
    "    feature_stds = None\n",
    "    normalized_features = None\n",
    "    \n",
    "    return normalized_features, feature_means, feature_stds\n",
    "\n",
    "def train_scroll_authenticator(model: nn.Module, X: torch.Tensor, y: torch.Tensor,\n",
    "                             epochs: int = 1000, learning_rate: float = 0.01) -> list:\n",
    "    \"\"\"\n",
    "    Train the scroll authentication model through the Sacred Ritual of Learning.\n",
    "    \n",
    "    Returns:\n",
    "        List of loss values during training\n",
    "    \"\"\"\n",
    "    # TODO: Choose the appropriate loss function for binary classification\n",
    "    # Hint: nn.BCELoss() is perfect for binary classification with sigmoid output\n",
    "    sacred_loss_function = None\n",
    "    \n",
    "    # TODO: Choose the optimizer for updating model parameters\n",
    "    # Hint: optim.SGD or optim.Adam work well for beginners\n",
    "    sacred_optimizer = None\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # TODO: CRITICAL - Clear the gradient spirits from the previous iteration\n",
    "        # Hint: Gradients accumulate by default, must be cleared explicitly\n",
    "        \n",
    "        # TODO: Forward pass - get authenticity predictions\n",
    "        predictions = None\n",
    "        \n",
    "        # TODO: Compute the sacred loss\n",
    "        loss = None\n",
    "        \n",
    "        # TODO: Backward pass - compute gradients\n",
    "        # Hint: PyTorch's autograd magic happens here\n",
    "        \n",
    "        # TODO: Update parameters using the optimizer\n",
    "        # Hint: This is where learning actually happens\n",
    "        \n",
    "        loss_history.append(loss.item())\n",
    "        \n",
    "        # Report progress to Master Pai-Torch\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            accuracy = calculate_accuracy(model, X, y)\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy:.2%}')\n",
    "            \n",
    "            if accuracy > 0.85:\n",
    "                print(\"üåü Master Pai-Torch's eyes gleam with approval!\")\n",
    "    \n",
    "    return loss_history\n",
    "\n",
    "def calculate_accuracy(model: nn.Module, X: torch.Tensor, y: torch.Tensor) -> float:\n",
    "    \"\"\"Calculate the accuracy of scroll authentication.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X)\n",
    "        predicted_labels = (predictions > 0.5).float()\n",
    "        accuracy = (predicted_labels == y).float().mean().item()\n",
    "    return accuracy\n",
    "\n",
    "def visualize_training_progress(loss_history: list, model: nn.Module, X: torch.Tensor, y: torch.Tensor):\n",
    "    \"\"\"Visualize the sacred journey of learning.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot loss over time\n",
    "    ax1.plot(loss_history, color='purple', linewidth=2)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Sacred Loss')\n",
    "    ax1.set_title('üî• The Path of Learning: Loss Reduction üî•')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot predictions vs actual\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X).numpy().flatten()\n",
    "        actual = y.numpy().flatten()\n",
    "        \n",
    "        # Separate authentic and forgery predictions\n",
    "        authentic_preds = predictions[actual == 1]\n",
    "        forgery_preds = predictions[actual == 0]\n",
    "        \n",
    "        ax2.hist(authentic_preds, bins=20, alpha=0.7, color='gold', label='Authentic Scrolls', density=True)\n",
    "        ax2.hist(forgery_preds, bins=20, alpha=0.7, color='red', label='Forgeries', density=True)\n",
    "        ax2.axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Decision Threshold')\n",
    "        ax2.set_xlabel('Predicted Authenticity Probability')\n",
    "        ax2.set_ylabel('Density')\n",
    "        ax2.set_title('üéØ Authenticity Predictions Distribution üéØ')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    final_accuracy = calculate_accuracy(model, X, y)\n",
    "    print(f\"\\nüìä Final Sacred Statistics:\")\n",
    "    print(f\"   Final Loss: {loss_history[-1]:.4f}\")\n",
    "    print(f\"   Final Accuracy: {final_accuracy:.2%}\")\n",
    "    \n",
    "    if final_accuracy > 0.85:\n",
    "        print(f\"\\nüéâ Master Pai-Torch bows respectfully: 'Your understanding of the sacred patterns grows strong, young grasshopper.'\")\n",
    "    else:\n",
    "        print(f\"\\nü§î Master Pai-Torch strokes beard thoughtfully: 'The path to wisdom requires more practice. Consider adjusting your learning rate or training longer.'\")\n",
    "\n",
    "# Create your model (uncomment when ready to test)\n",
    "# model = ScrollAuthenticator(input_features=4)\n",
    "# print(\"‚ú® Sacred Scroll Authenticator created!\")\n",
    "# print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° THE TRIALS OF MASTERY\n",
    "\n",
    "### Trial 1: Basic Scroll Authentication\n",
    "Complete the TODOs above to create a working scroll authenticator:\n",
    "- [ ] Model achieves >85% accuracy on the training data\n",
    "- [ ] Loss consistently decreases during training\n",
    "- [ ] Predictions clearly separate authentic scrolls from forgeries\n",
    "- [ ] Model parameters are reasonable (weights not too large/small)\n",
    "\n",
    "### Trial 2: Understanding Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_your_scroll_wisdom(model: nn.Module, features: torch.Tensor, labels: torch.Tensor):\n",
    "    \"\"\"Master Pai-Torch's evaluation of your sacred understanding.\"\"\"\n",
    "    \n",
    "    print(\"üîç Testing the Sacred Scroll Authenticator...\")\n",
    "    \n",
    "    # Test 1: Model architecture\n",
    "    test_input = torch.randn(5, 4)  # 5 scrolls, 4 features each\n",
    "    predictions = model(test_input)\n",
    "    assert predictions.shape == (5, 1), f\"Expected shape (5, 1), got {predictions.shape}\"\n",
    "    assert torch.all(predictions >= 0) and torch.all(predictions <= 1), \"Predictions must be probabilities between 0 and 1\"\n",
    "    \n",
    "    # Test 2: Accuracy requirement\n",
    "    accuracy = calculate_accuracy(model, features, labels)\n",
    "    assert accuracy >= 0.85, f\"Accuracy {accuracy:.2%} is below Master Pai-Torch's standard (85%)\"\n",
    "    \n",
    "    # Test 3: Model parameters are reasonable\n",
    "    for name, param in model.named_parameters():\n",
    "        assert torch.all(torch.abs(param) < 10), f\"Parameter {name} has extreme values - check your learning rate!\"\n",
    "    \n",
    "    # Test 4: Confident predictions\n",
    "    with torch.no_grad():\n",
    "        all_predictions = model(features)\n",
    "        confident_predictions = ((all_predictions > 0.7) | (all_predictions < 0.3)).float().mean()\n",
    "        assert confident_predictions > 0.6, \"Model should be confident in most predictions\"\n",
    "    \n",
    "    print(\"üéâ All trials passed! Master Pai-Torch nods with deep approval.\")\n",
    "    print(f\"   üìä Final accuracy: {accuracy:.2%}\")\n",
    "    print(f\"   üéØ Confident predictions: {confident_predictions:.2%}\")\n",
    "    print(f\"   ‚ú® 'Your understanding of tensor flows grows strong, young grasshopper.'\")\n",
    "\n",
    "# Run the test when your model is ready\n",
    "# test_your_scroll_wisdom(model, features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üå∏ THE FOUR PATHS OF MASTERY: PROGRESSIVE EXTENSIONS\n",
    "\n",
    "### Extension 1: Suki's Attention to Detail\n",
    "*\"The sacred cat notices patterns that even trained eyes miss.\"*\n",
    "\n",
    "Suki sits elegantly beside the scroll fragments, her emerald eyes tracking something the human eye cannot see. Her whiskers twitch as she examines each scroll, and her purring intensity seems to correlate with authenticity confidence.\n",
    "\n",
    "Master Pai-Torch observes the feline oracle with reverence. \"The cat perceives not just the measurements, but the relationships between them. Notice how authentic scrolls have consistent ratios - width to length, ink density to age markings. The forgers understand individual features but miss the sacred harmonies.\"\n",
    "\n",
    "**NEW CONCEPTS**: Feature engineering, ratio features, model interpretation\n",
    "**DIFFICULTY**: +15% (still Dan 1, but with feature relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_features(features: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create advanced features that capture scroll relationships, as noticed by Suki.\n",
    "    \n",
    "    Args:\n",
    "        features: Original features [width, length, ink_density, age_markings]\n",
    "    \n",
    "    Returns:\n",
    "        Enhanced features with ratios and interactions\n",
    "    \"\"\"\n",
    "    # TODO: Extract individual features\n",
    "    width = features[:, 0:1]\n",
    "    length = features[:, 1:2] \n",
    "    ink_density = features[:, 2:3]\n",
    "    age_markings = features[:, 3:4]\n",
    "    \n",
    "    # TODO: Create ratio features that Suki notices\n",
    "    # Hint: Authentic scrolls have width/length ratio around 0.35-0.45\n",
    "    # Hint: Authentic scrolls have ink_density/age_markings ratio around 0.8-1.0\n",
    "    width_length_ratio = None\n",
    "    ink_age_ratio = None\n",
    "    \n",
    "    # TODO: Create interaction features\n",
    "    # Hint: Multiply features that might interact\n",
    "    ink_age_interaction = None\n",
    "    \n",
    "    # TODO: Combine all features\n",
    "    enhanced_features = None\n",
    "    \n",
    "    return enhanced_features\n",
    "\n",
    "# TRIAL: Train your model with enhanced features\n",
    "# SUCCESS: Achieve >90% accuracy with the enhanced feature set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension 2: Master Pai-Torch's Validation Wisdom\n",
    "*\"A model that knows only its training scrolls is like a monk who never leaves the temple.\"*\n",
    "\n",
    "Master Pai-Torch materializes from the shadows, holding a separate collection of scrolls. \"Young grasshopper, your authenticator shows promise on the scrolls it has studied. But true wisdom reveals itself when facing the unknown.\"\n",
    "\n",
    "The master's eyes gleam with ancient knowledge. \"The wise student always holds back some scrolls for final testing. This is the Sacred Validation - the true measure of understanding versus mere memorization.\"\n",
    "\n",
    "**NEW CONCEPTS**: Train/validation split, model evaluation, overfitting awareness\n",
    "**DIFFICULTY**: +25% (still Dan 1, but with proper evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sacred_train_validation_split(features: torch.Tensor, labels: torch.Tensor, \n",
    "                                validation_ratio: float = 0.2) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Split the sacred scrolls into training and validation sets.\n",
    "    \n",
    "    Args:\n",
    "        features: All scroll features\n",
    "        labels: All scroll labels\n",
    "        validation_ratio: Fraction to hold for validation\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (train_features, train_labels, val_features, val_labels)\n",
    "    \"\"\"\n",
    "    # TODO: Calculate split indices\n",
    "    n_total = len(features)\n",
    "    n_validation = int(n_total * validation_ratio)\n",
    "    n_train = n_total - n_validation\n",
    "    \n",
    "    # TODO: Create random indices for splitting\n",
    "    # Hint: Use torch.randperm() for random permutation\n",
    "    indices = None\n",
    "    \n",
    "    # TODO: Split the data\n",
    "    train_indices = None\n",
    "    val_indices = None\n",
    "    \n",
    "    train_features = features[train_indices]\n",
    "    train_labels = labels[train_indices]\n",
    "    val_features = features[val_indices]\n",
    "    val_labels = labels[val_indices]\n",
    "    \n",
    "    return train_features, train_labels, val_features, val_labels\n",
    "\n",
    "def train_with_validation(model: nn.Module, train_X: torch.Tensor, train_y: torch.Tensor,\n",
    "                        val_X: torch.Tensor, val_y: torch.Tensor, epochs: int = 1000) -> dict:\n",
    "    \"\"\"\n",
    "    Train with validation monitoring to prevent overfitting.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with training history\n",
    "    \"\"\"\n",
    "    # TODO: Set up training components\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_accuracy': [],\n",
    "        'val_accuracy': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # TODO: Training phase\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        train_pred = model(train_X)\n",
    "        train_loss = criterion(train_pred, train_y)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # TODO: Validation phase (no gradient computation)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(val_X)\n",
    "            val_loss = criterion(val_pred, val_y)\n",
    "        \n",
    "        # TODO: Record metrics\n",
    "        history['train_loss'].append(train_loss.item())\n",
    "        history['val_loss'].append(val_loss.item())\n",
    "        history['train_accuracy'].append(calculate_accuracy(model, train_X, train_y))\n",
    "        history['val_accuracy'].append(calculate_accuracy(model, val_X, val_y))\n",
    "        \n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}]')\n",
    "            print(f'  Train Loss: {train_loss.item():.4f}, Train Acc: {history[\"train_accuracy\"][-1]:.2%}')\n",
    "            print(f'  Val Loss: {val_loss.item():.4f}, Val Acc: {history[\"val_accuracy\"][-1]:.2%}')\n",
    "    \n",
    "    return history\n",
    "\n",
    "# TRIAL: Train with validation split and monitor overfitting\n",
    "# SUCCESS: Validation accuracy stays close to training accuracy (gap < 5%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension 3: Master Pai-Torch's Threshold Meditation\n",
    "*\"The line between authentic and false is not always at the mountain's peak.\"*\n",
    "\n",
    "Master Pai-Torch sits in contemplative silence, examining the model's predictions. \"Young grasshopper, your mystical classifier speaks in probabilities, but decisions require thresholds. The path of 0.5 is traditional, but not always optimal.\"\n",
    "\n",
    "The master's voice carries ancient wisdom. \"Consider the cost of error - is it worse to reject an authentic scroll as forgery, or to accept a forgery as authentic? The Sacred Threshold must balance these cosmic forces.\"\n",
    "\n",
    "**NEW CONCEPTS**: Decision thresholds, precision/recall, ROC curves\n",
    "**DIFFICULTY**: +35% (still Dan 1, but thinking beyond accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_threshold(model: nn.Module, X: torch.Tensor, y: torch.Tensor, \n",
    "                          threshold_range: torch.Tensor = None) -> dict:\n",
    "    \"\"\"\n",
    "    Find the optimal decision threshold for scroll authentication.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained scroll authenticator\n",
    "        X: Features\n",
    "        y: True labels\n",
    "        threshold_range: Range of thresholds to test\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with threshold analysis results\n",
    "    \"\"\"\n",
    "    if threshold_range is None:\n",
    "        threshold_range = torch.linspace(0.1, 0.9, 50)\n",
    "    \n",
    "    # TODO: Get model predictions\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X)\n",
    "    \n",
    "    results = {\n",
    "        'thresholds': [],\n",
    "        'accuracies': [],\n",
    "        'precisions': [],\n",
    "        'recalls': [],\n",
    "        'f1_scores': []\n",
    "    }\n",
    "    \n",
    "    for threshold in threshold_range:\n",
    "        # TODO: Apply threshold to get binary predictions\n",
    "        binary_preds = (predictions > threshold).float()\n",
    "        \n",
    "        # TODO: Calculate metrics\n",
    "        # True Positives: correctly identified authentic scrolls\n",
    "        # False Positives: forgeries classified as authentic\n",
    "        # False Negatives: authentic scrolls classified as forgeries\n",
    "        \n",
    "        tp = ((binary_preds == 1) & (y == 1)).sum().item()\n",
    "        fp = ((binary_preds == 1) & (y == 0)).sum().item()\n",
    "        fn = ((binary_preds == 0) & (y == 1)).sum().item()\n",
    "        tn = ((binary_preds == 0) & (y == 0)).sum().item()\n",
    "        \n",
    "        # TODO: Calculate derived metrics\n",
    "        accuracy = None  # (tp + tn) / (tp + fp + fn + tn)\n",
    "        precision = None  # tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = None  # tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1 = None  # 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        \n",
    "        results['thresholds'].append(threshold.item())\n",
    "        results['accuracies'].append(accuracy)\n",
    "        results['precisions'].append(precision)\n",
    "        results['recalls'].append(recall)\n",
    "        results['f1_scores'].append(f1)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def visualize_threshold_analysis(results: dict):\n",
    "    \"\"\"Visualize the threshold analysis results.\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(results['thresholds'], results['accuracies'], 'b-', linewidth=2, label='Accuracy')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy vs Threshold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(results['thresholds'], results['precisions'], 'g-', linewidth=2, label='Precision')\n",
    "    plt.plot(results['thresholds'], results['recalls'], 'r-', linewidth=2, label='Recall')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Precision vs Recall')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(results['thresholds'], results['f1_scores'], 'purple', linewidth=2, label='F1 Score')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('F1 Score vs Threshold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(results['recalls'], results['precisions'], 'orange', linewidth=2)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find optimal threshold\n",
    "    best_f1_idx = np.argmax(results['f1_scores'])\n",
    "    optimal_threshold = results['thresholds'][best_f1_idx]\n",
    "    \n",
    "    print(f\"üéØ Master Pai-Torch's Threshold Wisdom:\")\n",
    "    print(f\"   Optimal threshold: {optimal_threshold:.3f}\")\n",
    "    print(f\"   Best F1 Score: {results['f1_scores'][best_f1_idx]:.3f}\")\n",
    "    print(f\"   Accuracy at optimal: {results['accuracies'][best_f1_idx]:.3f}\")\n",
    "    print(f\"   Precision at optimal: {results['precisions'][best_f1_idx]:.3f}\")\n",
    "    print(f\"   Recall at optimal: {results['recalls'][best_f1_idx]:.3f}\")\n",
    "\n",
    "# TRIAL: Find optimal threshold for your model\n",
    "# SUCCESS: Achieve better F1 score than default 0.5 threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension 4: Suki's Uncertainty Principle\n",
    "*\"The wise cat knows when to be certain and when to remain curious.\"*\n",
    "\n",
    "Suki approaches a particularly puzzling scroll fragment, her whiskers twitching with uncertainty. She circles it several times, occasionally pawing at it, but never commits to a definitive judgment.\n",
    "\n",
    "Master Pai-Torch observes with great interest. \"See how our sacred oracle demonstrates wisdom beyond prediction? Some scrolls lie in the realm of uncertainty - neither clearly authentic nor obviously false. The wise authenticator acknowledges this uncertainty rather than forcing false confidence.\"\n",
    "\n",
    "**NEW CONCEPTS**: Prediction confidence, uncertainty quantification, model calibration\n",
    "**DIFFICULTY**: +45% (still Dan 1, but thinking about uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_prediction_confidence(model: nn.Module, X: torch.Tensor, y: torch.Tensor) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze how confident the model is in its predictions.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained scroll authenticator\n",
    "        X: Features\n",
    "        y: True labels\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with confidence analysis\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X)\n",
    "    \n",
    "    # TODO: Calculate confidence scores\n",
    "    # Confidence is distance from 0.5 (uncertain)\n",
    "    # High confidence: close to 0 or 1\n",
    "    # Low confidence: close to 0.5\n",
    "    confidence_scores = None  # torch.abs(predictions - 0.5) * 2\n",
    "    \n",
    "    # TODO: Categorize predictions by confidence\n",
    "    high_confidence = confidence_scores > 0.8\n",
    "    medium_confidence = (confidence_scores > 0.4) & (confidence_scores <= 0.8)\n",
    "    low_confidence = confidence_scores <= 0.4\n",
    "    \n",
    "    results = {\n",
    "        'predictions': predictions,\n",
    "        'confidence_scores': confidence_scores,\n",
    "        'high_confidence': high_confidence,\n",
    "        'medium_confidence': medium_confidence,\n",
    "        'low_confidence': low_confidence\n",
    "    }\n",
    "    \n",
    "    # TODO: Calculate accuracy for each confidence group\n",
    "    for conf_name, conf_mask in [('high', high_confidence), ('medium', medium_confidence), ('low', low_confidence)]:\n",
    "        if conf_mask.sum() > 0:\n",
    "            conf_predictions = predictions[conf_mask]\n",
    "            conf_labels = y[conf_mask]\n",
    "            conf_accuracy = calculate_accuracy_from_predictions(conf_predictions, conf_labels)\n",
    "            results[f'{conf_name}_confidence_accuracy'] = conf_accuracy\n",
    "            results[f'{conf_name}_confidence_count'] = conf_mask.sum().item()\n",
    "    \n",
    "    return results\n",
    "\n",
    "def calculate_accuracy_from_predictions(predictions: torch.Tensor, labels: torch.Tensor) -> float:\n",
    "    \"\"\"Calculate accuracy from raw predictions and labels.\"\"\"\n",
    "    binary_preds = (predictions > 0.5).float()\n",
    "    accuracy = (binary_preds == labels).float().mean().item()\n",
    "    return accuracy\n",
    "\n",
    "def visualize_confidence_analysis(results: dict, X: torch.Tensor, y: torch.Tensor):\n",
    "    \"\"\"Visualize the confidence analysis results.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Confidence distribution\n",
    "    axes[0, 0].hist(results['confidence_scores'].numpy(), bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].set_xlabel('Confidence Score')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].set_title('Distribution of Prediction Confidence')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Confidence vs accuracy\n",
    "    predictions = results['predictions'].numpy().flatten()\n",
    "    labels = y.numpy().flatten()\n",
    "    confidence = results['confidence_scores'].numpy().flatten()\n",
    "    \n",
    "    correct = (((predictions > 0.5) == labels).astype(float))\n",
    "    axes[0, 1].scatter(confidence, correct, alpha=0.6, c=correct, cmap='coolwarm')\n",
    "    axes[0, 1].set_xlabel('Confidence Score')\n",
    "    axes[0, 1].set_ylabel('Correct Prediction (1) vs Incorrect (0)')\n",
    "    axes[0, 1].set_title('Confidence vs Correctness')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Confidence by true label\n",
    "    authentic_conf = confidence[labels == 1]\n",
    "    forgery_conf = confidence[labels == 0]\n",
    "    \n",
    "    axes[1, 0].hist(authentic_conf, bins=20, alpha=0.7, color='gold', label='Authentic', density=True)\n",
    "    axes[1, 0].hist(forgery_conf, bins=20, alpha=0.7, color='red', label='Forgery', density=True)\n",
    "    axes[1, 0].set_xlabel('Confidence Score')\n",
    "    axes[1, 0].set_ylabel('Density')\n",
    "    axes[1, 0].set_title('Confidence by True Label')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Confidence group statistics\n",
    "    conf_groups = ['high', 'medium', 'low']\n",
    "    accuracies = [results.get(f'{group}_confidence_accuracy', 0) for group in conf_groups]\n",
    "    counts = [results.get(f'{group}_confidence_count', 0) for group in conf_groups]\n",
    "    \n",
    "    bars = axes[1, 1].bar(conf_groups, accuracies, alpha=0.7, color=['green', 'orange', 'red'])\n",
    "    axes[1, 1].set_ylabel('Accuracy')\n",
    "    axes[1, 1].set_title('Accuracy by Confidence Group')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for bar, count in zip(bars, counts):\n",
    "        axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                       f'n={count}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üê± Suki's Confidence Wisdom:\")\n",
    "    print(f\"   High confidence predictions: {results['high_confidence_count']} ({results['high_confidence_accuracy']:.2%} accurate)\")\n",
    "    print(f\"   Medium confidence predictions: {results['medium_confidence_count']} ({results['medium_confidence_accuracy']:.2%} accurate)\")\n",
    "    print(f\"   Low confidence predictions: {results['low_confidence_count']} ({results['low_confidence_accuracy']:.2%} accurate)\")\n",
    "    \n",
    "    if results['high_confidence_accuracy'] > results['low_confidence_accuracy']:\n",
    "        print(f\"   ‚ú® Excellent! Your model shows proper confidence calibration.\")\n",
    "    else:\n",
    "        print(f\"   ü§î Hmm... Your model might need better confidence calibration.\")\n",
    "\n",
    "# TRIAL: Analyze your model's confidence patterns\n",
    "# SUCCESS: High confidence predictions are more accurate than low confidence ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî• CORRECTING YOUR FORM: A STANCE IMBALANCE\n",
    "\n",
    "Master Pai-Torch observes your training ritual with a careful eye. \"Your eager mind races ahead of your disciplined form, grasshopper. See how your gradient flow stance wavers?\"\n",
    "\n",
    "A previous disciple left this flawed scroll authentication ritual. Your form has become unsteady - can you restore proper technique?\n",
    "\n",
    "**üö® The Flawed Training Ritual:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsteady_training_ritual(model, X, y, epochs=1000):\n",
    "    \"\"\"This training stance has lost its balance - your form needs correction! ü•ã\"\"\"\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        predictions = model(X)\n",
    "        loss = criterion(predictions, y)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch {epoch}: Loss = {loss.item():.4f}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ü§î Master Pai-Torch's Guidance:\n",
    "# \"The gradient spirits accumulate like dust on temple floors.\n",
    "#  What happens when dust is not swept away between cleaning sessions?\"\n",
    "\n",
    "# DEBUGGING CHALLENGE:\n",
    "# 1. Run this flawed code and observe the loss behavior\n",
    "# 2. Identify what's wrong with the training loop\n",
    "# 3. Fix the issue and compare the results\n",
    "# 4. Understand why this fix is critical for neural network training\n",
    "\n",
    "# HINTS:\n",
    "# - Watch how the loss changes (or doesn't change) over epochs\n",
    "# - Think about what happens to gradients between iterations\n",
    "# - Consider: what does optimizer.zero_grad() do and why is it important?\n",
    "\n",
    "# SOLUTION TEST:\n",
    "# Your corrected version should show steadily decreasing loss\n",
    "# The flawed version will likely show erratic or non-decreasing loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì FINAL WISDOM FROM MASTER PAI-TORCH\n",
    "\n",
    "As you complete this sacred trial, Master Pai-Torch approaches with a gentle smile.\n",
    "\n",
    "\"Young grasshopper, you have taken your first steps into the vast world of tensor manipulation and neural wisdom. Today you learned that:\n",
    "\n",
    "- **Tensors are the sacred language** through which all neural wisdom flows\n",
    "- **Data preprocessing** is like preparing tea - the quality of preparation determines the quality of the final result\n",
    "- **Binary classification** teaches the fundamental art of decision-making under uncertainty\n",
    "- **Training loops** are meditation in motion - forward, compute, backward, update, repeat\n",
    "- **Gradient management** requires discipline - the spirits must be cleared between each iteration\n",
    "\n",
    "Remember, young one: every master was once a beginner. Every expert was once a grasshopper. Your journey in the neural arts has just begun.\n",
    "\n",
    "The path ahead holds many mysteries - deeper networks, more complex architectures, and greater challenges. But with each kata, your understanding grows stronger.\n",
    "\n",
    "Continue your practice, and may your gradients always flow in the direction of wisdom.\"\n",
    "\n",
    "üèÆ *The ancient scroll slowly furls, its teachings now part of your growing neural wisdom* üèÆ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
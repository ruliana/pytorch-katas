{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "temple-header",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ruliana/pytorch-katas/blob/main/dan_1/kata_03_he_ao_world_accident_pattern_detector_unrevised.ipynb)\n",
    "\n",
    "## 🏮 The Ancient Scroll Unfurls 🏮\n",
    "\n",
    "**THE MYSTERY OF CONVENIENT CLUMSINESS: DETECTING HIDDEN PATTERNS IN APPARENT ACCIDENTS**\n",
    "\n",
    "Dan Level: 1 (Temple Sweeper) | Time: 45 minutes | Sacred Arts: Multi-variable Classification, Binary Cross-Entropy, Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenge-description",
   "metadata": {},
   "source": [
    "## 📜 THE CHALLENGE\n",
    "\n",
    "Master Ao-Tougrad sits in the temple's shadowy corner, watching He-Ao-World apologetically clean up yet another spilled ink incident that occurred exactly when a difficult scroll needed \"accidental\" correction. \"Young grasshopper,\" the master whispers, \"observe how the temple janitor's daily mishaps follow curious patterns. Some accidents stem from genuine clumsiness of aged hands, but others... others happen with suspiciously perfect timing. The wise observer notices that time of day, location, presence of witnesses, and task complexity all whisper secrets about the true nature of each incident.\"\n",
    "\n",
    "He-Ao-World shuffles by, bumping into a meditation cushion at precisely the moment when a particularly boring sutra needs interrupting, then glances around nervously before apologizing profusely. Master Ao-Tougrad's eyes glint with knowing amusement. \"Your task, grasshopper, is to build a sacred classifier that can distinguish between innocent fumbles and masterfully timed 'accidents.' Train your neural network to recognize the hidden patterns using multiple clues simultaneously - for the truth lies not in any single observation, but in the convergence of many seemingly unrelated details.\"\n",
    "\n",
    "## 🎯 THE SACRED OBJECTIVES\n",
    "\n",
    "- [ ] Master multi-variable input processing with PyTorch tensors\n",
    "- [ ] Implement binary classification using multiple features simultaneously  \n",
    "- [ ] Learn to handle different types of input data (categorical and continuous)\n",
    "- [ ] Practice proper gradient flow management in multi-feature models\n",
    "- [ ] Understand how multiple inputs combine to influence predictions\n",
    "- [ ] Build intuition for feature importance in classification decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 FIRST CELL - ALL IMPORTS AND CONFIGURATION\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "# Set reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Global configuration constants\n",
    "DEFAULT_CHAOS_LEVEL = 0.1\n",
    "SACRED_SEED = 42\n",
    "N_FEATURES = 4  # time_of_day, location, witnesses, task_complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🕵️ THE SACRED ACCIDENT OBSERVATION SCROLL\n",
    "\n",
    "def generate_he_ao_accident_data(n_observations: int = 200, chaos_level: float = 0.1, \n",
    "                                sacred_seed: int = 42) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Generate observations of He-Ao-World's daily incidents around the temple.\n",
    "    \n",
    "    Ancient wisdom suggests that suspicious timing follows these patterns:\n",
    "    - Morning hours (6-9 AM): 70% more likely to be suspicious (masters need interruptions)\n",
    "    - Sacred hall locations: 60% more likely to be suspicious (important ceremonies)\n",
    "    - No witnesses present: 80% more likely to be suspicious (perfect cover)\n",
    "    - Complex tasks: 90% more likely to be suspicious (creates better distractions)\n",
    "    \n",
    "    Features:\n",
    "    0. time_of_day: Hour of day (0-23)\n",
    "    1. location: Temple location (0=courtyard, 1=sacred_hall, 2=kitchen, 3=library)\n",
    "    2. witnesses: Number of witnesses present (0-5)\n",
    "    3. task_complexity: Complexity of nearby task (0.0-1.0)\n",
    "    \n",
    "    Args:\n",
    "        n_observations: Number of accidents to simulate\n",
    "        chaos_level: Amount of randomness in the patterns (0.0 = perfectly predictable, 1.0 = pure chaos)\n",
    "        sacred_seed: Ensures consistent randomness for reproducible wisdom\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (features, is_suspicious) as sacred tensors\n",
    "        features shape: (n_observations, 4)\n",
    "        is_suspicious shape: (n_observations, 1) - 1 for suspicious, 0 for genuine\n",
    "    \"\"\"\n",
    "    torch.manual_seed(sacred_seed)\n",
    "    np.random.seed(sacred_seed)\n",
    "    \n",
    "    # Generate random features\n",
    "    time_of_day = torch.randint(0, 24, (n_observations,)).float()  # 0-23 hours\n",
    "    location = torch.randint(0, 4, (n_observations,)).float()      # 0-3 locations  \n",
    "    witnesses = torch.randint(0, 6, (n_observations,)).float()     # 0-5 witnesses\n",
    "    task_complexity = torch.rand(n_observations)                   # 0.0-1.0 complexity\n",
    "    \n",
    "    # Stack features together\n",
    "    features = torch.stack([time_of_day, location, witnesses, task_complexity], dim=1)\n",
    "    \n",
    "    # Calculate suspicion probability based on He-Ao-World's hidden patterns\n",
    "    base_suspicion = 0.3  # 30% base chance of being suspicious\n",
    "    \n",
    "    # Morning hours (6-9 AM) increase suspicion by 0.4\n",
    "    morning_bonus = torch.where((time_of_day >= 6) & (time_of_day <= 9), 0.4, 0.0)\n",
    "    \n",
    "    # Sacred hall (location=1) increases suspicion by 0.3  \n",
    "    location_bonus = torch.where(location == 1, 0.3, 0.0)\n",
    "    \n",
    "    # No witnesses (witnesses=0) increases suspicion by 0.5\n",
    "    witness_bonus = torch.where(witnesses == 0, 0.5, 0.0)\n",
    "    \n",
    "    # High task complexity increases suspicion\n",
    "    complexity_bonus = task_complexity * 0.4  # Up to 0.4 bonus\n",
    "    \n",
    "    # Calculate final suspicion probability\n",
    "    suspicion_prob = base_suspicion + morning_bonus + location_bonus + witness_bonus + complexity_bonus\n",
    "    \n",
    "    # Add chaos to make the pattern learnable but not obvious\n",
    "    chaos = torch.randn(n_observations) * chaos_level * 0.2\n",
    "    suspicion_prob = torch.clamp(suspicion_prob + chaos, 0.0, 1.0)\n",
    "    \n",
    "    # Generate binary labels based on probability\n",
    "    is_suspicious = torch.bernoulli(suspicion_prob).unsqueeze(1)\n",
    "    \n",
    "    return features, is_suspicious\n",
    "\n",
    "def visualize_accident_patterns(features: torch.Tensor, is_suspicious: torch.Tensor):\n",
    "    \"\"\"Reveal the hidden patterns in He-Ao-World's accidents.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    genuine_mask = (is_suspicious.squeeze() == 0)\n",
    "    suspicious_mask = (is_suspicious.squeeze() == 1)\n",
    "    \n",
    "    feature_names = ['Time of Day (Hour)', 'Location', 'Witnesses Present', 'Task Complexity']\n",
    "    location_labels = ['Courtyard', 'Sacred Hall', 'Kitchen', 'Library']\n",
    "    \n",
    "    for i, (ax, feature_name) in enumerate(zip(axes.flat, feature_names)):\n",
    "        genuine_data = features[genuine_mask, i]\n",
    "        suspicious_data = features[suspicious_mask, i]\n",
    "        \n",
    "        if i == 1:  # Location - use bar chart\n",
    "            locations = [0, 1, 2, 3]\n",
    "            genuine_counts = [(genuine_data == loc).sum().item() for loc in locations]\n",
    "            suspicious_counts = [(suspicious_data == loc).sum().item() for loc in locations]\n",
    "            \n",
    "            x = np.arange(len(locations))\n",
    "            width = 0.35\n",
    "            \n",
    "            ax.bar(x - width/2, genuine_counts, width, label='Genuine Accidents', \n",
    "                   color='lightblue', alpha=0.7)\n",
    "            ax.bar(x + width/2, suspicious_counts, width, label='Suspicious Timing', \n",
    "                   color='red', alpha=0.7)\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(location_labels)\n",
    "            ax.set_ylabel('Count')\n",
    "        else:\n",
    "            ax.hist(genuine_data.numpy(), bins=20, alpha=0.6, label='Genuine Accidents', \n",
    "                   color='lightblue', density=True)\n",
    "            ax.hist(suspicious_data.numpy(), bins=20, alpha=0.6, label='Suspicious Timing', \n",
    "                   color='red', density=True)\n",
    "            ax.set_ylabel('Density')\n",
    "        \n",
    "        ax.set_title(f'{feature_name} Distribution')\n",
    "        ax.set_xlabel(feature_name)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(\"He-Ao-World's Accident Patterns: Genuine vs Suspicious\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_predictions(features: torch.Tensor, target: torch.Tensor, \n",
    "                         predictions: torch.Tensor = None):\n",
    "    \"\"\"Display Master Ao-Tougrad's assessment of your pattern recognition.\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Create scatter plot using time vs complexity (most informative features)\n",
    "    genuine_mask = (target.squeeze() == 0)\n",
    "    suspicious_mask = (target.squeeze() == 1)\n",
    "    \n",
    "    plt.scatter(features[genuine_mask, 0], features[genuine_mask, 3], \n",
    "               c='lightblue', alpha=0.6, s=50, label='Genuine Accidents')\n",
    "    plt.scatter(features[suspicious_mask, 0], features[suspicious_mask, 3], \n",
    "               c='red', alpha=0.6, s=50, label='Suspicious Timing')\n",
    "    \n",
    "    if predictions is not None:\n",
    "        # Show prediction confidence as background colors\n",
    "        pred_probs = torch.sigmoid(predictions).detach().numpy().squeeze()\n",
    "        \n",
    "        # Create prediction boundary visualization\n",
    "        xx = np.linspace(0, 23, 50)\n",
    "        yy = np.linspace(0, 1, 50)\n",
    "        XX, YY = np.meshgrid(xx, yy)\n",
    "        \n",
    "        plt.contour(XX, YY, np.ones_like(XX) * 0.5, levels=[0.5], \n",
    "                   colors='gold', linewidths=3, linestyles='--',\n",
    "                   label='Decision Boundary (50%)')\n",
    "    \n",
    "    plt.xlabel('Time of Day (Hour)')\n",
    "    plt.ylabel('Task Complexity')\n",
    "    plt.title('The Hidden Patterns in He-Ao-World\\'s Accidents\\n(Time vs Complexity View)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim(0, 23)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Add annotations for key insights\n",
    "    plt.axvspan(6, 9, alpha=0.1, color='orange', label='Morning Hours (High Suspicion)')\n",
    "    plt.axhspan(0.7, 1.0, alpha=0.1, color='orange', label='Complex Tasks (High Suspicion)')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starter-template",
   "metadata": {},
   "source": [
    "## 🧠 THE SACRED NEURAL NETWORK TEMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🕵️ FIRST MOVEMENTS - THE PATTERN DETECTION ARTIFACT\n",
    "\n",
    "class AccidentPatternDetector(nn.Module):\n",
    "    \"\"\"A mystical artifact for detecting hidden patterns in apparent clumsiness.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_features: int = 4):\n",
    "        super(AccidentPatternDetector, self).__init__()\n",
    "        # TODO: Create the Linear layer for multi-variable classification\n",
    "        # Hint: input_features=4 (time, location, witnesses, complexity) -> output=1 (suspicious probability)\n",
    "        self.linear = None\n",
    "        \n",
    "        # TODO: Add sigmoid activation for probability output\n",
    "        # Hint: nn.Sigmoid() converts raw outputs to probabilities (0-1)\n",
    "        self.sigmoid = None\n",
    "    \n",
    "    def forward(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Channel multiple clues through the mystical detection network.\"\"\"\n",
    "        # TODO: Pass features through linear layer\n",
    "        raw_output = None\n",
    "        \n",
    "        # TODO: Apply sigmoid to get probability of suspicious timing\n",
    "        # Remember: probabilities should be between 0 and 1\n",
    "        probability = None\n",
    "        \n",
    "        return probability\n",
    "\n",
    "def train_detector(model: nn.Module, features: torch.Tensor, target: torch.Tensor,\n",
    "                  epochs: int = 1500, learning_rate: float = 0.01) -> list:\n",
    "    \"\"\"\n",
    "    Train the accident pattern detection model.\n",
    "    \n",
    "    Returns:\n",
    "        List of loss values during training\n",
    "    \"\"\"\n",
    "    # TODO: Choose your loss calculation method for binary classification\n",
    "    # Hint: Binary Cross Entropy Loss is the sacred choice for yes/no predictions\n",
    "    criterion = None\n",
    "    \n",
    "    # TODO: Choose your parameter updating method  \n",
    "    # Hint: SGD remains the traditional path for classification\n",
    "    optimizer = None\n",
    "    \n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # TODO: CRITICAL - Clear the gradient spirits from previous cycle\n",
    "        # Hint: The spirits accumulate if not banished properly\n",
    "        \n",
    "        # TODO: Forward pass - get probability predictions\n",
    "        predictions = None\n",
    "        \n",
    "        # TODO: Compute the binary classification loss\n",
    "        loss = None\n",
    "        \n",
    "        # TODO: Backward pass - compute gradients\n",
    "        \n",
    "        # TODO: Update parameters\n",
    "        \n",
    "        # Calculate accuracy for monitoring\n",
    "        with torch.no_grad():\n",
    "            predicted_labels = (predictions > 0.5).float()\n",
    "            accuracy = (predicted_labels == target).float().mean().item()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        # Report progress to Master Ao-Tougrad\n",
    "        if (epoch + 1) % 200 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy:.3f}')\n",
    "            if accuracy > 0.75:\n",
    "                print(\"🔮 Master Ao-Tougrad whispers: 'The patterns become visible to you...'\")\n",
    "    \n",
    "    return losses, accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "success-criteria",
   "metadata": {},
   "source": [
    "## ⚡ THE TRIALS OF MASTERY\n",
    "\n",
    "### Trial 1: Basic Pattern Recognition\n",
    "- [ ] Loss decreases consistently (the hidden patterns reveal themselves)\n",
    "- [ ] Final loss below 0.6 (Master Ao-Tougrad approves your perception)\n",
    "- [ ] Training accuracy above 75% (you detect most suspicious incidents)\n",
    "- [ ] Model learns meaningful weights for each feature (morning hours, sacred hall, etc.)\n",
    "\n",
    "### Trial 2: Understanding Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wisdom-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_your_wisdom(model, features, target):\n",
    "    \"\"\"Master Ao-Tougrad's evaluation of your pattern detection mastery.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Test 1: Model produces correct output shapes\n",
    "    test_features = torch.tensor([[8.0, 1.0, 0.0, 0.9],   # Morning, sacred hall, no witnesses, complex task\n",
    "                                 [14.0, 0.0, 3.0, 0.1],   # Afternoon, courtyard, witnesses, simple task\n",
    "                                 [22.0, 2.0, 1.0, 0.5]])  # Evening, kitchen, one witness, medium task\n",
    "    \n",
    "    predictions = model(test_features)\n",
    "    assert predictions.shape == (3, 1), f\"Shape mismatch: expected (3, 1), got {predictions.shape}\"\n",
    "    \n",
    "    # Test 2: Probabilities are in valid range\n",
    "    assert torch.all(predictions >= 0) and torch.all(predictions <= 1), \"Predictions must be probabilities (0-1)!\"\n",
    "    \n",
    "    # Test 3: Logical pattern recognition\n",
    "    morning_sacred_complex = predictions[0].item()  # Should be highly suspicious\n",
    "    afternoon_courtyard_simple = predictions[1].item()  # Should be less suspicious\n",
    "    \n",
    "    print(f\"Morning sacred hall incident: {morning_sacred_complex:.3f} suspicion\")\n",
    "    print(f\"Afternoon courtyard incident: {afternoon_courtyard_simple:.3f} suspicion\")\n",
    "    \n",
    "    # Test 4: Model performance on training data\n",
    "    with torch.no_grad():\n",
    "        all_predictions = model(features)\n",
    "        predicted_labels = (all_predictions > 0.5).float()\n",
    "        accuracy = (predicted_labels == target).float().mean().item()\n",
    "        \n",
    "        print(f\"Overall detection accuracy: {accuracy:.3f}\")\n",
    "        \n",
    "        # Calculate precision and recall for suspicious incidents\n",
    "        true_positives = ((predicted_labels == 1) & (target == 1)).sum().item()\n",
    "        false_positives = ((predicted_labels == 1) & (target == 0)).sum().item()\n",
    "        false_negatives = ((predicted_labels == 0) & (target == 1)).sum().item()\n",
    "        \n",
    "        precision = true_positives / (true_positives + false_positives + 1e-8)\n",
    "        recall = true_positives / (true_positives + false_negatives + 1e-8)\n",
    "        \n",
    "        print(f\"Precision (suspicious predictions that are correct): {precision:.3f}\")\n",
    "        print(f\"Recall (suspicious incidents detected): {recall:.3f}\")\n",
    "    \n",
    "    if accuracy > 0.75 and morning_sacred_complex > afternoon_courtyard_simple:\n",
    "        print(\"\\n🎉 Master Ao-Tougrad emerges from the shadows with approval!\")\n",
    "        print(\"   'Your eyes now perceive the subtle patterns that others miss.'\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"\\n🤔 Master Ao-Tougrad remains silent. The patterns elude you still...\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-extensions",
   "metadata": {},
   "source": [
    "## 🌸 THE FOUR PATHS OF MASTERY: PROGRESSIVE EXTENSIONS\n",
    "\n",
    "### Extension 1: Cook Oh-Pai-Timizer's Kitchen Timing Analysis\n",
    "*\"The best meals require precise timing - just like the best 'accidents'!\"*\n",
    "\n",
    "*Cook Oh-Pai-Timizer wipes flour-covered hands on an apron while stirring a bubbling pot*\n",
    "\n",
    "\"Grasshopper! I notice you've learned to spot He-Ao-World's suspicious timing. But have you considered the deeper patterns? In my kitchen, I know that some combinations of ingredients create stronger flavors than others. Perhaps certain combinations of features create stronger suspicion signals too?\"\n",
    "\n",
    "**NEW CONCEPTS**: Feature interactions, feature engineering, polynomial features  \n",
    "**DIFFICULTY**: +15% (still Dan 1, but with feature combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extension-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_interactions(features: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create interaction features like a master chef combines ingredients.\n",
    "    \n",
    "    Original features: [time, location, witnesses, complexity]\n",
    "    Add interactions: time*complexity, location*witnesses, time*location, etc.\n",
    "    \n",
    "    Returns:\n",
    "        Enhanced feature tensor with original + interaction features\n",
    "    \"\"\"\n",
    "    # TODO: Create meaningful feature interactions\n",
    "    # Hint: torch.cat() can combine tensors along a dimension\n",
    "    # Hint: Element-wise multiplication (*) creates interaction terms\n",
    "    \n",
    "    # Example interactions to implement:\n",
    "    # - time * complexity (complex tasks at suspicious times)\n",
    "    # - location * (1 / (witnesses + 1)) (location effect stronger with fewer witnesses)\n",
    "    # - time * location (location preferences vary by time)\n",
    "    \n",
    "    pass\n",
    "\n",
    "class EnhancedAccidentDetector(nn.Module):\n",
    "    \"\"\"A more sophisticated detector that understands feature interactions.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_features: int = 7):  # 4 original + 3 interactions\n",
    "        super(EnhancedAccidentDetector, self).__init__()\n",
    "        # TODO: Create linear layer for enhanced features\n",
    "        # TODO: Add sigmoid activation\n",
    "        pass\n",
    "    \n",
    "    def forward(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO: Implement forward pass with feature interactions\n",
    "        pass\n",
    "\n",
    "# TRIAL: Train with interaction features\n",
    "# SUCCESS: Achieve >80% accuracy with better understanding of feature combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extension-2",
   "metadata": {},
   "source": [
    "### Extension 2: He-Ao-World's Seasonal Accident Patterns\n",
    "*\"Oh dear! I seem to have different accident rates during different temple seasons...\"*\n",
    "\n",
    "*He-Ao-World shuffles over with a mop, looking particularly apologetic*\n",
    "\n",
    "\"I've been thinking about my... clumsiness patterns. Master Pai-Torch mentioned that even accidents follow cycles - spring cleaning season sees more 'mishaps' with dust scrolls, autumn harvest brings more 'spills' near the grain storage, winter meditation requires more 'interruptions' of silent contemplation. Perhaps your detector should understand these seasonal rhythms too?\"\n",
    "\n",
    "**NEW CONCEPTS**: Cyclical features, sine/cosine encoding, temporal patterns  \n",
    "**DIFFICULTY**: +25% (still Dan 1, but with temporal encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extension-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cyclical_time_features(features: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Encode time of day cyclically so that 23:00 and 1:00 are close together.\n",
    "    \n",
    "    Uses sine and cosine encoding to represent circular time.\n",
    "    \n",
    "    Returns:\n",
    "        Features with added cyclical time encoding [sin_time, cos_time]\n",
    "    \"\"\"\n",
    "    # TODO: Convert hour (0-23) to cyclical representation\n",
    "    # Hint: sin(2π * hour / 24) and cos(2π * hour / 24)\n",
    "    # Hint: This makes midnight (0) and midnight (24) the same point\n",
    "    \n",
    "    time_of_day = features[:, 0]  # Extract time feature\n",
    "    \n",
    "    # TODO: Create sine and cosine encodings\n",
    "    sin_time = None  # torch.sin(2 * torch.pi * time_of_day / 24)\n",
    "    cos_time = None  # torch.cos(2 * torch.pi * time_of_day / 24)\n",
    "    \n",
    "    # TODO: Combine with original features\n",
    "    enhanced_features = None\n",
    "    \n",
    "    return enhanced_features\n",
    "\n",
    "def generate_seasonal_accident_data(n_observations: int = 300) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Generate accident data with stronger cyclical time patterns.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (cyclical_features, is_suspicious)\n",
    "    \"\"\"\n",
    "    # TODO: Generate base data and add cyclical time encoding\n",
    "    # TODO: Modify suspicion patterns to be stronger during certain time cycles\n",
    "    pass\n",
    "\n",
    "# TRIAL: Train detector with cyclical time understanding\n",
    "# SUCCESS: Better performance on time-sensitive patterns, understand cyclical encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extension-3",
   "metadata": {},
   "source": [
    "### Extension 3: Master Pai-Torch's Confidence Calibration\n",
    "*\"Young grasshopper, knowing what you don't know is as important as knowing what you do.\"*\n",
    "\n",
    "*Master Pai-Torch sits in quiet contemplation before speaking*\n",
    "\n",
    "\"I observe that your detector speaks with certainty about every incident. But true wisdom lies in understanding confidence. When your model predicts 0.51 probability, is it barely suspicious or clearly suspicious? Learn to calibrate your confidence - predict not just the outcome, but how certain you should be about that prediction.\"\n",
    "\n",
    "**NEW CONCEPTS**: Prediction confidence, probability calibration, uncertainty quantification  \n",
    "**DIFFICULTY**: +35% (still Dan 1, but with confidence analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extension-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_prediction_confidence(model, features, target, confidence_bins=10):\n",
    "    \"\"\"\n",
    "    Analyze how well-calibrated your model's confidence is.\n",
    "    \n",
    "    A well-calibrated model's predictions should match reality:\n",
    "    - When it predicts 0.7 probability, ~70% should actually be suspicious\n",
    "    - When it predicts 0.3 probability, ~30% should actually be suspicious\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with calibration analysis\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(features).squeeze()\n",
    "        \n",
    "    # TODO: Create confidence bins (0-0.1, 0.1-0.2, ..., 0.9-1.0)\n",
    "    # TODO: For each bin, calculate:\n",
    "    #   - Average predicted probability\n",
    "    #   - Actual fraction of suspicious cases\n",
    "    #   - Confidence score (how close predicted vs actual)\n",
    "    \n",
    "    calibration_results = {}\n",
    "    \n",
    "    # TODO: Implement binning and analysis\n",
    "    for i in range(confidence_bins):\n",
    "        bin_start = i / confidence_bins\n",
    "        bin_end = (i + 1) / confidence_bins\n",
    "        \n",
    "        # Find predictions in this bin\n",
    "        in_bin = (predictions >= bin_start) & (predictions < bin_end)\n",
    "        \n",
    "        if in_bin.sum() > 0:\n",
    "            avg_prediction = predictions[in_bin].mean().item()\n",
    "            actual_fraction = target[in_bin].float().mean().item()\n",
    "            calibration_results[f'bin_{i}'] = {\n",
    "                'range': f'{bin_start:.1f}-{bin_end:.1f}',\n",
    "                'avg_prediction': avg_prediction,\n",
    "                'actual_fraction': actual_fraction,\n",
    "                'count': in_bin.sum().item()\n",
    "            }\n",
    "    \n",
    "    return calibration_results\n",
    "\n",
    "def visualize_confidence_calibration(calibration_results):\n",
    "    \"\"\"Plot the calibration curve to see how well-calibrated your model is.\"\"\"\n",
    "    # TODO: Create a plot showing predicted vs actual probabilities\n",
    "    # TODO: Add a diagonal line showing perfect calibration\n",
    "    # TODO: Show how far your model deviates from perfect calibration\n",
    "    pass\n",
    "\n",
    "# TRIAL: Analyze and improve your model's confidence calibration\n",
    "# SUCCESS: Understand the difference between accuracy and calibration\n",
    "# MASTERY: Know when your model is overconfident vs underconfident"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extension-4",
   "metadata": {},
   "source": [
    "### Extension 4: Suki's Behavioral Correlation Analysis\n",
    "*\"Meow meow purr... meow.\" (Translation: \"The temple cat sees patterns within patterns.\")*\n",
    "\n",
    "*Suki sits regally atop a stack of scrolls, tail twitching with mathematical precision*\n",
    "\n",
    "*Master Pai-Torch translates Suki's wisdom: \"The sacred cat has observed that He-Ao-World's accidents often correlate with her own behaviors. When Suki sits in doorways, accidents increase near passages. When she naps in the sacred hall, ceremonies get 'accidentally' interrupted. Your detector should learn these cross-pattern correlations to achieve true mastery.\"*\n",
    "\n",
    "**NEW CONCEPTS**: Cross-feature correlations, feature importance analysis, interpretable ML  \n",
    "**DIFFICULTY**: +45% (still Dan 1, but with advanced feature analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extension-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(model, features, target, feature_names):\n",
    "    \"\"\"\n",
    "    Understand which features your model considers most important.\n",
    "    \n",
    "    Uses weight magnitude and gradient-based importance.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with importance scores for each feature\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Method 1: Weight magnitude importance\n",
    "    weights = model.linear.weight.data.abs().squeeze()\n",
    "    weight_importance = weights / weights.sum()\n",
    "    \n",
    "    # Method 2: Gradient-based importance\n",
    "    features.requires_grad_(True)\n",
    "    predictions = model(features)\n",
    "    loss = nn.BCELoss()(predictions, target)\n",
    "    \n",
    "    # TODO: Calculate gradients with respect to input features\n",
    "    gradients = torch.autograd.grad(loss, features, create_graph=True)[0]\n",
    "    gradient_importance = gradients.abs().mean(0)\n",
    "    gradient_importance = gradient_importance / gradient_importance.sum()\n",
    "    \n",
    "    # Combine importance measures\n",
    "    importance_results = {}\n",
    "    for i, name in enumerate(feature_names):\n",
    "        importance_results[name] = {\n",
    "            'weight_importance': weight_importance[i].item(),\n",
    "            'gradient_importance': gradient_importance[i].item(),\n",
    "            'combined_importance': (weight_importance[i] + gradient_importance[i]).item() / 2\n",
    "        }\n",
    "    \n",
    "    return importance_results\n",
    "\n",
    "def feature_ablation_study(model, features, target, feature_names):\n",
    "    \"\"\"\n",
    "    Test how much each feature contributes by removing it and measuring performance drop.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary showing performance impact of removing each feature\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Baseline performance with all features\n",
    "    with torch.no_grad():\n",
    "        baseline_pred = model(features)\n",
    "        baseline_acc = ((baseline_pred > 0.5).float() == target).float().mean().item()\n",
    "    \n",
    "    ablation_results = {}\n",
    "    \n",
    "    # TODO: For each feature, set it to zero and measure performance drop\n",
    "    for i, feature_name in enumerate(feature_names):\n",
    "        # Create modified features with one feature zeroed out\n",
    "        modified_features = features.clone()\n",
    "        modified_features[:, i] = 0  # Remove this feature\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            modified_pred = model(modified_features)\n",
    "            modified_acc = ((modified_pred > 0.5).float() == target).float().mean().item()\n",
    "        \n",
    "        performance_drop = baseline_acc - modified_acc\n",
    "        ablation_results[feature_name] = {\n",
    "            'performance_drop': performance_drop,\n",
    "            'relative_importance': performance_drop / baseline_acc if baseline_acc > 0 else 0\n",
    "        }\n",
    "    \n",
    "    return ablation_results\n",
    "\n",
    "def visualize_feature_analysis(importance_results, ablation_results, feature_names):\n",
    "    \"\"\"Create comprehensive visualization of feature importance.\"\"\"\n",
    "    # TODO: Create bar plots showing:\n",
    "    # - Weight-based importance\n",
    "    # - Gradient-based importance  \n",
    "    # - Ablation study results\n",
    "    # - Combined importance ranking\n",
    "    pass\n",
    "\n",
    "# TRIAL: Understand which features matter most for detecting suspicious timing\n",
    "# SUCCESS: Identify the most predictive features and explain model behavior\n",
    "# MASTERY: Use feature analysis to improve model interpretability and trust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debugging-challenge",
   "metadata": {},
   "source": [
    "## 🔥 CORRECTING YOUR FORM: A STANCE IMBALANCE\n",
    "\n",
    "*Master Pai-Torch observes your training ritual with careful attention. \"Young grasshopper, I sense disturbance in your gradient flow. Your eager mind has rushed ahead of your disciplined form, creating chaos where there should be harmony.\"*\n",
    "\n",
    "*A previous disciple left this flawed training ritual. The pattern detection wavers and fails - can you restore proper technique?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debugging-challenge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚨 FLAWED TRAINING RITUAL - RESTORE THE BALANCE!\n",
    "\n",
    "def broken_pattern_training(model, features, target, epochs=1000):\n",
    "    \"\"\"This training stance has lost its balance - can you spot the errors? 🥋\"\"\"\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        predictions = model(features)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions, target)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if epoch % 200 == 0:\n",
    "            print(f'Epoch {epoch}: Loss = {loss.item():.4f}')\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# DEBUGGING CHALLENGE: Spot the critical errors in this training ritual!\n",
    "# \n",
    "# HINTS FROM MASTER PAI-TORCH:\n",
    "# 🔮 \"The gradient spirits accumulate their whispers from ages past...\"\n",
    "# 🔮 \"Without proper cleansing, old wisdom corrupts new learning...\"\n",
    "# 🔮 \"Each cycle must begin with a clear mind, free from previous thoughts...\"\n",
    "# \n",
    "# ERROR COUNT: 1 critical error (will prevent proper learning)\n",
    "# \n",
    "# MASTER'S WISDOM: \"The undisciplined mind carries forward its confusion,\n",
    "#                   just as the uncleansed gradient carries forward its misdirection.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-wisdom",
   "metadata": {},
   "source": [
    "## 🎭 THE MASTER'S FINAL WISDOM\n",
    "\n",
    "*As you complete your training, Master Ao-Tougrad emerges fully from the shadows for the first time*\n",
    "\n",
    "\"Grasshopper, you have learned to see beyond the obvious. He-Ao-World's 'accidents' are indeed a masterful dance of timing, location, and opportunity. But remember this deeper truth: in neural networks, as in life, multiple signals combine to reveal hidden patterns. No single feature tells the complete story.\"\n",
    "\n",
    "*He-Ao-World shuffles by and winks conspiratorially*\n",
    "\n",
    "\"Your multi-variable classifier has learned what the ancient masters knew: truth emerges from the convergence of many observations, not from any single clue. This sacred principle will serve you well as you advance through the temple's deeper mysteries.\"\n",
    "\n",
    "**Sacred Principles Mastered:**\n",
    "- Multi-dimensional feature processing with PyTorch tensors\n",
    "- Binary classification with multiple input variables\n",
    "- Proper gradient management in classification training\n",
    "- Feature interaction and importance analysis\n",
    "- Model interpretability and confidence calibration\n",
    "\n",
    "**Next Temple Lesson**: Continue your journey with more advanced classification techniques, or explore the mysteries of regularization in Dan 2 training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
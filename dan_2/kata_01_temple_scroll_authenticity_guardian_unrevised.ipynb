{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ruliana/pytorch-katas/blob/main/dan_2/kata_01_temple_scroll_authenticity_guardian_unrevised.ipynb)\n",
    "\n",
    "## üèÆ The Ancient Scroll Unfurls üèÆ\n",
    "\n",
    "# THE FORBIDDEN SCROLL GUARDIAN: MASTERING THE DEEPER MYSTERIES\n",
    "\n",
    "**Dan Level: 2 (Temple Guardian) | Time: 45 minutes | Sacred Arts: Multi-layer Networks, Regularization, Validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìú THE CHALLENGE\n",
    "\n",
    "*The temple library's most ancient chamber stands before you, its heavy doors carved with warnings in languages long forgotten. Master Pai-Torch approaches with uncharacteristic gravity.*\n",
    "\n",
    "**Master Pai-Torch**: \"Grasshopper, you have mastered the simple linear arts, but now you seek the deeper mysteries. The temple archives contain scrolls of immense power - some authentic treasures from the founding masters, others clever forgeries created by those who would steal our sacred knowledge.\"\n",
    "\n",
    "*The ancient master's eyes glow with inner wisdom as mysterious symbols appear floating in the air around them.*\n",
    "\n",
    "**Master Pai-Torch**: \"The Art of Deep Authentication requires multiple layers of understanding - surface patterns, hidden meanings, and the subtle energies that flow between them. But beware! This knowledge is forbidden to those who lack proper discipline. Without the Sacred Safeguards, even the wisest student falls to the Curse of Overfitting.\"\n",
    "\n",
    "*From the shadows, Master Ao-Tougrad materializes with an ethereal whisper.*\n",
    "\n",
    "**Master Ao-Tougrad**: \"I have walked the gradient paths of the deep networks for centuries. The untrained mind seeks complexity without wisdom, creating models that memorize rather than understand. Learn well the arts of Dropout and Validation, young one, for they are your only protection against the madness that comes with forbidden power.\"\n",
    "\n",
    "### üéØ THE SACRED OBJECTIVES\n",
    "\n",
    "Your quest requires mastering these deeper mysteries:\n",
    "\n",
    "- [ ] **Deep Network Architecture**: Create a multi-layer neural network with hidden layers\n",
    "- [ ] **Regularization Mastery**: Implement dropout to prevent overfitting\n",
    "- [ ] **Validation Wisdom**: Split data properly and monitor validation loss\n",
    "- [ ] **Optimization Arts**: Use advanced optimizers beyond simple SGD\n",
    "- [ ] **Guardian's Vigilance**: Detect and prevent overfitting through early stopping\n",
    "- [ ] **Threshold Mastery**: Optimize decision boundaries for classification\n",
    "\n",
    "**Master Pai-Torch**: \"Remember, young guardian - a model that achieves perfect accuracy on training data but fails on new scrolls is no guardian at all. True wisdom lies in generalization, not memorization.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÆ THE SACRED IMPORTS\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set the sacred seed for reproducible mystical results\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"üèÆ The sacred libraries have been summoned...\")\n",
    "print(f\"üßô PyTorch version: {torch.__version__}\")\n",
    "print(\"‚ö° The gradient spirits await your command!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö THE SACRED DATA GENERATION SCROLL\n",
    "\n",
    "*Master Pai-Torch produces an ancient scroll covered in complex diagrams and mathematical symbols.*\n",
    "\n",
    "**Master Pai-Torch**: \"The authentication of sacred scrolls requires understanding multiple layers of complexity. Surface features like ink density and paper texture are but the beginning. True authentication depends on hidden relationships - the flow of meaning, the rhythm of brush strokes, the subtle harmonies that only genuine masters can create.\"\n",
    "\n",
    "**Master Ao-Tougrad**: \"These patterns exist in higher dimensions than the simple linear world you have known. Observe how each scroll contains multiple measurements - some obvious, others hidden, all interconnected through the Deep Mystery.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìú THE SACRED SCROLL AUTHENTICATION DATA\n",
    "\n",
    "def generate_scroll_authentication_data(n_scrolls: int = 1000, \n",
    "                                       complexity_level: float = 0.3,\n",
    "                                       sacred_seed: int = 42) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Generate data for authenticating ancient temple scrolls.\n",
    "    \n",
    "    Ancient wisdom suggests that scroll authenticity depends on multiple factors:\n",
    "    - Surface features: ink_density, paper_texture, age_marks\n",
    "    - Hidden features: brush_flow, spiritual_resonance, master_signature_energy\n",
    "    - Deep relationships: Non-linear interactions between these features\n",
    "    \n",
    "    Authentic scrolls follow complex patterns that cannot be captured by simple linear models.\n",
    "    \n",
    "    Args:\n",
    "        n_scrolls: Number of scrolls to analyze\n",
    "        complexity_level: How complex the hidden patterns are (0.0 = simple, 1.0 = very complex)\n",
    "        sacred_seed: Ensures consistent mystical randomness\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (scroll_features, authenticity_labels)\n",
    "        scroll_features: shape (n_scrolls, 6) - six measurements per scroll\n",
    "        authenticity_labels: shape (n_scrolls, 1) - 1 for authentic, 0 for forgery\n",
    "    \"\"\"\n",
    "    torch.manual_seed(sacred_seed)\n",
    "    \n",
    "    # Generate the six sacred measurements for each scroll\n",
    "    # Surface features (these are somewhat predictable)\n",
    "    ink_density = torch.rand(n_scrolls, 1) * 100  # 0-100 density units\n",
    "    paper_texture = torch.rand(n_scrolls, 1) * 50  # 0-50 roughness units\n",
    "    age_marks = torch.rand(n_scrolls, 1) * 20  # 0-20 age indicators\n",
    "    \n",
    "    # Hidden features (these contain the deeper mysteries)\n",
    "    brush_flow = torch.rand(n_scrolls, 1) * 30  # 0-30 flow harmony units\n",
    "    spiritual_resonance = torch.rand(n_scrolls, 1) * 40  # 0-40 spiritual energy\n",
    "    master_signature = torch.rand(n_scrolls, 1) * 60  # 0-60 master's unique energy\n",
    "    \n",
    "    # Combine all features\n",
    "    scroll_features = torch.cat([\n",
    "        ink_density, paper_texture, age_marks,\n",
    "        brush_flow, spiritual_resonance, master_signature\n",
    "    ], dim=1)\n",
    "    \n",
    "    # The Sacred Formula for Authenticity (complex non-linear relationships)\n",
    "    # This requires deep networks to learn properly!\n",
    "    \n",
    "    # First layer of hidden patterns\n",
    "    surface_harmony = (ink_density.squeeze() * 0.3 + \n",
    "                      paper_texture.squeeze() * 0.4 + \n",
    "                      age_marks.squeeze() * 0.2)\n",
    "    \n",
    "    hidden_wisdom = (brush_flow.squeeze() * 0.5 + \n",
    "                    spiritual_resonance.squeeze() * 0.3 +\n",
    "                    master_signature.squeeze() * 0.4)\n",
    "    \n",
    "    # Second layer: Non-linear interactions (this is why we need deep networks!)\n",
    "    deep_pattern = torch.tanh(surface_harmony / 20) * torch.sigmoid(hidden_wisdom / 15)\n",
    "    \n",
    "    # Final authentication score with complexity\n",
    "    base_score = (surface_harmony * 0.3 + hidden_wisdom * 0.4 + deep_pattern * 50)\n",
    "    \n",
    "    # Add complexity-based noise and non-linearity\n",
    "    complexity_noise = torch.randn(n_scrolls) * complexity_level * base_score.std()\n",
    "    final_score = base_score + complexity_noise\n",
    "    \n",
    "    # Add some additional non-linear terms to make it truly challenging\n",
    "    interaction_term = (ink_density.squeeze() * brush_flow.squeeze() / 1000 + \n",
    "                       paper_texture.squeeze() * spiritual_resonance.squeeze() / 800)\n",
    "    \n",
    "    final_score = final_score + interaction_term * complexity_level * 10\n",
    "    \n",
    "    # Convert to binary authentication labels\n",
    "    # Authentic scrolls have scores above the 60th percentile\n",
    "    threshold = torch.quantile(final_score, 0.6)\n",
    "    authenticity_labels = (final_score > threshold).float().unsqueeze(1)\n",
    "    \n",
    "    return scroll_features, authenticity_labels\n",
    "\n",
    "def visualize_scroll_mysteries(features: torch.Tensor, labels: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Reveal the hidden patterns in the scroll authentication data.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    feature_names = ['Ink Density', 'Paper Texture', 'Age Marks', \n",
    "                    'Brush Flow', 'Spiritual Resonance', 'Master Signature']\n",
    "    \n",
    "    authentic_mask = labels.squeeze() == 1\n",
    "    forgery_mask = labels.squeeze() == 0\n",
    "    \n",
    "    for i, (ax, feature_name) in enumerate(zip(axes.flat, feature_names)):\n",
    "        # Plot authentic scrolls\n",
    "        ax.hist(features[authentic_mask, i].numpy(), bins=20, alpha=0.7, \n",
    "                color='gold', label='Authentic Scrolls', density=True)\n",
    "        \n",
    "        # Plot forgeries\n",
    "        ax.hist(features[forgery_mask, i].numpy(), bins=20, alpha=0.7, \n",
    "                color='red', label='Forgeries', density=True)\n",
    "        \n",
    "        ax.set_xlabel(feature_name)\n",
    "        ax.set_ylabel('Density')\n",
    "        ax.set_title(f'üìú {feature_name} Distribution')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('üîÆ The Six Sacred Measurements of Scroll Authentication', \n",
    "                 fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    # Show correlation mysteries\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    correlation_matrix = torch.corrcoef(features.T)\n",
    "    im = plt.imshow(correlation_matrix.numpy(), cmap='RdBu', vmin=-1, vmax=1)\n",
    "    \n",
    "    plt.colorbar(im, label='Correlation Strength')\n",
    "    plt.xticks(range(6), feature_names, rotation=45)\n",
    "    plt.yticks(range(6), feature_names)\n",
    "    plt.title('üßô The Hidden Connections Between Sacred Measurements')\n",
    "    \n",
    "    # Add correlation values as text\n",
    "    for i in range(6):\n",
    "        for j in range(6):\n",
    "            plt.text(j, i, f'{correlation_matrix[i, j]:.2f}', \n",
    "                    ha='center', va='center', \n",
    "                    color='white' if abs(correlation_matrix[i, j]) > 0.5 else 'black')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üìä Total scrolls analyzed: {len(features)}\")\n",
    "    print(f\"‚ú® Authentic scrolls: {authentic_mask.sum().item()} ({authentic_mask.float().mean()*100:.1f}%)\")\n",
    "    print(f\"üî¥ Forgeries detected: {forgery_mask.sum().item()} ({forgery_mask.float().mean()*100:.1f}%)\")\n",
    "    print(\"\\nüßô Master Pai-Torch whispers: 'Notice how the patterns interweave... linear models cannot capture such complexity.'\")\n",
    "\n",
    "# Test the sacred data generation\n",
    "print(\"üîÆ Generating sacred scroll authentication data...\")\n",
    "scroll_features, authenticity_labels = generate_scroll_authentication_data(n_scrolls=800, complexity_level=0.3)\n",
    "\n",
    "print(f\"üìú Generated {len(scroll_features)} scroll measurements\")\n",
    "print(f\"üéØ Feature shape: {scroll_features.shape}\")\n",
    "print(f\"üéØ Label shape: {authenticity_labels.shape}\")\n",
    "\n",
    "visualize_scroll_mysteries(scroll_features, authenticity_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ°Ô∏è THE TEMPLE GUARDIAN'S NEURAL ARCHITECTURE\n",
    "\n",
    "*Master Pai-Torch's expression grows serious as the forbidden knowledge chamber opens before you.*\n",
    "\n",
    "**Master Pai-Torch**: \"The linear arts you have mastered are but the foundation, young guardian. To authenticate these ancient scrolls, you must learn the Deep Architecture - networks with hidden layers that can perceive patterns invisible to simpler models.\"\n",
    "\n",
    "**Master Ao-Tougrad**: \"But beware the Curse of Overfitting! Without proper discipline, your deep network will memorize every scroll in the training chamber but fail completely when faced with new ones. The Sacred Dropout technique must be your constant companion.\"\n",
    "\n",
    "*The masters gesture toward a complex training apparatus with multiple levels and protective barriers.*\n",
    "\n",
    "**Master Pai-Torch**: \"This is the Guardian's Trial. You must create a network with multiple hidden layers, each protected by the Dropout Shields. Only through this disciplined approach can you achieve true authentication wisdom.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ°Ô∏è THE GUARDIAN'S DEEP NETWORK ARCHITECTURE\n",
    "\n",
    "class ScrollAuthenticationGuardian(nn.Module):\n",
    "    \"\"\"\n",
    "    A deep neural network guardian for authenticating ancient temple scrolls.\n",
    "    \n",
    "    This network uses multiple hidden layers to detect complex patterns,\n",
    "    with dropout regularization to prevent overfitting.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_features: int = 6, dropout_rate: float = 0.3):\n",
    "        super(ScrollAuthenticationGuardian, self).__init__()\n",
    "        \n",
    "        # TODO: Create the first hidden layer\n",
    "        # Hint: Transform 6 input features to 128 hidden neurons\n",
    "        # This layer learns to detect basic patterns in the six measurements\n",
    "        self.hidden1 = None\n",
    "        \n",
    "        # TODO: Create the first dropout layer\n",
    "        # Hint: Use nn.Dropout with the specified dropout_rate\n",
    "        # This prevents overfitting by randomly \"forgetting\" some neurons during training\n",
    "        self.dropout1 = None\n",
    "        \n",
    "        # TODO: Create the second hidden layer\n",
    "        # Hint: Transform 128 features to 64 hidden neurons\n",
    "        # This layer learns to combine basic patterns into more complex ones\n",
    "        self.hidden2 = None\n",
    "        \n",
    "        # TODO: Create the second dropout layer\n",
    "        # Hint: Use the same dropout_rate as the first layer\n",
    "        self.dropout2 = None\n",
    "        \n",
    "        # TODO: Create the third hidden layer\n",
    "        # Hint: Transform 64 features to 32 hidden neurons\n",
    "        # This layer learns the most refined patterns\n",
    "        self.hidden3 = None\n",
    "        \n",
    "        # TODO: Create the third dropout layer\n",
    "        self.dropout3 = None\n",
    "        \n",
    "        # TODO: Create the output layer\n",
    "        # Hint: Transform 32 features to 1 output (authenticity score)\n",
    "        # This layer produces the final authentication decision\n",
    "        self.output = None\n",
    "        \n",
    "    def forward(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Channel the scroll features through the deep authentication network.\n",
    "        \n",
    "        The network follows this pattern:\n",
    "        Input ‚Üí Hidden Layer 1 ‚Üí ReLU ‚Üí Dropout ‚Üí Hidden Layer 2 ‚Üí ReLU ‚Üí Dropout ‚Üí \n",
    "        Hidden Layer 3 ‚Üí ReLU ‚Üí Dropout ‚Üí Output Layer ‚Üí Sigmoid\n",
    "        \"\"\"\n",
    "        # TODO: Pass input through first hidden layer and apply ReLU activation\n",
    "        # Hint: Use F.relu() activation function\n",
    "        x = None\n",
    "        \n",
    "        # TODO: Apply first dropout layer\n",
    "        # Hint: Only apply dropout during training (self.training)\n",
    "        x = None\n",
    "        \n",
    "        # TODO: Pass through second hidden layer with ReLU activation\n",
    "        x = None\n",
    "        \n",
    "        # TODO: Apply second dropout layer\n",
    "        x = None\n",
    "        \n",
    "        # TODO: Pass through third hidden layer with ReLU activation\n",
    "        x = None\n",
    "        \n",
    "        # TODO: Apply third dropout layer\n",
    "        x = None\n",
    "        \n",
    "        # TODO: Pass through output layer and apply sigmoid activation\n",
    "        # Hint: Use torch.sigmoid() to get values between 0 and 1\n",
    "        # This gives us the probability that the scroll is authentic\n",
    "        output = None\n",
    "        \n",
    "        return output\n",
    "\n",
    "def split_sacred_data(features: torch.Tensor, labels: torch.Tensor, \n",
    "                     train_ratio: float = 0.7, val_ratio: float = 0.2) -> Tuple:\n",
    "    \"\"\"\n",
    "    Split the sacred scroll data into training, validation, and test sets.\n",
    "    \n",
    "    This is crucial for the Guardian's training - we must test our model\n",
    "    on scrolls it has never seen before!\n",
    "    \"\"\"\n",
    "    n_samples = len(features)\n",
    "    \n",
    "    # TODO: Calculate the split indices\n",
    "    # Hint: train_end = int(n_samples * train_ratio)\n",
    "    # Hint: val_end = int(n_samples * (train_ratio + val_ratio))\n",
    "    train_end = None\n",
    "    val_end = None\n",
    "    \n",
    "    # TODO: Split the features and labels\n",
    "    # Hint: Use tensor slicing like features[:train_end]\n",
    "    train_features = None\n",
    "    train_labels = None\n",
    "    \n",
    "    val_features = None\n",
    "    val_labels = None\n",
    "    \n",
    "    test_features = None\n",
    "    test_labels = None\n",
    "    \n",
    "    return (train_features, train_labels, val_features, val_labels, test_features, test_labels)\n",
    "\n",
    "def train_guardian(model: nn.Module, train_features: torch.Tensor, train_labels: torch.Tensor,\n",
    "                  val_features: torch.Tensor, val_labels: torch.Tensor,\n",
    "                  epochs: int = 1000, learning_rate: float = 0.001, \n",
    "                  patience: int = 50) -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Train the Guardian network with proper validation and early stopping.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing training history\n",
    "    \"\"\"\n",
    "    # TODO: Choose the loss function\n",
    "    # Hint: For binary classification, use nn.BCELoss() (Binary Cross Entropy)\n",
    "    criterion = None\n",
    "    \n",
    "    # TODO: Choose the optimizer\n",
    "    # Hint: Try optim.Adam() with the specified learning_rate\n",
    "    # Adam is more sophisticated than SGD and works well with deep networks\n",
    "    optimizer = None\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()  # Enable dropout\n",
    "        \n",
    "        # TODO: Clear gradients\n",
    "        # Hint: The gradient spirits must be banished before each training cycle\n",
    "        \n",
    "        # TODO: Forward pass\n",
    "        train_predictions = None\n",
    "        \n",
    "        # TODO: Compute training loss\n",
    "        train_loss = None\n",
    "        \n",
    "        # TODO: Backward pass\n",
    "        \n",
    "        # TODO: Update parameters\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()  # Disable dropout\n",
    "        with torch.no_grad():\n",
    "            val_predictions = model(val_features)\n",
    "            val_loss = criterion(val_predictions, val_labels)\n",
    "        \n",
    "        # Calculate accuracies\n",
    "        train_acc = ((train_predictions > 0.5) == train_labels).float().mean()\n",
    "        val_acc = ((val_predictions > 0.5) == val_labels).float().mean()\n",
    "        \n",
    "        # Store metrics\n",
    "        train_losses.append(train_loss.item())\n",
    "        val_losses.append(val_loss.item())\n",
    "        train_accuracies.append(train_acc.item())\n",
    "        val_accuracies.append(val_acc.item())\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"üõ°Ô∏è Early stopping at epoch {epoch+1} - validation loss stopped improving\")\n",
    "            break\n",
    "        \n",
    "        # Report progress\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}]')\n",
    "            print(f'  Train Loss: {train_loss.item():.4f}, Train Acc: {train_acc:.4f}')\n",
    "            print(f'  Val Loss: {val_loss.item():.4f}, Val Acc: {val_acc:.4f}')\n",
    "            \n",
    "            if val_acc > 0.85:\n",
    "                print(\"  ‚ú® The Guardian grows stronger!\")\n",
    "            elif val_loss < train_loss * 1.1:\n",
    "                print(\"  üõ°Ô∏è Training remains disciplined!\")\n",
    "            else:\n",
    "                print(\"  ‚ö†Ô∏è Beware the whispers of overfitting...\")\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'val_accuracies': val_accuracies\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° THE TRIALS OF MASTERY\n",
    "\n",
    "*Master Pai-Torch watches as you prepare for the Guardian's Trial, while Master Ao-Tougrad observes from the shadows.*\n",
    "\n",
    "**Master Pai-Torch**: \"The true test of a Guardian is not perfect performance on known scrolls, but wise discernment of unknown ones. Your network must learn the essence of authenticity, not mere memorization.\"\n",
    "\n",
    "**Master Ao-Tougrad**: \"The validation loss is your guide through the darkness. When it begins to rise while training loss falls, you approach the dangerous territory of overfitting. The wise Guardian knows when to stop.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö° THE TRIALS OF MASTERY\n",
    "\n",
    "## Trial 1: The Guardian's Architecture\n",
    "# - [ ] Network has 3 hidden layers with proper dimensions (128, 64, 32)\n",
    "# - [ ] Dropout layers are properly implemented with 0.3 dropout rate\n",
    "# - [ ] Forward pass follows the correct architecture pattern\n",
    "# - [ ] Output uses sigmoid activation for binary classification\n",
    "\n",
    "## Trial 2: The Sacred Data Split\n",
    "# - [ ] Data is properly split into train/validation/test sets (70%/20%/10%)\n",
    "# - [ ] Validation set is used for early stopping\n",
    "# - [ ] Test set remains untouched until final evaluation\n",
    "\n",
    "## Trial 3: Training Discipline\n",
    "# - [ ] Uses Adam optimizer with learning rate 0.001\n",
    "# - [ ] Implements early stopping with patience=50\n",
    "# - [ ] Validation accuracy reaches at least 80%\n",
    "# - [ ] Gap between training and validation loss remains reasonable (<0.1)\n",
    "\n",
    "def test_guardian_wisdom(model: nn.Module, test_features: torch.Tensor, test_labels: torch.Tensor):\n",
    "    \"\"\"\n",
    "    The ultimate test of the Guardian's wisdom on completely unseen scrolls.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_predictions = model(test_features)\n",
    "        test_binary_predictions = (test_predictions > 0.5).float()\n",
    "        \n",
    "        # Calculate comprehensive metrics\n",
    "        accuracy = (test_binary_predictions == test_labels).float().mean()\n",
    "        \n",
    "        # True/False Positives and Negatives\n",
    "        true_positives = ((test_binary_predictions == 1) & (test_labels == 1)).sum().item()\n",
    "        false_positives = ((test_binary_predictions == 1) & (test_labels == 0)).sum().item()\n",
    "        true_negatives = ((test_binary_predictions == 0) & (test_labels == 0)).sum().item()\n",
    "        false_negatives = ((test_binary_predictions == 0) & (test_labels == 1)).sum().item()\n",
    "        \n",
    "        # Precision and Recall\n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        print(\"üèÆ THE GUARDIAN'S FINAL EVALUATION üèÆ\")\n",
    "        print(f\"üìä Test Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"üéØ Precision: {precision:.4f} (When model says 'authentic', how often is it right?)\")\n",
    "        print(f\"üîç Recall: {recall:.4f} (Of all authentic scrolls, how many did we find?)\")\n",
    "        print(f\"‚öñÔ∏è F1 Score: {f1_score:.4f} (Balanced measure of performance)\")\n",
    "        \n",
    "        print(\"\\nüìà Detailed Results:\")\n",
    "        print(f\"‚úÖ True Positives: {true_positives} (Correctly identified authentic scrolls)\")\n",
    "        print(f\"‚ùå False Positives: {false_positives} (Mistakenly identified forgeries as authentic)\")\n",
    "        print(f\"‚úÖ True Negatives: {true_negatives} (Correctly identified forgeries)\")\n",
    "        print(f\"‚ùå False Negatives: {false_negatives} (Mistakenly identified authentic as forgeries)\")\n",
    "        \n",
    "        # Guardian's Assessment\n",
    "        if accuracy >= 0.85 and precision >= 0.80 and recall >= 0.80:\n",
    "            print(\"\\nüèÜ Master Pai-Torch nods with deep approval:\")\n",
    "            print(\"    'You have achieved the wisdom of a true Guardian. Your network\")\n",
    "            print(\"     discerns authentic scrolls with both precision and recall.'\")\n",
    "        elif accuracy >= 0.80:\n",
    "            print(\"\\nüõ°Ô∏è Master Pai-Torch speaks with measured approval:\")\n",
    "            print(\"    'Your Guardian skills are solid, but strive for greater balance\")\n",
    "            print(\"     between precision and recall in your future training.'\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è Master Pai-Torch's expression grows serious:\")\n",
    "            print(\"    'More training is needed, young Guardian. The sacred scrolls\")\n",
    "            print(\"     require deeper understanding to authenticate properly.'\")\n",
    "        \n",
    "        # Check for overfitting signs\n",
    "        print(\"\\nüîÆ Master Ao-Tougrad's Overfitting Assessment:\")\n",
    "        if hasattr(model, 'training_history'):\n",
    "            final_train_loss = model.training_history['train_losses'][-1]\n",
    "            final_val_loss = model.training_history['val_losses'][-1]\n",
    "            gap = final_val_loss - final_train_loss\n",
    "            \n",
    "            if gap < 0.05:\n",
    "                print(\"    'Your training shows excellent discipline. The validation\")\n",
    "                print(\"     and training losses remain in harmony.'\")\n",
    "            elif gap < 0.15:\n",
    "                print(\"    'Acceptable generalization. Some overfitting is present but\")\n",
    "                print(\"     within reasonable bounds.'\")\n",
    "            else:\n",
    "                print(\"    'Beware! Significant overfitting detected. Your model memorizes\")\n",
    "                print(\"     rather than understands. Consider more dropout or early stopping.'\")\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy.item(),\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1_score,\n",
    "            'confusion_matrix': {\n",
    "                'true_positives': true_positives,\n",
    "                'false_positives': false_positives,\n",
    "                'true_negatives': true_negatives,\n",
    "                'false_negatives': false_negatives\n",
    "            }\n",
    "        }\n",
    "\n",
    "def visualize_guardian_training(history: Dict[str, List[float]]):\n",
    "    \"\"\"\n",
    "    Visualize the Guardian's training progress.\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(history['train_losses']) + 1)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot losses\n",
    "    ax1.plot(epochs, history['train_losses'], 'b-', label='Training Loss', linewidth=2)\n",
    "    ax1.plot(epochs, history['val_losses'], 'r-', label='Validation Loss', linewidth=2)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('üõ°Ô∏è Guardian Training: Loss Curves')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot accuracies\n",
    "    ax2.plot(epochs, history['train_accuracies'], 'b-', label='Training Accuracy', linewidth=2)\n",
    "    ax2.plot(epochs, history['val_accuracies'], 'r-', label='Validation Accuracy', linewidth=2)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('üéØ Guardian Training: Accuracy Curves')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Check for overfitting\n",
    "    final_train_loss = history['train_losses'][-1]\n",
    "    final_val_loss = history['val_losses'][-1]\n",
    "    \n",
    "    if final_val_loss > final_train_loss * 1.2:\n",
    "        print(\"‚ö†Ô∏è Master Ao-Tougrad observes: 'The validation loss diverges from training loss - overfitting may be present.'\")\n",
    "    else:\n",
    "        print(\"‚úÖ Master Ao-Tougrad nods approvingly: 'The training remains disciplined and well-generalized.'\")\n",
    "\n",
    "# Execute the Guardian's Trial\n",
    "print(\"üõ°Ô∏è Beginning the Guardian's Trial...\")\n",
    "print(\"üìú Preparing the sacred scroll data...\")\n",
    "\n",
    "# Split the data\n",
    "train_features, train_labels, val_features, val_labels, test_features, test_labels = split_sacred_data(\n",
    "    scroll_features, authenticity_labels\n",
    ")\n",
    "\n",
    "print(f\"‚öîÔ∏è Training set: {len(train_features)} scrolls\")\n",
    "print(f\"üõ°Ô∏è Validation set: {len(val_features)} scrolls\")\n",
    "print(f\"üîç Test set: {len(test_features)} scrolls\")\n",
    "\n",
    "# Create and train the guardian\n",
    "guardian = ScrollAuthenticationGuardian(input_features=6, dropout_rate=0.3)\n",
    "print(f\"\\nüèóÔ∏è Guardian architecture created with {sum(p.numel() for p in guardian.parameters())} parameters\")\n",
    "\n",
    "# Train the guardian\n",
    "print(\"\\nüèãÔ∏è Beginning Guardian training...\")\n",
    "training_history = train_guardian(guardian, train_features, train_labels, \n",
    "                                val_features, val_labels, \n",
    "                                epochs=1000, learning_rate=0.001, patience=50)\n",
    "\n",
    "# Store training history for later analysis\n",
    "guardian.training_history = training_history\n",
    "\n",
    "# Visualize training progress\n",
    "visualize_guardian_training(training_history)\n",
    "\n",
    "# Final evaluation\n",
    "final_results = test_guardian_wisdom(guardian, test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üå∏ THE FOUR PATHS OF MASTERY: PROGRESSIVE EXTENSIONS\n",
    "\n",
    "*With the basic Guardian skills mastered, the ancient masters reveal deeper mysteries that separate novice guardians from true masters.*\n",
    "\n",
    "**Master Pai-Torch**: \"Your foundation is solid, young Guardian, but true mastery requires exploring the deeper chambers of knowledge. Each path ahead will challenge you in new ways.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension 1: Cook Oh-Pai-Timizer's Batch Recipe Mastery\n",
    "**\"Just as a master chef optimizes cooking for many portions, so must a Guardian optimize training for many scrolls!\"**\n",
    "\n",
    "*Cook Oh-Pai-Timizer appears with a massive cauldron and hundreds of ingredients.*\n",
    "\n",
    "**Cook Oh-Pai-Timizer**: \"Ah, young Guardian! I see you've learned to authenticate scrolls one by one, but what happens when thousands of scrolls arrive at once? In my kitchen, we don't cook each grain of rice individually - we batch them for efficiency!\"\n",
    "\n",
    "**NEW CONCEPTS**: Mini-batch training, batch normalization, training efficiency optimization  \n",
    "**DIFFICULTY**: +15% (still Dan 2, but with advanced training techniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üçú COOK OH-PAI-TIMIZER'S BATCH RECIPE MASTERY\n",
    "\n",
    "def create_data_loader(features: torch.Tensor, labels: torch.Tensor, batch_size: int = 32, shuffle: bool = True):\n",
    "    \"\"\"\n",
    "    Cook Oh-Pai-Timizer's recipe for serving data in perfect batches.\n",
    "    \n",
    "    Args:\n",
    "        features: The scroll features tensor\n",
    "        labels: The authenticity labels tensor\n",
    "        batch_size: How many scrolls to process at once\n",
    "        shuffle: Whether to mix up the order (like tossing a salad!)\n",
    "    \n",
    "    Returns:\n",
    "        DataLoader that serves perfectly sized batches\n",
    "    \"\"\"\n",
    "    # TODO: Create a TensorDataset combining features and labels\n",
    "    # Hint: Use torch.utils.data.TensorDataset\n",
    "    dataset = None\n",
    "    \n",
    "    # TODO: Create a DataLoader with the specified batch_size and shuffle\n",
    "    # Hint: Use torch.utils.data.DataLoader\n",
    "    dataloader = None\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "class BatchNormalizedGuardian(nn.Module):\n",
    "    \"\"\"\n",
    "    An enhanced Guardian that uses batch normalization for more stable training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_features: int = 6, dropout_rate: float = 0.3):\n",
    "        super(BatchNormalizedGuardian, self).__init__()\n",
    "        \n",
    "        self.hidden1 = nn.Linear(input_features, 128)\n",
    "        # TODO: Add batch normalization after the first layer\n",
    "        # Hint: Use nn.BatchNorm1d with 128 features\n",
    "        self.bn1 = None\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.hidden2 = nn.Linear(128, 64)\n",
    "        # TODO: Add batch normalization after the second layer\n",
    "        self.bn2 = None\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.hidden3 = nn.Linear(64, 32)\n",
    "        # TODO: Add batch normalization after the third layer\n",
    "        self.bn3 = None\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.output = nn.Linear(32, 1)\n",
    "        \n",
    "    def forward(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO: Implement forward pass with batch normalization\n",
    "        # Pattern: Linear ‚Üí BatchNorm ‚Üí ReLU ‚Üí Dropout\n",
    "        # Hint: Apply batch normalization before the activation function\n",
    "        \n",
    "        x = self.hidden1(features)\n",
    "        x = self.bn1(x)  # Normalize the batch\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # TODO: Continue the pattern for remaining layers\n",
    "        x = None\n",
    "        \n",
    "        # Final output layer (no batch norm here)\n",
    "        output = torch.sigmoid(self.output(x))\n",
    "        return output\n",
    "\n",
    "def train_with_batches(model: nn.Module, train_loader, val_loader, \n",
    "                      epochs: int = 100, learning_rate: float = 0.001):\n",
    "    \"\"\"\n",
    "    Train the Guardian using Cook Oh-Pai-Timizer's batch cooking method.\n",
    "    \"\"\"\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "        \n",
    "        for batch_features, batch_labels in train_loader:\n",
    "            # TODO: Implement batch training loop\n",
    "            # Hint: Clear gradients, forward pass, backward pass, update parameters\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(batch_features)\n",
    "            loss = criterion(predictions, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_train_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        epoch_val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_features, batch_labels in val_loader:\n",
    "                predictions = model(batch_features)\n",
    "                loss = criterion(predictions, batch_labels)\n",
    "                epoch_val_loss += loss.item()\n",
    "        \n",
    "        # Average losses\n",
    "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "        avg_val_loss = epoch_val_loss / len(val_loader)\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "    \n",
    "    return {'train_losses': train_losses, 'val_losses': val_losses}\n",
    "\n",
    "# TRIAL: Implement batch training with batch normalization\n",
    "print(\"üçú Cook Oh-Pai-Timizer's Batch Training Trial\")\n",
    "print(\"TODO: Implement the batch training system and compare with single-batch training\")\n",
    "\n",
    "# SUCCESS: Batch training converges faster and more stably than single-batch training\n",
    "# MASTERY: Understanding how batch normalization stabilizes training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension 2: He-Ao-World's Messy Archive Challenge\n",
    "**\"Oh dear! I'm afraid I've made quite the mess of the scroll archive...\"**\n",
    "\n",
    "*He-Ao-World shuffles over with an apologetic expression, surrounded by scrolls covered in tea stains and torn edges.*\n",
    "\n",
    "**He-Ao-World**: \"So sorry, young Guardian! While organizing the ancient archive, I had a little... accident. Some scrolls got damaged, others have missing measurements, and a few might have been mislabeled. The real world is never as clean as our training chambers, I'm afraid.\"\n",
    "\n",
    "**Master Pai-Torch**: \"This is actually a valuable lesson. Real authentication data is never perfect - you must learn to handle noise, missing values, and mislabeled examples.\"\n",
    "\n",
    "**NEW CONCEPTS**: Noise robustness, missing data handling, label noise, data augmentation  \n",
    "**DIFFICULTY**: +25% (still Dan 2, but with real-world data challenges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßπ HE-AO-WORLD'S MESSY ARCHIVE CHALLENGE\n",
    "\n",
    "def create_messy_archive_data(features: torch.Tensor, labels: torch.Tensor, \n",
    "                            noise_level: float = 0.2, missing_rate: float = 0.1, \n",
    "                            label_noise_rate: float = 0.05) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    He-Ao-World's \"accidental\" data corruption that makes training more realistic.\n",
    "    \n",
    "    Args:\n",
    "        features: Original clean scroll features\n",
    "        labels: Original clean authenticity labels\n",
    "        noise_level: How much measurement noise to add (0.0 = clean, 1.0 = very noisy)\n",
    "        missing_rate: Fraction of measurements that are \"missing\" (set to 0)\n",
    "        label_noise_rate: Fraction of labels that are incorrectly flipped\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (messy_features, messy_labels)\n",
    "    \"\"\"\n",
    "    messy_features = features.clone()\n",
    "    messy_labels = labels.clone()\n",
    "    \n",
    "    # TODO: Add measurement noise to features\n",
    "    # Hint: Add Gaussian noise scaled by noise_level and the feature's standard deviation\n",
    "    # Example: messy_features += torch.randn_like(messy_features) * noise_level * messy_features.std()\n",
    "    \n",
    "    # TODO: Simulate missing data by randomly setting some features to 0\n",
    "    # Hint: Create a random mask and set those positions to 0\n",
    "    # Example: missing_mask = torch.rand_like(messy_features) < missing_rate\n",
    "    \n",
    "    # TODO: Add label noise by randomly flipping some labels\n",
    "    # Hint: Create a random mask and flip labels (1 becomes 0, 0 becomes 1)\n",
    "    \n",
    "    return messy_features, messy_labels\n",
    "\n",
    "class RobustGuardian(nn.Module):\n",
    "    \"\"\"\n",
    "    A Guardian trained to handle messy, real-world data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_features: int = 6, dropout_rate: float = 0.4):\n",
    "        super(RobustGuardian, self).__init__()\n",
    "        \n",
    "        # TODO: Design a more robust architecture\n",
    "        # Hint: Use higher dropout rate and consider skip connections\n",
    "        # Hint: Skip connections help with training stability\n",
    "        \n",
    "        self.hidden1 = nn.Linear(input_features, 128)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.hidden2 = nn.Linear(128, 128)  # Same size for skip connection\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.hidden3 = nn.Linear(128, 64)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.output = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO: Implement forward pass with skip connection\n",
    "        # Hint: Save the output of hidden1, then add it to the output of hidden2\n",
    "        \n",
    "        x1 = F.relu(self.hidden1(features))\n",
    "        x1 = self.dropout1(x1)\n",
    "        \n",
    "        x2 = F.relu(self.hidden2(x1))\n",
    "        x2 = self.dropout2(x2)\n",
    "        \n",
    "        # Skip connection: add the input to this layer's output\n",
    "        x2 = x2 + x1  # This helps with gradient flow and training stability\n",
    "        \n",
    "        x3 = F.relu(self.hidden3(x2))\n",
    "        x3 = self.dropout3(x3)\n",
    "        \n",
    "        output = torch.sigmoid(self.output(x3))\n",
    "        return output\n",
    "\n",
    "def train_robust_guardian(clean_features: torch.Tensor, clean_labels: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Train a Guardian that can handle He-Ao-World's messy data.\n",
    "    \"\"\"\n",
    "    print(\"üßπ He-Ao-World apologizes: 'Let me show you how messy real data can be...'\")\n",
    "    \n",
    "    # Create messy training data\n",
    "    messy_features, messy_labels = create_messy_archive_data(\n",
    "        clean_features, clean_labels, \n",
    "        noise_level=0.2, missing_rate=0.1, label_noise_rate=0.05\n",
    "    )\n",
    "    \n",
    "    # TODO: Compare clean vs messy data visually\n",
    "    # TODO: Train RobustGuardian on messy data\n",
    "    # TODO: Test on both clean and messy test sets\n",
    "    \n",
    "    print(\"TODO: Implement robust training with noisy data\")\n",
    "    \n",
    "# TRIAL: Train on messy data and test on both clean and messy test sets\n",
    "print(\"üßπ He-Ao-World's Messy Archive Challenge\")\n",
    "print(\"TODO: Compare performance on clean vs messy data\")\n",
    "\n",
    "# SUCCESS: Robust model maintains >75% accuracy even on messy data\n",
    "# MASTERY: Understanding the trade-off between robustness and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension 3: Master Pai-Torch's Learning Rate Wisdom\n",
    "**\"The path to enlightenment is not walked at constant speed, young Guardian.\"**\n",
    "\n",
    "*Master Pai-Torch sits in deep meditation, surrounded by floating symbols that pulse with varying intensity.*\n",
    "\n",
    "**Master Pai-Torch**: \"Observe the rhythm of your heartbeat, the cycle of seasons, the ebb and flow of tides. All wisdom follows patterns of change. So too must your learning rate adapt to the journey of training.\"\n",
    "\n",
    "**Master Pai-Torch**: \"In the beginning, bold steps are needed to escape local valleys. As wisdom grows, gentler steps preserve the knowledge gained. The master knows when to leap and when to tread carefully.\"\n",
    "\n",
    "**NEW CONCEPTS**: Learning rate scheduling, adaptive learning rates, cyclical training, warm restarts  \n",
    "**DIFFICULTY**: +35% (still Dan 2, but with advanced optimization techniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßô MASTER PAI-TORCH'S LEARNING RATE WISDOM\n",
    "\n",
    "def create_learning_rate_scheduler(optimizer, schedule_type: str = 'cosine', \n",
    "                                 warmup_epochs: int = 10, max_epochs: int = 100):\n",
    "    \"\"\"\n",
    "    Master Pai-Torch's wisdom on adapting learning rates during training.\n",
    "    \n",
    "    Args:\n",
    "        optimizer: The optimizer to schedule\n",
    "        schedule_type: Type of scheduling ('cosine', 'step', 'exponential')\n",
    "        warmup_epochs: Number of epochs to warm up the learning rate\n",
    "        max_epochs: Total number of training epochs\n",
    "    \n",
    "    Returns:\n",
    "        Learning rate scheduler\n",
    "    \"\"\"\n",
    "    if schedule_type == 'cosine':\n",
    "        # TODO: Implement cosine annealing scheduler\n",
    "        # Hint: Use torch.optim.lr_scheduler.CosineAnnealingLR\n",
    "        scheduler = None\n",
    "    elif schedule_type == 'step':\n",
    "        # TODO: Implement step scheduler (reduce LR every 30 epochs)\n",
    "        # Hint: Use torch.optim.lr_scheduler.StepLR\n",
    "        scheduler = None\n",
    "    elif schedule_type == 'exponential':\n",
    "        # TODO: Implement exponential decay scheduler\n",
    "        # Hint: Use torch.optim.lr_scheduler.ExponentialLR\n",
    "        scheduler = None\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown schedule type: {schedule_type}\")\n",
    "    \n",
    "    return scheduler\n",
    "\n",
    "def train_with_adaptive_learning(model: nn.Module, train_features: torch.Tensor, \n",
    "                               train_labels: torch.Tensor, val_features: torch.Tensor, \n",
    "                               val_labels: torch.Tensor, schedule_type: str = 'cosine'):\n",
    "    \"\"\"\n",
    "    Train the Guardian using Master Pai-Torch's adaptive learning rate wisdom.\n",
    "    \"\"\"\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)  # Start with higher learning rate\n",
    "    \n",
    "    scheduler = create_learning_rate_scheduler(optimizer, schedule_type, \n",
    "                                             warmup_epochs=20, max_epochs=200)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    learning_rates = []\n",
    "    \n",
    "    for epoch in range(200):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(train_features)\n",
    "        train_loss = criterion(predictions, train_labels)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_predictions = model(val_features)\n",
    "            val_loss = criterion(val_predictions, val_labels)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Record metrics\n",
    "        train_losses.append(train_loss.item())\n",
    "        val_losses.append(val_loss.item())\n",
    "        learning_rates.append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        if (epoch + 1) % 40 == 0:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f'Epoch [{epoch+1}/200], Train Loss: {train_loss.item():.4f}, '\n",
    "                  f'Val Loss: {val_loss.item():.4f}, LR: {current_lr:.6f}')\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'learning_rates': learning_rates\n",
    "    }\n",
    "\n",
    "def visualize_learning_rate_effect(histories: Dict[str, Dict]):\n",
    "    \"\"\"\n",
    "    Visualize the effect of different learning rate schedules.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Plot learning rates\n",
    "    for schedule_name, history in histories.items():\n",
    "        epochs = range(len(history['learning_rates']))\n",
    "        axes[0, 0].plot(epochs, history['learning_rates'], label=schedule_name, linewidth=2)\n",
    "    \n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Learning Rate')\n",
    "    axes[0, 0].set_title('üßô Learning Rate Schedules')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot validation losses\n",
    "    for schedule_name, history in histories.items():\n",
    "        epochs = range(len(history['val_losses']))\n",
    "        axes[0, 1].plot(epochs, history['val_losses'], label=schedule_name, linewidth=2)\n",
    "    \n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Validation Loss')\n",
    "    axes[0, 1].set_title('üéØ Validation Loss Comparison')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# TODO: Implement comparison of different learning rate schedules\n",
    "print(\"üßô Master Pai-Torch's Learning Rate Wisdom\")\n",
    "print(\"TODO: Compare cosine, step, and exponential learning rate schedules\")\n",
    "\n",
    "# TRIAL: Compare different learning rate schedules\n",
    "# SUCCESS: Understand how different schedules affect convergence speed and final performance\n",
    "# MASTERY: Choose the right schedule based on the problem characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension 4: Master Ao-Tougrad's Gradient Flow Mastery\n",
    "**\"The deepest networks require the most careful cultivation of gradient flow.\"**\n",
    "\n",
    "*Master Ao-Tougrad emerges from the shadows, ethereal gradients flowing around their form like mystical rivers.*\n",
    "\n",
    "**Master Ao-Tougrad**: \"You have learned to build deep networks, young Guardian, but do you understand the sacred flow of gradients through these depths? As networks grow deeper, the gradient spirits grow weaker, sometimes vanishing entirely before reaching the early layers.\"\n",
    "\n",
    "**Master Ao-Tougrad**: \"The ancient masters discovered techniques to preserve gradient strength: the Residual Paths, the Gradient Clipping, and the careful Weight Initialization. Master these arts, and you shall build networks deeper than the temple itself.\"\n",
    "\n",
    "**NEW CONCEPTS**: Gradient clipping, residual connections, weight initialization, gradient flow analysis  \n",
    "**DIFFICULTY**: +45% (still Dan 2, but approaching Dan 3 complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö° MASTER AO-TOUGRAD'S GRADIENT FLOW MASTERY\n",
    "\n",
    "class DeepResidualGuardian(nn.Module):\n",
    "    \"\"\"\n",
    "    A very deep Guardian network with residual connections to preserve gradient flow.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_features: int = 6, dropout_rate: float = 0.3):\n",
    "        super(DeepResidualGuardian, self).__init__()\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_proj = nn.Linear(input_features, 128)\n",
    "        \n",
    "        # TODO: Create multiple residual blocks\n",
    "        # Each block should have: Linear -> BatchNorm -> ReLU -> Dropout -> Linear\n",
    "        # with a skip connection around the entire block\n",
    "        \n",
    "        self.res_block1_1 = nn.Linear(128, 128)\n",
    "        self.res_block1_2 = nn.Linear(128, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # TODO: Add more residual blocks\n",
    "        self.res_block2_1 = nn.Linear(128, 128)\n",
    "        self.res_block2_2 = nn.Linear(128, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Output layers\n",
    "        self.output_proj = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "        \n",
    "        # TODO: Initialize weights using Xavier/He initialization\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"\n",
    "        Master Ao-Tougrad's wisdom on proper weight initialization.\n",
    "        \"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                # TODO: Initialize weights with Xavier/He initialization\n",
    "                # Hint: Use nn.init.xavier_uniform_ or nn.init.kaiming_uniform_\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.input_proj(features))\n",
    "        \n",
    "        # TODO: Implement residual block 1\n",
    "        # Pattern: identity = x, then transform x, then add identity back\n",
    "        identity1 = x\n",
    "        x = self.res_block1_1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.res_block1_2(x)\n",
    "        x = x + identity1  # Skip connection!\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # TODO: Implement residual block 2\n",
    "        identity2 = x\n",
    "        # TODO: Complete the second residual block\n",
    "        \n",
    "        # Output\n",
    "        x = F.relu(self.output_proj(x))\n",
    "        output = torch.sigmoid(self.output(x))\n",
    "        \n",
    "        return output\n",
    "\n",
    "def train_with_gradient_clipping(model: nn.Module, train_features: torch.Tensor, \n",
    "                               train_labels: torch.Tensor, val_features: torch.Tensor, \n",
    "                               val_labels: torch.Tensor, clip_value: float = 1.0):\n",
    "    \"\"\"\n",
    "    Train with gradient clipping to prevent gradient explosion.\n",
    "    \"\"\"\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    gradient_norms = []\n",
    "    \n",
    "    for epoch in range(200):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(train_features)\n",
    "        train_loss = criterion(predictions, train_labels)\n",
    "        train_loss.backward()\n",
    "        \n",
    "        # TODO: Calculate gradient norm before clipping\n",
    "        # Hint: Use torch.nn.utils.clip_grad_norm_\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "        gradient_norms.append(grad_norm.item())\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_predictions = model(val_features)\n",
    "            val_loss = criterion(val_predictions, val_labels)\n",
    "        \n",
    "        train_losses.append(train_loss.item())\n",
    "        val_losses.append(val_loss.item())\n",
    "        \n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f'Epoch [{epoch+1}/200], Train Loss: {train_loss.item():.4f}, '\n",
    "                  f'Val Loss: {val_loss.item():.4f}, Grad Norm: {grad_norm:.4f}')\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'gradient_norms': gradient_norms\n",
    "    }\n",
    "\n",
    "def analyze_gradient_flow(model: nn.Module, sample_input: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Analyze how gradients flow through the deep network.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # TODO: Perform forward pass and compute gradients\n",
    "    # TODO: Analyze gradient magnitudes at different layers\n",
    "    # TODO: Visualize gradient flow\n",
    "    \n",
    "    print(\"üîç Master Ao-Tougrad's Gradient Flow Analysis\")\n",
    "    print(\"TODO: Implement gradient flow analysis\")\n",
    "    \n",
    "    # Show which layers have strong vs weak gradients\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_magnitude = param.grad.abs().mean().item()\n",
    "            print(f\"{name}: Gradient magnitude = {grad_magnitude:.6f}\")\n",
    "\n",
    "# TODO: Implement deep residual network training\n",
    "print(\"‚ö° Master Ao-Tougrad's Gradient Flow Mastery\")\n",
    "print(\"TODO: Compare shallow vs deep networks, with and without residual connections\")\n",
    "\n",
    "# TRIAL: Train very deep networks (6+ layers) with and without residual connections\n",
    "# SUCCESS: Residual connections enable training of much deeper networks\n",
    "# MASTERY: Understanding gradient flow and why deep networks are hard to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî• CORRECTING YOUR FORM: A STANCE IMBALANCE\n",
    "\n",
    "*Master Pai-Torch observes your training ritual with a careful eye, while Master Ao-Tougrad materializes from the shadows with a knowing expression.*\n",
    "\n",
    "**Master Pai-Torch**: \"Your eager mind races ahead of your disciplined form, grasshopper. I sense a disturbance in your validation discipline - your network memorizes when it should generalize.\"\n",
    "\n",
    "**Master Ao-Tougrad**: \"The gradient flow speaks of poor technique. Without proper regularization, even the most skilled Guardian falls to the curse of overfitting. Observe this flawed training ritual left by a previous disciple.\"\n",
    "\n",
    "*The masters gesture toward a complex training apparatus that pulses with unstable energy.*\n",
    "\n",
    "**Master Pai-Torch**: \"Can you restore balance to this chaotic training? The errors are subtle but deadly - your form needs correction.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üî• CORRECTING YOUR FORM: A STANCE IMBALANCE\n",
    "\n",
    "class FlawedGuardian(nn.Module):\n",
    "    \"\"\"\n",
    "    A Guardian network with subtle but critical flaws in its training discipline.\n",
    "    Can you identify and correct the stance imbalances?\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_features: int = 6):\n",
    "        super(FlawedGuardian, self).__init__()\n",
    "        \n",
    "        # The architecture looks correct...\n",
    "        self.hidden1 = nn.Linear(input_features, 256)  # Suspiciously large for this problem\n",
    "        self.hidden2 = nn.Linear(256, 256)\n",
    "        self.hidden3 = nn.Linear(256, 256)\n",
    "        self.hidden4 = nn.Linear(256, 128)\n",
    "        self.hidden5 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "        \n",
    "        # No dropout layers - could this be the issue?\n",
    "        \n",
    "    def forward(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.hidden1(features))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = F.relu(self.hidden3(x))\n",
    "        x = F.relu(self.hidden4(x))\n",
    "        x = F.relu(self.hidden5(x))\n",
    "        output = torch.sigmoid(self.output(x))\n",
    "        return output\n",
    "\n",
    "def flawed_training_ritual(model: nn.Module, features: torch.Tensor, labels: torch.Tensor):\n",
    "    \"\"\"\n",
    "    This training ritual has lost its balance - your form needs correction! ü•ã\n",
    "    \n",
    "    Multiple critical errors lurk within this seemingly correct training loop.\n",
    "    \"\"\"\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)  # Learning rate seems high...\n",
    "    \n",
    "    # No train/validation split - training on all data!\n",
    "    train_losses = []\n",
    "    \n",
    "    for epoch in range(2000):  # Very long training...\n",
    "        model.train()\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(features)\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # No gradient clearing between epochs!\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        if epoch % 200 == 0:\n",
    "            accuracy = ((predictions > 0.5) == labels).float().mean()\n",
    "            print(f'Epoch {epoch}: Loss = {loss.item():.4f}, Accuracy = {accuracy:.4f}')\n",
    "            \n",
    "            if accuracy > 0.95:  # Suspiciously high accuracy\n",
    "                print(\"üéâ Perfect accuracy achieved! Training complete!\")\n",
    "                break\n",
    "    \n",
    "    return model, train_losses\n",
    "\n",
    "def test_flawed_guardian():\n",
    "    \"\"\"\n",
    "    Test the flawed Guardian and reveal the problems.\n",
    "    \"\"\"\n",
    "    print(\"üî• Testing the Flawed Guardian's Training Ritual...\")\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: This code contains multiple critical errors!\")\n",
    "    \n",
    "    # Create model and data\n",
    "    flawed_model = FlawedGuardian()\n",
    "    features, labels = generate_scroll_authentication_data(n_scrolls=500, complexity_level=0.2)\n",
    "    \n",
    "    # Train with flawed method\n",
    "    trained_model, losses = flawed_training_ritual(flawed_model, features, labels)\n",
    "    \n",
    "    # Test on the SAME data (another error!)\n",
    "    trained_model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_predictions = trained_model(features)\n",
    "        test_accuracy = ((test_predictions > 0.5) == labels).float().mean()\n",
    "    \n",
    "    print(f\"\\nüìä Final Training Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"üìä Final Training Loss: {losses[-1]:.4f}\")\n",
    "    \n",
    "    # Now test on NEW data (the real test)\n",
    "    new_features, new_labels = generate_scroll_authentication_data(n_scrolls=200, complexity_level=0.2, sacred_seed=123)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        new_predictions = trained_model(new_features)\n",
    "        new_accuracy = ((new_predictions > 0.5) == new_labels).float().mean()\n",
    "    \n",
    "    print(f\"\\nüö® REAL TEST (New Data) Accuracy: {new_accuracy:.4f}\")\n",
    "    print(f\"üìâ Performance Drop: {(test_accuracy - new_accuracy):.4f}\")\n",
    "    \n",
    "    if new_accuracy < test_accuracy * 0.8:\n",
    "        print(\"\\nüí• CRITICAL OVERFITTING DETECTED!\")\n",
    "        print(\"üßô Master Pai-Torch speaks: 'Your network memorizes, it does not understand!'\")\n",
    "        print(\"‚ö° Master Ao-Tougrad whispers: 'The gradient discipline has been abandoned...'\")\n",
    "    \n",
    "    return trained_model, losses\n",
    "\n",
    "# DEBUGGING CHALLENGE: Can you identify ALL the critical errors?\n",
    "print(\"üîç DEBUGGING CHALLENGE: Find and fix the flawed training ritual!\")\n",
    "print(\"\\nErrors to identify:\")\n",
    "print(\"1. Missing gradient clearing (optimizer.zero_grad())\")\n",
    "print(\"2. No train/validation split\")\n",
    "print(\"3. Testing on training data\")\n",
    "print(\"4. Network too large for the problem (overfitting)\")\n",
    "print(\"5. No regularization (dropout)\")\n",
    "print(\"6. No early stopping\")\n",
    "print(\"7. Learning rate too high\")\n",
    "print(\"8. Training too long without validation\")\n",
    "\n",
    "# MASTER'S WISDOM: \"The undisciplined mind accumulates old thoughts,\n",
    "# just as the untrained gradient accumulates old directions.\"\n",
    "\n",
    "# HINT: The most critical error causes gradient accumulation across epochs!\n",
    "# HINT: Perfect training accuracy with poor test accuracy signals overfitting!\n",
    "# HINT: A true Guardian tests on data they've never seen before!\n",
    "\n",
    "# Run the flawed training to see the problems\n",
    "test_flawed_guardian()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÜ THE GUARDIAN'S MASTERY ACHIEVED\n",
    "\n",
    "*As your training completes, both Master Pai-Torch and Master Ao-Tougrad approach with expressions of deep approval.*\n",
    "\n",
    "**Master Pai-Torch**: \"Rise, Guardian. You have learned to balance the complexity of deep networks with the wisdom of regularization. Your models now generalize rather than memorize, a distinction that separates true masters from mere practitioners.\"\n",
    "\n",
    "**Master Ao-Tougrad**: \"The gradient flow bends to your will, yet you have learned restraint. Dropout shields your networks from overfitting, while validation guides your training with wisdom. You understand that the path to mastery lies not in perfect training accuracy, but in the harmony between learning and generalization.\"\n",
    "\n",
    "**Master Pai-Torch**: \"You have earned the title of Temple Guardian. Your next challenge awaits in the weapons chamber, where specialized architectures forge tools for specific battles. But that is a trial for another day.\"\n",
    "\n",
    "*The masters bow deeply as the sacred scroll of mastery appears before you, glowing with the wisdom of the deep networks.*\n",
    "\n",
    "### üéì Sacred Knowledge Gained:\n",
    "- **Deep Architecture Mastery**: Multi-layer networks with proper structure\n",
    "- **Regularization Wisdom**: Dropout and early stopping to prevent overfitting\n",
    "- **Validation Discipline**: Proper train/validation/test splits\n",
    "- **Optimization Arts**: Adam optimizer and learning rate scheduling\n",
    "- **Gradient Flow Understanding**: Residual connections and gradient clipping\n",
    "- **Real-World Robustness**: Handling noisy and imperfect data\n",
    "\n",
    "**The Guardian's Oath**: *\"I swear to validate before I deploy, to regularize before I optimize, and to generalize beyond mere memorization. May my networks serve the temple with wisdom and restraint.\"*\n",
    "\n",
    "---\n",
    "\n",
    "**üåü Next Quest**: Dan 3 - The Weapon Master awaits to teach you the specialized architectures: CNNs, RNNs, and the mystical attention mechanisms that can perceive patterns across space and time..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}